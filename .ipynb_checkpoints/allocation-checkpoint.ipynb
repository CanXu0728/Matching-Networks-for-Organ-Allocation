{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from IPython.display import Image \n",
    "from sklearn.cluster import KMeans\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Lambda, Dot, Subtract, Multiply, Concatenate, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import KLD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import permutations\n",
    "import colorsys\n",
    "import random\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/paired_small.json'\n",
    "model_path = './model'\n",
    "fully_observed = False\n",
    "\n",
    "\n",
    "with open(data_path, 'r', encoding='utf8') as f:\n",
    "    data_dic = json.loads(f.read())\n",
    "    \n",
    "    \n",
    "data = np.array(data_dic['data'], dtype='float32')\n",
    "# shuffle data\n",
    "np.random.shuffle(data)\n",
    "# split train-test set\n",
    "train_split = int(data_dic['size'] * 9 / 10)\n",
    "train = data[:train_split]\n",
    "test = data[train_split:]\n",
    "\n",
    "if 'fully_observed' in data_dic and data_dic['fully_observed']:\n",
    "    fully_observed = True\n",
    "    \n",
    "    D_train = train[:, :data_dic['donor_dim']]\n",
    "    R_train = train[:, data_dic['recipientID_col']+1:data_dic['recipientID_col']+1+data_dic['recipient_dim']]\n",
    "    y_full_train = train[:, -data_dic['n_class']-1:-1]\n",
    "    t_train = train[:, -1].astype('int')\n",
    "    y_train = y_full_train[np.arange(len(t_train)), t_train]\n",
    "\n",
    "    D_test = test[:, :data_dic['donor_dim']]\n",
    "    R_test = test[:, data_dic['recipientID_col']+1:data_dic['recipientID_col']+1+data_dic['recipient_dim']]\n",
    "    y_full_test = test[:, -data_dic['n_class']-1:-1]\n",
    "    t_test = test[:, -1].astype('int')\n",
    "    y_test = y_full_test[np.arange(len(t_test)), t_test]\n",
    "else:\n",
    "    D_train = train[:, :data_dic['donor_dim']]\n",
    "    R_train = train[:, data_dic['recipientID_col']+1:data_dic['recipientID_col']+1+data_dic['recipient_dim']]\n",
    "    y_train = train[:, -1]\n",
    "\n",
    "    D_test = test[:, :data_dic['donor_dim']]\n",
    "    R_test = test[:, data_dic['recipientID_col']+1:data_dic['recipientID_col']+1+data_dic['recipient_dim']]\n",
    "    y_test = test[:, -1]\n",
    "    \n",
    "    \n",
    "X_train = np.hstack([D_train, R_train])\n",
    "X_test = np.hstack([D_test, R_test])\n",
    "\n",
    "DD = data[:, :data_dic['donor_dim']]\n",
    "RR = data[:, data_dic['recipientID_col']+1:data_dic['recipientID_col']+1+data_dic['recipient_dim']]\n",
    "np.random.shuffle(DD)\n",
    "np.random.shuffle(RR)\n",
    "SS = np.absolute(np.arange(len(DD)) + np.random.normal(0, 1000, size=len(DD)).astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "# fully_observed = False\n",
    "\n",
    "rdims = [48, 96, 20]\n",
    "bdims = [48, 48, 96]\n",
    "cdims = [D_train.shape[-1], 48, 96]\n",
    "AEdims = [D_train.shape[-1], 48, 48, 96, 5]\n",
    "\n",
    "\n",
    "def plot_cluster(kmeansdata, label_list, num_cluster, scale=10):\n",
    "    \"\"\"\n",
    "    Function to convert the n-dimensional cluster to \n",
    "    2-dimensional cluster and plotting 50 random clusters\n",
    "    file%d.png    -> file where the output is stored indexed\n",
    "                     by first available file index\n",
    "                     e.g. file1.png , file2.png ...\n",
    "    \"\"\"\n",
    "    pca = PCA(2)\n",
    "    pca.fit(kmeansdata)\n",
    "    users_2d = pca.transform(kmeansdata)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlim([users_2d[:, 0].min() - scale, users_2d[:, 0].max() + scale])\n",
    "    plt.ylim([users_2d[:, 1].min() - scale, users_2d[:, 1].max() + scale])\n",
    "\n",
    "\n",
    "    # Plotting only the points whose centers were plotted\n",
    "    # Points are represented as a small '+' marker\n",
    "    for i, position in enumerate(label_list):\n",
    "        plt.scatter(users_2d[i, 0], users_2d[i, 1] , marker='+' , c=[colors[position]])\n",
    "\n",
    "#     filename = \"sDEC\"\n",
    "#     i = 0\n",
    "#     while True:\n",
    "#         if os.path.isfile(filename + str(i) + \".png\") == False:\n",
    "#             #new index found write file and return\n",
    "#             plt.savefig(filename + str(i) + \".png\")\n",
    "#             break\n",
    "#         else:\n",
    "#             #Changing index to next number\n",
    "#             i = i + 1\n",
    "    return\n",
    "\n",
    "\n",
    "colors = [(1, 0, 0), (0, 1, 0), (0, 0, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (.5, .1, .1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def REGModel():\n",
    "    inp = Input(shape=(X_train.shape[-1], ), name='pair_input')\n",
    "    x = BatchNormalization()(inp)\n",
    "    for i in range(len(rdims)):\n",
    "        x = Dense(rdims[i], activation='relu', kernel_initializer='normal')(x)\n",
    "    out = Dense(1, activation='relu', kernel_initializer='normal')(x)\n",
    "    \n",
    "    return Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "REG = REGModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "4014/4014 [==============================] - 2s 423us/step - loss: 2434897.2953 - val_loss: 2564396.3688\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2564396.36883, saving model to ./model/REGCheckpoint\n",
      "Epoch 2/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 2434174.1913 - val_loss: 2562832.4092\n",
      "\n",
      "Epoch 00002: val_loss improved from 2564396.36883 to 2562832.40919, saving model to ./model/REGCheckpoint\n",
      "Epoch 3/150\n",
      "4014/4014 [==============================] - 0s 12us/step - loss: 2430334.8935 - val_loss: 2555212.1132\n",
      "\n",
      "Epoch 00003: val_loss improved from 2562832.40919 to 2555212.11323, saving model to ./model/REGCheckpoint\n",
      "Epoch 4/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 2413685.9920 - val_loss: 2524862.5437\n",
      "\n",
      "Epoch 00004: val_loss improved from 2555212.11323 to 2524862.54372, saving model to ./model/REGCheckpoint\n",
      "Epoch 5/150\n",
      "4014/4014 [==============================] - 0s 12us/step - loss: 2353864.3039 - val_loss: 2424462.2130\n",
      "\n",
      "Epoch 00005: val_loss improved from 2524862.54372 to 2424462.21300, saving model to ./model/REGCheckpoint\n",
      "Epoch 6/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 2177052.7864 - val_loss: 2150613.8318\n",
      "\n",
      "Epoch 00006: val_loss improved from 2424462.21300 to 2150613.83184, saving model to ./model/REGCheckpoint\n",
      "Epoch 7/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 1776701.8535 - val_loss: 1599900.7909\n",
      "\n",
      "Epoch 00007: val_loss improved from 2150613.83184 to 1599900.79092, saving model to ./model/REGCheckpoint\n",
      "Epoch 8/150\n",
      "4014/4014 [==============================] - 0s 12us/step - loss: 1201688.8446 - val_loss: 1052604.4005\n",
      "\n",
      "Epoch 00008: val_loss improved from 1599900.79092 to 1052604.40050, saving model to ./model/REGCheckpoint\n",
      "Epoch 9/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 935245.6274 - val_loss: 905763.8442\n",
      "\n",
      "Epoch 00009: val_loss improved from 1052604.40050 to 905763.84417, saving model to ./model/REGCheckpoint\n",
      "Epoch 10/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 840759.9843 - val_loss: 824824.5796\n",
      "\n",
      "Epoch 00010: val_loss improved from 905763.84417 to 824824.57960, saving model to ./model/REGCheckpoint\n",
      "Epoch 11/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 772895.0255 - val_loss: 749210.9840\n",
      "\n",
      "Epoch 00011: val_loss improved from 824824.57960 to 749210.98402, saving model to ./model/REGCheckpoint\n",
      "Epoch 12/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 725304.3162 - val_loss: 694071.9664\n",
      "\n",
      "Epoch 00012: val_loss improved from 749210.98402 to 694071.96637, saving model to ./model/REGCheckpoint\n",
      "Epoch 13/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 684579.7897 - val_loss: 652628.0457\n",
      "\n",
      "Epoch 00013: val_loss improved from 694071.96637 to 652628.04568, saving model to ./model/REGCheckpoint\n",
      "Epoch 14/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 647759.9901 - val_loss: 612986.9327\n",
      "\n",
      "Epoch 00014: val_loss improved from 652628.04568 to 612986.93274, saving model to ./model/REGCheckpoint\n",
      "Epoch 15/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 616415.1919 - val_loss: 573573.9961\n",
      "\n",
      "Epoch 00015: val_loss improved from 612986.93274 to 573573.99608, saving model to ./model/REGCheckpoint\n",
      "Epoch 16/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 589542.3643 - val_loss: 546337.0413\n",
      "\n",
      "Epoch 00016: val_loss improved from 573573.99608 to 546337.04134, saving model to ./model/REGCheckpoint\n",
      "Epoch 17/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 563495.2216 - val_loss: 523263.1404\n",
      "\n",
      "Epoch 00017: val_loss improved from 546337.04134 to 523263.14041, saving model to ./model/REGCheckpoint\n",
      "Epoch 18/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 544847.6734 - val_loss: 501524.2095\n",
      "\n",
      "Epoch 00018: val_loss improved from 523263.14041 to 501524.20950, saving model to ./model/REGCheckpoint\n",
      "Epoch 19/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 528354.7158 - val_loss: 484630.0446\n",
      "\n",
      "Epoch 00019: val_loss improved from 501524.20950 to 484630.04456, saving model to ./model/REGCheckpoint\n",
      "Epoch 20/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 513016.0663 - val_loss: 472332.1453\n",
      "\n",
      "Epoch 00020: val_loss improved from 484630.04456 to 472332.14532, saving model to ./model/REGCheckpoint\n",
      "Epoch 21/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 503600.7354 - val_loss: 457720.4340\n",
      "\n",
      "Epoch 00021: val_loss improved from 472332.14532 to 457720.43400, saving model to ./model/REGCheckpoint\n",
      "Epoch 22/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 495829.9012 - val_loss: 449946.1917\n",
      "\n",
      "Epoch 00022: val_loss improved from 457720.43400 to 449946.19170, saving model to ./model/REGCheckpoint\n",
      "Epoch 23/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 488314.1132 - val_loss: 445582.4695\n",
      "\n",
      "Epoch 00023: val_loss improved from 449946.19170 to 445582.46945, saving model to ./model/REGCheckpoint\n",
      "Epoch 24/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 477451.7520 - val_loss: 436774.6047\n",
      "\n",
      "Epoch 00024: val_loss improved from 445582.46945 to 436774.60468, saving model to ./model/REGCheckpoint\n",
      "Epoch 25/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 477014.2635 - val_loss: 437234.6897\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 436774.60468\n",
      "Epoch 26/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 472249.7395 - val_loss: 429215.0848\n",
      "\n",
      "Epoch 00026: val_loss improved from 436774.60468 to 429215.08478, saving model to ./model/REGCheckpoint\n",
      "Epoch 27/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 467903.5102 - val_loss: 431121.5277\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 429215.08478\n",
      "Epoch 28/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 465879.1404 - val_loss: 422231.2598\n",
      "\n",
      "Epoch 00028: val_loss improved from 429215.08478 to 422231.25981, saving model to ./model/REGCheckpoint\n",
      "Epoch 29/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 460806.3924 - val_loss: 426253.5301\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 422231.25981\n",
      "Epoch 30/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 457799.5417 - val_loss: 421909.9217\n",
      "\n",
      "Epoch 00030: val_loss improved from 422231.25981 to 421909.92166, saving model to ./model/REGCheckpoint\n",
      "Epoch 31/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 455240.4263 - val_loss: 421945.2490\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 421909.92166\n",
      "Epoch 32/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 454874.2664 - val_loss: 418574.6333\n",
      "\n",
      "Epoch 00032: val_loss improved from 421909.92166 to 418574.63327, saving model to ./model/REGCheckpoint\n",
      "Epoch 33/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 447186.5216 - val_loss: 419488.7294\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 418574.63327\n",
      "Epoch 34/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 448658.9111 - val_loss: 418891.1078\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 418574.63327\n",
      "Epoch 35/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 449639.8968 - val_loss: 416696.8587\n",
      "\n",
      "Epoch 00035: val_loss improved from 418574.63327 to 416696.85874, saving model to ./model/REGCheckpoint\n",
      "Epoch 36/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 445312.4650 - val_loss: 419433.1812\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 416696.85874\n",
      "Epoch 37/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 443983.5454 - val_loss: 415385.1263\n",
      "\n",
      "Epoch 00037: val_loss improved from 416696.85874 to 415385.12626, saving model to ./model/REGCheckpoint\n",
      "Epoch 38/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 441701.9111 - val_loss: 415240.0212\n",
      "\n",
      "Epoch 00038: val_loss improved from 415385.12626 to 415240.02116, saving model to ./model/REGCheckpoint\n",
      "Epoch 39/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 442065.2829 - val_loss: 415546.0929\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 415240.02116\n",
      "Epoch 40/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 441872.9890 - val_loss: 412547.4246\n",
      "\n",
      "Epoch 00040: val_loss improved from 415240.02116 to 412547.42461, saving model to ./model/REGCheckpoint\n",
      "Epoch 41/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 440933.2403 - val_loss: 412870.5352\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 412547.42461\n",
      "Epoch 42/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 441351.1513 - val_loss: 412320.5425\n",
      "\n",
      "Epoch 00042: val_loss improved from 412547.42461 to 412320.54246, saving model to ./model/REGCheckpoint\n",
      "Epoch 43/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 434549.0300 - val_loss: 413056.8746\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 412320.54246\n",
      "Epoch 44/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 434577.1265 - val_loss: 410263.9645\n",
      "\n",
      "Epoch 00044: val_loss improved from 412320.54246 to 410263.96455, saving model to ./model/REGCheckpoint\n",
      "Epoch 45/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 435102.3480 - val_loss: 409530.2636\n",
      "\n",
      "Epoch 00045: val_loss improved from 410263.96455 to 409530.26359, saving model to ./model/REGCheckpoint\n",
      "Epoch 46/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 431530.2005 - val_loss: 410040.9630\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 409530.26359\n",
      "Epoch 47/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 432571.8902 - val_loss: 408887.6993\n",
      "\n",
      "Epoch 00047: val_loss improved from 409530.26359 to 408887.69927, saving model to ./model/REGCheckpoint\n",
      "Epoch 48/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 429542.4472 - val_loss: 410539.5542\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 408887.69927\n",
      "Epoch 49/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 431448.6095 - val_loss: 409595.7089\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 408887.69927\n",
      "Epoch 50/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 428615.5948 - val_loss: 409836.0420\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 408887.69927\n",
      "Epoch 51/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 428603.5989 - val_loss: 408818.4888\n",
      "\n",
      "Epoch 00051: val_loss improved from 408887.69927 to 408818.48879, saving model to ./model/REGCheckpoint\n",
      "Epoch 52/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 431567.4560 - val_loss: 407778.7585\n",
      "\n",
      "Epoch 00052: val_loss improved from 408818.48879 to 407778.75855, saving model to ./model/REGCheckpoint\n",
      "Epoch 53/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 427175.1625 - val_loss: 408931.3401\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 407778.75855\n",
      "Epoch 54/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 426510.1067 - val_loss: 408703.6637\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 407778.75855\n",
      "Epoch 55/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 426003.7128 - val_loss: 412129.1893\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 407778.75855\n",
      "Epoch 56/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 425569.6039 - val_loss: 409245.5032\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 407778.75855\n",
      "Epoch 57/150\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 425396.8050 - val_loss: 407864.5436\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 407778.75855\n",
      "Epoch 58/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 424374.9996 - val_loss: 410871.7824\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 407778.75855\n",
      "Epoch 59/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 428420.7922 - val_loss: 408001.0020\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 407778.75855\n",
      "Epoch 60/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 422643.5715 - val_loss: 405484.0517\n",
      "\n",
      "Epoch 00060: val_loss improved from 407778.75855 to 405484.05171, saving model to ./model/REGCheckpoint\n",
      "Epoch 61/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 422940.3294 - val_loss: 406946.4992\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 405484.05171\n",
      "Epoch 62/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 419479.7987 - val_loss: 406063.7216\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 405484.05171\n",
      "Epoch 63/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 424309.2157 - val_loss: 410014.2478\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 405484.05171\n",
      "Epoch 64/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 419659.2799 - val_loss: 405394.6473\n",
      "\n",
      "Epoch 00064: val_loss improved from 405484.05171 to 405394.64728, saving model to ./model/REGCheckpoint\n",
      "Epoch 65/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 423121.1137 - val_loss: 408821.3052\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 405394.64728\n",
      "Epoch 66/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 418686.4030 - val_loss: 404905.4822\n",
      "\n",
      "Epoch 00066: val_loss improved from 405394.64728 to 404905.48220, saving model to ./model/REGCheckpoint\n",
      "Epoch 67/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 418289.3920 - val_loss: 407766.3565\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 404905.48220\n",
      "Epoch 68/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 419393.6512 - val_loss: 405700.7679\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 404905.48220\n",
      "Epoch 69/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 424238.5587 - val_loss: 410038.9996\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 404905.48220\n",
      "Epoch 70/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 417342.7363 - val_loss: 404998.7487\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 404905.48220\n",
      "Epoch 71/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 417640.9162 - val_loss: 405496.4212\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 404905.48220\n",
      "Epoch 72/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 420292.4108 - val_loss: 404841.7867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: val_loss improved from 404905.48220 to 404841.78672, saving model to ./model/REGCheckpoint\n",
      "Epoch 73/150\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 414158.2562 - val_loss: 409067.3572\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 404841.78672\n",
      "Epoch 74/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 419005.7936 - val_loss: 405096.8061\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 404841.78672\n",
      "Epoch 75/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 413983.7261 - val_loss: 404768.7113\n",
      "\n",
      "Epoch 00075: val_loss improved from 404841.78672 to 404768.71132, saving model to ./model/REGCheckpoint\n",
      "Epoch 76/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 414196.4299 - val_loss: 405788.3446\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 404768.71132\n",
      "Epoch 77/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 415917.1538 - val_loss: 406503.5636\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 404768.71132\n",
      "Epoch 78/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 413958.3635 - val_loss: 406186.9491\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 404768.71132\n",
      "Epoch 79/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 412887.6002 - val_loss: 404497.9145\n",
      "\n",
      "Epoch 00079: val_loss improved from 404768.71132 to 404497.91452, saving model to ./model/REGCheckpoint\n",
      "Epoch 80/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 412382.4639 - val_loss: 403202.9205\n",
      "\n",
      "Epoch 00080: val_loss improved from 404497.91452 to 403202.92054, saving model to ./model/REGCheckpoint\n",
      "Epoch 81/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 411345.2233 - val_loss: 405211.2151\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 403202.92054\n",
      "Epoch 82/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 417597.6284 - val_loss: 403856.1691\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 403202.92054\n",
      "Epoch 83/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 408442.0486 - val_loss: 405246.6712\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 403202.92054\n",
      "Epoch 84/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 411554.0015 - val_loss: 405396.2007\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 403202.92054\n",
      "Epoch 85/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 409212.8526 - val_loss: 403967.5582\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 403202.92054\n",
      "Epoch 86/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 413369.6508 - val_loss: 406231.4284\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 403202.92054\n",
      "Epoch 87/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 408561.7814 - val_loss: 407124.1375\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 403202.92054\n",
      "Epoch 88/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 406012.8524 - val_loss: 404006.9473\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 403202.92054\n",
      "Epoch 89/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 409316.2824 - val_loss: 403920.5582\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 403202.92054\n",
      "Epoch 90/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 405085.0636 - val_loss: 404791.1917\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 403202.92054\n",
      "Epoch 91/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 410506.2054 - val_loss: 405170.8042\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 403202.92054\n",
      "Epoch 92/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 408643.8118 - val_loss: 404813.2365\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 403202.92054\n",
      "Epoch 93/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 410279.6515 - val_loss: 404313.8529\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 403202.92054\n",
      "Epoch 94/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 411845.4343 - val_loss: 403850.1293\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 403202.92054\n",
      "Epoch 95/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 407123.9034 - val_loss: 406164.6948\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 403202.92054\n",
      "Epoch 96/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 406956.9742 - val_loss: 404809.2780\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 403202.92054\n",
      "Epoch 97/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 407300.7777 - val_loss: 402770.8171\n",
      "\n",
      "Epoch 00097: val_loss improved from 403202.92054 to 402770.81712, saving model to ./model/REGCheckpoint\n",
      "Epoch 98/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 405687.8942 - val_loss: 406824.8601\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 402770.81712\n",
      "Epoch 99/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 403709.9819 - val_loss: 404000.6047\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 402770.81712\n",
      "Epoch 100/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 401651.8379 - val_loss: 405187.8522\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 402770.81712\n",
      "Epoch 101/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 402652.8092 - val_loss: 404275.8327\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 402770.81712\n",
      "Epoch 102/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 401657.8937 - val_loss: 405329.1372\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 402770.81712\n",
      "Epoch 103/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 400933.3372 - val_loss: 405901.8983\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 402770.81712\n",
      "Epoch 104/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 396968.7793 - val_loss: 404753.1065\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 402770.81712\n",
      "Epoch 105/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 401635.4764 - val_loss: 404815.8736\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 402770.81712\n",
      "Epoch 106/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 397773.5113 - val_loss: 405098.9850\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 402770.81712\n",
      "Epoch 107/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 396954.4455 - val_loss: 405635.8976\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 402770.81712\n",
      "Epoch 108/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 400479.6647 - val_loss: 405979.3100\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 402770.81712\n",
      "Epoch 109/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 403038.3423 - val_loss: 407696.6229\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 402770.81712\n",
      "Epoch 110/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 399199.6673 - val_loss: 405170.3564\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 402770.81712\n",
      "Epoch 111/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 398781.8532 - val_loss: 406235.0703\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 402770.81712\n",
      "Epoch 112/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 395510.2715 - val_loss: 405347.6960\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 402770.81712\n",
      "Epoch 113/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 400768.7281 - val_loss: 408907.1745\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 402770.81712\n",
      "Epoch 114/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 400078.5725 - val_loss: 405924.1571\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 402770.81712\n",
      "Epoch 115/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 397105.7583 - val_loss: 405720.2905\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 402770.81712\n",
      "Epoch 116/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 395842.0803 - val_loss: 403111.7705\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 402770.81712\n",
      "Epoch 117/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 395427.8919 - val_loss: 405808.8880\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 402770.81712\n",
      "Epoch 118/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 391786.0384 - val_loss: 404910.5856\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 402770.81712\n",
      "Epoch 119/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 13us/step - loss: 396182.3055 - val_loss: 405199.3721\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 402770.81712\n",
      "Epoch 120/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 392826.9949 - val_loss: 404445.8540\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 402770.81712\n",
      "Epoch 121/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 396036.0855 - val_loss: 406525.1833\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 402770.81712\n",
      "Epoch 122/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 391931.4204 - val_loss: 406003.5940\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 402770.81712\n",
      "Epoch 123/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 393462.6132 - val_loss: 405344.6495\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 402770.81712\n",
      "Epoch 124/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 392745.6826 - val_loss: 407193.1015\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 402770.81712\n",
      "Epoch 125/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 394612.7632 - val_loss: 406082.9718\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 402770.81712\n",
      "Epoch 126/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 395450.5426 - val_loss: 410387.0610\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 402770.81712\n",
      "Epoch 127/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 389896.2525 - val_loss: 406037.8876\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 402770.81712\n",
      "Epoch 128/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 392513.2418 - val_loss: 406852.3928\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 402770.81712\n",
      "Epoch 129/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 390674.0924 - val_loss: 406545.9012\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 402770.81712\n",
      "Epoch 130/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 392283.1167 - val_loss: 407102.1518\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 402770.81712\n",
      "Epoch 131/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 386044.2300 - val_loss: 407796.0765\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 402770.81712\n",
      "Epoch 132/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 390557.7475 - val_loss: 408143.3519\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 402770.81712\n",
      "Epoch 133/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 391629.7108 - val_loss: 407947.6606\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 402770.81712\n",
      "Epoch 134/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 388865.6686 - val_loss: 405734.6153\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 402770.81712\n",
      "Epoch 135/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 386312.8342 - val_loss: 407245.0746\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 402770.81712\n",
      "Epoch 136/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 387965.0785 - val_loss: 407244.6072\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 402770.81712\n",
      "Epoch 137/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 384526.6860 - val_loss: 407335.0908\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 402770.81712\n",
      "Epoch 138/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 388350.6349 - val_loss: 405670.1836\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 402770.81712\n",
      "Epoch 139/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 383540.4764 - val_loss: 408639.3680\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 402770.81712\n",
      "Epoch 140/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 383818.1719 - val_loss: 408395.7707\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 402770.81712\n",
      "Epoch 141/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 381546.0905 - val_loss: 409795.6605\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 402770.81712\n",
      "Epoch 142/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 385542.3787 - val_loss: 407757.1066\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 402770.81712\n",
      "Epoch 143/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 383533.4215 - val_loss: 407151.9940\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 402770.81712\n",
      "Epoch 144/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 383077.0298 - val_loss: 407637.0078\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 402770.81712\n",
      "Epoch 145/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 379027.2971 - val_loss: 406319.6163\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 402770.81712\n",
      "Epoch 146/150\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 383134.6609 - val_loss: 408081.8643\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 402770.81712\n",
      "Epoch 147/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 381035.0467 - val_loss: 407519.7011\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 402770.81712\n",
      "Epoch 148/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 382309.1021 - val_loss: 411987.6584\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 402770.81712\n",
      "Epoch 149/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 379688.3552 - val_loss: 408649.8959\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 402770.81712\n",
      "Epoch 150/150\n",
      "4014/4014 [==============================] - 0s 13us/step - loss: 379045.5732 - val_loss: 411920.7954\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 402770.81712\n"
     ]
    }
   ],
   "source": [
    "REG.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "checkpointer = ModelCheckpoint(filepath='./model/REGCheckpoint', verbose=1, save_best_only=True)\n",
    "REG.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=256, epochs=150, callbacks=[checkpointer])\n",
    "\n",
    "REG.load_weights('./model/REGCheckpoint')\n",
    "REG.save_weights('./model/REGweights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def autoencoder(dims, act='relu', init='random_normal'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_layers = len(dims) - 1\n",
    "    # input\n",
    "    inp = Input(shape=(dims[0],), name='AEinput')\n",
    "    x = BatchNormalization()(inp)\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_layers-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_layers - 1))(x)\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_layers-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    decoded = x\n",
    "    return Model(inputs=inp, outputs=decoded, name='AE'), Model(inputs=inp, outputs=encoded, name='encoder')\n",
    "\n",
    "AE, encoder = autoencoder(AEdims)\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.        \n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "# target distribution of DEC    \n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T\n",
    "\n",
    "\n",
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "DEC = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "\n",
    "# Initialize cluster centers using k-means.\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "D_clus = kmeans.fit_predict(encoder.predict(D_train))\n",
    "D_clus_pre = np.copy(D_clus)\n",
    "DEC.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4014/4014 [==============================] - 0s 103us/step - loss: 16689086751.7608\n",
      "Epoch 2/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 16689074068.8550\n",
      "Epoch 3/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 16688781994.6667\n",
      "Epoch 4/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 16683137831.6692\n",
      "Epoch 5/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 16612528165.2456\n",
      "Epoch 6/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 16008070596.5600\n",
      "Epoch 7/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 12368313281.7539\n",
      "Epoch 8/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 3380391336.6258\n",
      "Epoch 9/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 1241222215.1430\n",
      "Epoch 10/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 696836785.7459\n",
      "Epoch 11/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 486592262.5371\n",
      "Epoch 12/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 357757927.4938\n",
      "Epoch 13/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 277883729.3632\n",
      "Epoch 14/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 226897308.4365\n",
      "Epoch 15/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 203901211.4479\n",
      "Epoch 16/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 180010539.6034\n",
      "Epoch 17/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 172232732.3488\n",
      "Epoch 18/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 156329281.9292\n",
      "Epoch 19/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 143096289.5785\n",
      "Epoch 20/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 138723821.8117\n",
      "Epoch 21/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 132082317.2177\n",
      "Epoch 22/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 139470733.7120\n",
      "Epoch 23/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 123732149.7519\n",
      "Epoch 24/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 124129358.5012\n",
      "Epoch 25/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 120165325.1460\n",
      "Epoch 26/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 107882044.6358\n",
      "Epoch 27/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 113381244.7035\n",
      "Epoch 28/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 111558606.3976\n",
      "Epoch 29/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 102658573.5366\n",
      "Epoch 30/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 95608645.4888\n",
      "Epoch 31/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 104982083.4300\n",
      "Epoch 32/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 93074577.9173\n",
      "Epoch 33/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 98189543.2068\n",
      "Epoch 34/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 94354900.3368\n",
      "Epoch 35/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 90423646.7922\n",
      "Epoch 36/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 87551494.5491\n",
      "Epoch 37/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 84729847.5157\n",
      "Epoch 38/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 88480871.0095\n",
      "Epoch 39/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 80813019.7070\n",
      "Epoch 40/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 83466924.2073\n",
      "Epoch 41/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 78230086.0867\n",
      "Epoch 42/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 72818093.7718\n",
      "Epoch 43/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 74190196.2252\n",
      "Epoch 44/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 71657118.6826\n",
      "Epoch 45/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 66357341.8196\n",
      "Epoch 46/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 69021675.2407\n",
      "Epoch 47/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 68739910.6208\n",
      "Epoch 48/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 62059651.2247\n",
      "Epoch 49/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 62446559.4778\n",
      "Epoch 50/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 68484534.7364\n",
      "Epoch 51/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 68071209.4031\n",
      "Epoch 52/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 63191617.6323\n",
      "Epoch 53/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 68369729.4469\n",
      "Epoch 54/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 52157794.4714\n",
      "Epoch 55/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 58407745.3592\n",
      "Epoch 56/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 57745199.9283\n",
      "Epoch 57/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 54878228.0897\n",
      "Epoch 58/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 66904616.4743\n",
      "Epoch 59/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 49523451.3024\n",
      "Epoch 60/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 63601297.0603\n",
      "Epoch 61/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 65283962.4674\n",
      "Epoch 62/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 51373924.1256\n",
      "Epoch 63/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 46749694.3298\n",
      "Epoch 64/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 47625975.4778\n",
      "Epoch 65/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 45561883.4778\n",
      "Epoch 66/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 47501136.3707\n",
      "Epoch 67/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 50212857.1819\n",
      "Epoch 68/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 41516829.6462\n",
      "Epoch 69/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 51012731.7678\n",
      "Epoch 70/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 44851802.7564\n",
      "Epoch 71/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 44998915.1410\n",
      "Epoch 72/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 49018206.7165\n",
      "Epoch 73/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 43667128.8411\n",
      "Epoch 74/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 54035133.7220\n",
      "Epoch 75/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 77277649.7459\n",
      "Epoch 76/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 45331390.6408\n",
      "Epoch 77/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 43606848.1515\n",
      "Epoch 78/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 40174250.6786\n",
      "Epoch 79/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 44676682.8082\n",
      "Epoch 80/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 37587657.2616\n",
      "Epoch 81/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 37148385.8097\n",
      "Epoch 82/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 34982553.7459\n",
      "Epoch 83/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 34886548.1276\n",
      "Epoch 84/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 37941551.9841\n",
      "Epoch 85/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 39340020.5840\n",
      "Epoch 86/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 40957487.0872\n",
      "Epoch 87/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 39966229.9691\n",
      "Epoch 88/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 39914976.2810\n",
      "Epoch 89/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 35304751.6253\n",
      "Epoch 90/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 34572814.4355\n",
      "Epoch 91/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 33464652.6866\n",
      "Epoch 92/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 35747526.3876\n",
      "Epoch 93/300\n",
      "4014/4014 [==============================] - 0s 20us/step - loss: 41123496.5800\n",
      "Epoch 94/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 31081153.9133\n",
      "Epoch 95/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 33999421.7957\n",
      "Epoch 96/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 32615645.2207\n",
      "Epoch 97/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 35637901.8874\n",
      "Epoch 98/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 36870241.1300\n",
      "Epoch 99/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 30112066.2920\n",
      "Epoch 100/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 31021312.9985\n",
      "Epoch 101/300\n",
      "4014/4014 [==============================] - 0s 20us/step - loss: 33202947.1011\n",
      "Epoch 102/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 38457538.7504\n",
      "Epoch 103/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 38381853.9611\n",
      "Epoch 104/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 32953579.2686\n",
      "Epoch 105/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 32790647.7509\n",
      "Epoch 106/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 30246932.1614\n",
      "Epoch 107/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 46495882.4833\n",
      "Epoch 108/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 41614019.3622\n",
      "Epoch 109/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 28003090.2691\n",
      "Epoch 110/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 30158041.1888\n",
      "Epoch 111/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 33247081.9312\n",
      "Epoch 112/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 36542938.2556\n",
      "Epoch 113/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 33197174.7065\n",
      "Epoch 114/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 37994529.0752\n",
      "Epoch 115/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 25413937.0164\n",
      "Epoch 116/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 27329002.7195\n",
      "Epoch 117/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 35762851.3343\n",
      "Epoch 118/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 28361400.7942\n",
      "Epoch 119/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 26623919.4280\n",
      "Epoch 120/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 29546277.7559\n",
      "Epoch 121/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 28525537.3293\n",
      "Epoch 122/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 26580250.1545\n",
      "Epoch 123/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 23331191.3981\n",
      "Epoch 124/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 34810844.7334\n",
      "Epoch 125/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 28160977.3014\n",
      "Epoch 126/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 37005983.4898\n",
      "Epoch 127/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 38251787.0593\n",
      "Epoch 128/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 27639404.9736\n",
      "Epoch 129/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 25348063.1011\n",
      "Epoch 130/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 25952522.8789\n",
      "Epoch 131/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 25743446.7065\n",
      "Epoch 132/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 31020170.5949\n",
      "Epoch 133/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 28900252.1814\n",
      "Epoch 134/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 36683825.4150\n",
      "Epoch 135/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 22608138.8680\n",
      "Epoch 136/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 25336218.4395\n",
      "Epoch 137/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 29309044.2033\n",
      "Epoch 138/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 23921989.0573\n",
      "Epoch 139/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 27418331.9581\n",
      "Epoch 140/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 24136052.5401\n",
      "Epoch 141/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 24955160.1286\n",
      "Epoch 142/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 24127885.2845\n",
      "Epoch 143/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 26568049.1320\n",
      "Epoch 144/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 23597034.7364\n",
      "Epoch 145/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 25712205.2765\n",
      "Epoch 146/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 24084234.0389\n",
      "Epoch 147/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 26297757.9686\n",
      "Epoch 148/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 23888012.1893\n",
      "Epoch 149/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 19914138.1415\n",
      "Epoch 150/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 18975067.0214\n",
      "Epoch 151/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 20514644.6288\n",
      "Epoch 152/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 22784422.1674\n",
      "Epoch 153/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 21259982.0568\n",
      "Epoch 154/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 18607766.4325\n",
      "Epoch 155/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 27108516.6487\n",
      "Epoch 156/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 17852529.9482\n",
      "Epoch 157/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 16983024.9347\n",
      "Epoch 158/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 22369559.5247\n",
      "Epoch 159/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 35812832.8665\n",
      "Epoch 160/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 24730320.5361\n",
      "Epoch 161/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17697338.9447\n",
      "Epoch 162/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 24865329.6801\n",
      "Epoch 163/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 26977936.4564\n",
      "Epoch 164/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 22128729.3194\n",
      "Epoch 165/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 20624950.7334\n",
      "Epoch 166/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 20891380.5521\n",
      "Epoch 167/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 22271669.2157\n",
      "Epoch 168/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 18963594.5366\n",
      "Epoch 169/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 21345032.7190\n",
      "Epoch 170/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 20486249.6413\n",
      "Epoch 171/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 20273117.4121\n",
      "Epoch 172/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 22266885.6652\n",
      "Epoch 173/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 16220222.4205\n",
      "Epoch 174/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 16637398.2511\n",
      "Epoch 175/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 16us/step - loss: 26266448.7733\n",
      "Epoch 176/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 19681905.3204\n",
      "Epoch 177/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 16821310.5750\n",
      "Epoch 178/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 17092442.1754\n",
      "Epoch 179/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 25542057.6821\n",
      "Epoch 180/300\n",
      "4014/4014 [==============================] - 0s 19us/step - loss: 24664021.6627\n",
      "Epoch 181/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 21725197.4778\n",
      "Epoch 182/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 26347324.1649\n",
      "Epoch 183/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 18126667.3259\n",
      "Epoch 184/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 19637299.4579\n",
      "Epoch 185/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 18718230.8117\n",
      "Epoch 186/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 18813260.0189\n",
      "Epoch 187/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 16907844.2491\n",
      "Epoch 188/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 13731072.4704\n",
      "Epoch 189/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 20215633.5047\n",
      "Epoch 190/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 18795119.8356\n",
      "Epoch 191/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 18522649.0618\n",
      "Epoch 192/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17670459.2935\n",
      "Epoch 193/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 16618939.5456\n",
      "Epoch 194/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 19208889.2995\n",
      "Epoch 195/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 20702764.4723\n",
      "Epoch 196/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 14772718.6692\n",
      "Epoch 197/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 18328589.0842\n",
      "Epoch 198/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 22059246.0653\n",
      "Epoch 199/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 23777984.7165\n",
      "Epoch 200/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 20667768.5650\n",
      "Epoch 201/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 14033595.5277\n",
      "Epoch 202/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 17343106.8762\n",
      "Epoch 203/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 19627516.1774\n",
      "Epoch 204/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17852514.2760\n",
      "Epoch 205/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 18922167.8739\n",
      "Epoch 206/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 20395920.1006\n",
      "Epoch 207/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 18050353.7339\n",
      "Epoch 208/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15209360.5610\n",
      "Epoch 209/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 18128949.1749\n",
      "Epoch 210/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17718310.6019\n",
      "Epoch 211/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 16259660.5361\n",
      "Epoch 212/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 13260637.3333\n",
      "Epoch 213/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 21100225.8137\n",
      "Epoch 214/300\n",
      "4014/4014 [==============================] - 0s 18us/step - loss: 13088890.1435\n",
      "Epoch 215/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 13555270.8580\n",
      "Epoch 216/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15584891.0174\n",
      "Epoch 217/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15562998.3279\n",
      "Epoch 218/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15598113.4828\n",
      "Epoch 219/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 16130394.3508\n",
      "Epoch 220/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 12742107.3338\n",
      "Epoch 221/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 14278752.0309\n",
      "Epoch 222/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12955553.4611\n",
      "Epoch 223/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 16619088.6906\n",
      "Epoch 224/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17619679.1774\n",
      "Epoch 225/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 14320134.2753\n",
      "Epoch 226/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17641510.7653\n",
      "Epoch 227/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 18007055.7469\n",
      "Epoch 228/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 13838692.3627\n",
      "Epoch 229/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 18160936.8759\n",
      "Epoch 230/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17332892.5840\n",
      "Epoch 231/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 17322062.6841\n",
      "Epoch 232/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15426152.5421\n",
      "Epoch 233/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 13556383.7549\n",
      "Epoch 234/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 14370270.3707\n",
      "Epoch 235/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 19230272.3876\n",
      "Epoch 236/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 15672194.4564\n",
      "Epoch 237/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15708200.4420\n",
      "Epoch 238/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11969332.4743\n",
      "Epoch 239/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 13882016.6647\n",
      "Epoch 240/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 14127513.4325\n",
      "Epoch 241/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 18001854.0399\n",
      "Epoch 242/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12837798.0478\n",
      "Epoch 243/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11062464.7070\n",
      "Epoch 244/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 10029409.3881\n",
      "Epoch 245/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15037264.7813\n",
      "Epoch 246/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17145364.3438\n",
      "Epoch 247/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15305407.2008\n",
      "Epoch 248/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 12994866.0977\n",
      "Epoch 249/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 12838810.9801\n",
      "Epoch 250/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15504362.3717\n",
      "Epoch 251/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 16133728.4798\n",
      "Epoch 252/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 13019964.1338\n",
      "Epoch 253/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 14806144.7215\n",
      "Epoch 254/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12096867.2466\n",
      "Epoch 255/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12994509.2526\n",
      "Epoch 256/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11154206.5067\n",
      "Epoch 257/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 14750148.5316\n",
      "Epoch 258/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 10483092.2725\n",
      "Epoch 259/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 9429985.6672\n",
      "Epoch 260/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 16331090.0932\n",
      "Epoch 261/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 19223395.3443\n",
      "Epoch 262/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15663254.3478\n",
      "Epoch 263/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 16269986.4220\n",
      "Epoch 264/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15133882.9699\n",
      "Epoch 265/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12832259.6612\n",
      "Epoch 266/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 10325062.4599\n",
      "Epoch 267/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 10626981.8814\n",
      "Epoch 268/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 12358237.9033\n",
      "Epoch 269/300\n",
      "4014/4014 [==============================] - 0s 17us/step - loss: 19484282.7843\n",
      "Epoch 270/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11551120.0329\n",
      "Epoch 271/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 13324972.7359\n",
      "Epoch 272/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 11709669.9153\n",
      "Epoch 273/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 9719879.3343\n",
      "Epoch 274/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 10379756.1011\n",
      "Epoch 275/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 15081657.5087\n",
      "Epoch 276/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 14128596.8665\n",
      "Epoch 277/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11403495.5431\n",
      "Epoch 278/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 10904418.0110\n",
      "Epoch 279/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 17484367.3254\n",
      "Epoch 280/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 10419519.0140\n",
      "Epoch 281/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 14632312.9278\n",
      "Epoch 282/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12541879.7205\n",
      "Epoch 283/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 10060121.7389\n",
      "Epoch 284/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 10043115.2309\n",
      "Epoch 285/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 8768065.2688\n",
      "Epoch 286/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 14190317.7100\n",
      "Epoch 287/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11731071.1642\n",
      "Epoch 288/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11949254.6916\n",
      "Epoch 289/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 11094109.5800\n",
      "Epoch 290/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12380490.6861\n",
      "Epoch 291/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 10465642.5727\n",
      "Epoch 292/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 10529022.3089\n",
      "Epoch 293/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 9770563.4360\n",
      "Epoch 294/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 12997594.4738\n",
      "Epoch 295/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 14489270.0987\n",
      "Epoch 296/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 9425633.3423\n",
      "Epoch 297/300\n",
      "4014/4014 [==============================] - 0s 14us/step - loss: 10536359.5561\n",
      "Epoch 298/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 11476555.6677\n",
      "Epoch 299/300\n",
      "4014/4014 [==============================] - 0s 16us/step - loss: 12866412.2711\n",
      "Epoch 300/300\n",
      "4014/4014 [==============================] - 0s 15us/step - loss: 13198215.8012\n"
     ]
    }
   ],
   "source": [
    "AE.compile(optimizer='Adam', loss='mse')\n",
    "AE.fit(D_train, D_train, batch_size=256, epochs=300)\n",
    "\n",
    "AE.save_weights(model_path + '/AEweights')\n",
    "encoder.save_weights(model_path + '/encoderweights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train DEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "1.0789934e-06\n",
      "1.2301109e-06\n",
      "1.091711e-06\n",
      "1.0711541e-06\n",
      "7.992403e-07\n",
      "1.2926972e-06\n",
      "7.602603e-07\n",
      "1.0505088e-06\n",
      "9.627074e-07\n",
      "9.170974e-07\n",
      "9.2404946e-07\n",
      "1.4112046e-06\n",
      "9.408083e-07\n",
      "8.0947905e-07\n",
      "1.0727928e-06\n",
      "1.3737771e-06\n",
      "6.5936e-07\n",
      "7.455347e-07\n",
      "7.8446345e-07\n",
      "7.86424e-07\n",
      "7.4475486e-07\n",
      "6.621241e-07\n",
      "1.3381857e-06\n",
      "5.588533e-07\n",
      "5.5384703e-07\n",
      "5.132129e-07\n",
      "6.6037245e-07\n",
      "1.1101557e-06\n",
      "7.624848e-07\n",
      "6.089522e-07\n",
      "8.485712e-07\n",
      "1.1142753e-06\n",
      "4.8062475e-07\n",
      "5.6427064e-07\n",
      "7.0717533e-07\n",
      "7.7693164e-07\n",
      "9.500586e-07\n",
      "5.127773e-07\n",
      "1.8797487e-06\n",
      "4.5800607e-07\n",
      "4.594509e-07\n",
      "5.1545067e-07\n",
      "5.683202e-07\n",
      "9.652808e-07\n",
      "7.1405077e-07\n",
      "5.810243e-07\n",
      "9.1463585e-07\n",
      "1.1483044e-06\n",
      "4.5000473e-07\n",
      "4.967123e-07\n",
      "6.5872655e-07\n",
      "6.658252e-07\n",
      "7.974528e-07\n",
      "5.0170337e-07\n",
      "1.7380049e-06\n",
      "4.4750652e-07\n",
      "4.246453e-07\n",
      "4.216335e-07\n",
      "4.6844184e-07\n",
      "8.020785e-07\n",
      "6.180953e-07\n",
      "5.1853795e-07\n",
      "9.1631955e-07\n",
      "1.1199447e-06\n",
      "4.1963835e-07\n",
      "4.4908109e-07\n",
      "6.347947e-07\n",
      "5.816506e-07\n",
      "6.7294104e-07\n",
      "4.5288755e-07\n",
      "1.6031659e-06\n",
      "4.0908157e-07\n",
      "3.8444927e-07\n",
      "3.8190478e-07\n",
      "4.139903e-07\n",
      "6.963939e-07\n",
      "5.4512253e-07\n",
      "4.5719406e-07\n",
      "8.4807675e-07\n",
      "1.0281016e-06\n",
      "3.7517327e-07\n",
      "3.935084e-07\n",
      "6.2255833e-07\n",
      "5.4963715e-07\n",
      "6.686126e-07\n",
      "3.9855334e-07\n",
      "1.6187948e-06\n",
      "3.5355063e-07\n",
      "3.438709e-07\n",
      "3.4717888e-07\n",
      "3.846022e-07\n",
      "6.300873e-07\n",
      "5.0785826e-07\n",
      "4.2815412e-07\n",
      "8.301259e-07\n",
      "9.705021e-07\n",
      "3.3410222e-07\n",
      "3.5367634e-07\n",
      "6.1771084e-07\n",
      "5.3607437e-07\n",
      "2.3452317e-06\n",
      "4.2395905e-06\n",
      "1.5439825e-06\n",
      "3.1863606e-06\n",
      "2.8658258e-06\n",
      "2.4864141e-06\n",
      "2.5039853e-06\n",
      "2.181604e-06\n",
      "1.9886315e-06\n",
      "1.6608623e-06\n",
      "2.0289444e-06\n",
      "2.7226397e-06\n",
      "1.0695774e-06\n",
      "1.4438754e-06\n",
      "1.4758007e-06\n",
      "1.9542208e-06\n",
      "2.746192e-06\n",
      "9.4897405e-07\n",
      "4.3604123e-06\n",
      "1.077045e-06\n",
      "1.5722808e-06\n",
      "2.7087772e-06\n",
      "2.1142605e-06\n",
      "1.9683996e-06\n",
      "1.8156553e-06\n",
      "1.1038929e-06\n",
      "2.0653265e-06\n",
      "2.1475114e-06\n",
      "8.493555e-07\n",
      "1.136615e-06\n",
      "2.4499127e-06\n",
      "2.5201323e-06\n",
      "2.8382713e-06\n",
      "1.1284374e-06\n",
      "4.511895e-06\n",
      "8.420494e-07\n",
      "9.775262e-07\n",
      "1.264338e-06\n",
      "1.3682943e-06\n",
      "1.6963502e-06\n",
      "1.8981546e-06\n",
      "1.4430807e-06\n",
      "3.1304014e-06\n",
      "3.0681665e-06\n",
      "7.723335e-07\n",
      "7.2376935e-07\n",
      "1.5952072e-06\n",
      "1.5504972e-06\n",
      "2.4549797e-06\n",
      "9.103838e-07\n",
      "5.493748e-06\n",
      "7.5051884e-07\n",
      "7.4475753e-07\n",
      "6.603113e-07\n",
      "8.1525394e-07\n",
      "1.1667145e-06\n",
      "1.3775755e-06\n",
      "1.1572498e-06\n",
      "2.9038986e-06\n",
      "3.1213974e-06\n",
      "7.3757974e-07\n",
      "6.67696e-07\n",
      "1.2540849e-06\n",
      "1.1406642e-06\n",
      "1.8748153e-06\n",
      "6.4335177e-07\n",
      "4.9847927e-06\n",
      "6.2571473e-07\n",
      "6.7874566e-07\n",
      "6.436488e-07\n",
      "6.2729475e-07\n",
      "9.620694e-07\n",
      "1.0504026e-06\n",
      "8.8941357e-07\n",
      "2.524909e-06\n",
      "2.784198e-06\n",
      "6.6770565e-07\n",
      "6.216325e-07\n",
      "1.2384413e-06\n",
      "1.0288745e-06\n",
      "1.5944145e-06\n",
      "6.160813e-07\n",
      "4.41658e-06\n",
      "5.5116334e-07\n",
      "6.3689663e-07\n",
      "5.9904306e-07\n",
      "5.986031e-07\n",
      "8.791515e-07\n",
      "9.671171e-07\n",
      "8.0069316e-07\n",
      "2.3520276e-06\n",
      "2.5407408e-06\n",
      "5.879937e-07\n",
      "5.626228e-07\n",
      "1.2181289e-06\n",
      "1.0281922e-06\n",
      "1.687013e-06\n",
      "5.59332e-07\n",
      "4.418033e-06\n",
      "4.9399625e-07\n",
      "1.1598313e-05\n",
      "1.1841762e-05\n",
      "1.1949194e-05\n",
      "7.5489884e-06\n",
      "8.094663e-06\n",
      "6.7003266e-06\n",
      "8.503238e-06\n",
      "9.539946e-06\n",
      "3.943663e-06\n",
      "5.3044923e-06\n",
      "6.623418e-06\n",
      "8.990409e-06\n",
      "1.1724392e-05\n",
      "3.1089442e-06\n",
      "1.0653569e-05\n",
      "2.6620905e-06\n",
      "5.1293896e-06\n",
      "7.1455142e-06\n",
      "8.765063e-06\n",
      "6.908141e-06\n",
      "8.000248e-06\n",
      "5.2518494e-06\n",
      "1.0623193e-05\n",
      "8.615733e-06\n",
      "1.8959059e-06\n",
      "2.209169e-06\n",
      "7.66712e-06\n",
      "9.051901e-06\n",
      "1.1320225e-05\n",
      "3.2420414e-06\n",
      "1.6095526e-05\n",
      "2.1868157e-06\n",
      "2.8064533e-06\n",
      "3.213182e-06\n",
      "3.844902e-06\n",
      "3.7420764e-06\n",
      "5.7071657e-06\n",
      "5.131782e-06\n",
      "1.1730162e-05\n",
      "1.1351733e-05\n",
      "2.4440035e-06\n",
      "1.9313543e-06\n",
      "3.512139e-06\n",
      "3.091949e-06\n",
      "6.900411e-06\n",
      "1.937016e-06\n",
      "1.6081485e-05\n",
      "2.00462e-06\n",
      "2.0156047e-06\n",
      "1.8257217e-06\n",
      "1.7210403e-06\n",
      "2.0438497e-06\n",
      "2.883884e-06\n",
      "2.6766309e-06\n",
      "9.0640715e-06\n",
      "9.670981e-06\n",
      "1.9194451e-06\n",
      "1.5946023e-06\n",
      "3.2008688e-06\n",
      "2.7910337e-06\n",
      "4.4312023e-06\n",
      "1.4290881e-06\n",
      "1.2543851e-05\n",
      "1.3390438e-06\n",
      "1.5394833e-06\n",
      "1.6970157e-06\n",
      "1.4799728e-06\n",
      "1.7536165e-06\n",
      "2.2988188e-06\n",
      "2.005421e-06\n",
      "7.3568112e-06\n",
      "7.603543e-06\n",
      "1.4564322e-06\n",
      "1.3152303e-06\n",
      "3.0667175e-06\n",
      "2.6220223e-06\n",
      "4.9879554e-06\n",
      "1.2271527e-06\n",
      "1.1920381e-05\n",
      "1.190777e-06\n",
      "1.4408547e-06\n",
      "1.4132305e-06\n",
      "1.7660816e-06\n",
      "1.789128e-06\n",
      "2.4491299e-06\n",
      "2.088007e-06\n",
      "7.639646e-06\n",
      "7.649236e-06\n",
      "1.3864437e-06\n",
      "1.2082411e-06\n",
      "3.0519648e-06\n",
      "2.7915082e-06\n",
      "4.9585433e-06\n",
      "1.2548513e-06\n",
      "1.2182223e-05\n",
      "1.1847859e-06\n",
      "1.320831e-06\n",
      "1.404024e-06\n",
      "1.597632e-06\n",
      "1.7167729e-06\n",
      "3.144815e-05\n",
      "3.4900975e-05\n",
      "4.7305497e-05\n",
      "5.4118984e-05\n",
      "1.534986e-05\n",
      "1.4638723e-05\n",
      "1.6029275e-05\n",
      "1.975163e-05\n",
      "2.7823764e-05\n",
      "1.2463333e-05\n",
      "3.9832812e-05\n",
      "5.661894e-06\n",
      "6.717566e-06\n",
      "5.4665734e-06\n",
      "1.1924467e-05\n",
      "8.159631e-06\n",
      "1.2871537e-05\n",
      "1.0716526e-05\n",
      "2.9628207e-05\n",
      "2.5002259e-05\n",
      "5.0426224e-06\n",
      "4.1216053e-06\n",
      "1.6072285e-05\n",
      "1.945908e-05\n",
      "3.0337136e-05\n",
      "3.4661102e-06\n",
      "3.1860425e-05\n",
      "3.6617555e-06\n",
      "7.532857e-06\n",
      "5.254788e-06\n",
      "1.5103459e-05\n",
      "1.0000789e-05\n",
      "1.4073183e-05\n",
      "1.1165146e-05\n",
      "3.030742e-05\n",
      "2.5359446e-05\n",
      "4.202512e-06\n",
      "3.2423948e-06\n",
      "1.2566922e-05\n",
      "1.5555059e-05\n",
      "2.643004e-05\n",
      "6.71483e-06\n",
      "4.0051564e-05\n",
      "4.315888e-06\n",
      "4.6897335e-06\n",
      "3.7407897e-06\n",
      "9.003124e-06\n",
      "6.9964276e-06\n",
      "1.1813974e-05\n",
      "1.1660179e-05\n",
      "3.21198e-05\n",
      "3.216179e-05\n",
      "5.2953037e-06\n",
      "3.5565924e-06\n",
      "7.831536e-06\n",
      "8.0134005e-06\n",
      "1.6197482e-05\n",
      "3.9452534e-06\n",
      "3.811404e-05\n",
      "4.1004705e-06\n",
      "3.480572e-06\n",
      "4.388295e-06\n",
      "4.215375e-06\n",
      "3.7968584e-06\n",
      "6.4062865e-06\n",
      "6.2436657e-06\n",
      "2.4381772e-05\n",
      "2.4121106e-05\n",
      "4.3370196e-06\n",
      "3.1743689e-06\n",
      "7.170574e-06\n",
      "6.210471e-06\n",
      "1.236284e-05\n",
      "2.5661848e-06\n",
      "3.0661104e-05\n",
      "2.8612913e-06\n",
      "3.171746e-06\n",
      "3.6957304e-06\n",
      "4.049292e-06\n",
      "3.4453374e-06\n",
      "4.995941e-06\n",
      "4.657787e-06\n",
      "2.0666972e-05\n",
      "1.9675743e-05\n",
      "3.196812e-06\n",
      "2.5385436e-06\n",
      "6.996548e-06\n",
      "6.7862256e-06\n",
      "1.3271278e-05\n",
      "2.3474213e-06\n",
      "2.8987399e-05\n",
      "2.5517936e-06\n",
      "2.8947845e-06\n",
      "3.294987e-06\n",
      "4.6788223e-06\n",
      "3.538815e-06\n",
      "5.5808578e-06\n",
      "4.96162e-06\n",
      "2.1587555e-05\n",
      "2.0087684e-05\n",
      "9.625288e-05\n",
      "9.309435e-05\n",
      "5.5056404e-05\n",
      "4.639773e-05\n",
      "4.4361655e-05\n",
      "4.361542e-05\n",
      "0.00011043511\n",
      "3.0249315e-05\n",
      "3.622708e-05\n",
      "3.107395e-05\n",
      "1.8613744e-05\n",
      "1.23494865e-05\n",
      "1.4470967e-05\n",
      "1.6632355e-05\n",
      "6.757474e-05\n",
      "7.4231524e-05\n",
      "1.8308305e-05\n",
      "1.2617749e-05\n",
      "5.27305e-05\n",
      "4.890512e-05\n",
      "6.707327e-05\n",
      "1.0298349e-05\n",
      "5.04543e-05\n",
      "2.2594215e-05\n",
      "4.1424497e-05\n",
      "2.7615295e-05\n",
      "7.109183e-05\n",
      "3.1293082e-05\n",
      "3.495779e-05\n",
      "2.2931603e-05\n",
      "5.9737962e-05\n",
      "4.419526e-05\n",
      "2.1510463e-05\n",
      "2.8725859e-05\n",
      "7.631364e-05\n",
      "8.415566e-05\n",
      "0.000105408995\n",
      "1.3923707e-05\n",
      "7.543572e-05\n",
      "1.2859482e-05\n",
      "2.9719735e-05\n",
      "2.1936588e-05\n",
      "7.060865e-05\n",
      "3.8960607e-05\n",
      "5.9957798e-05\n",
      "5.395122e-05\n",
      "0.00011979997\n",
      "0.00010768395\n",
      "1.3548092e-05\n",
      "8.482221e-06\n",
      "2.737778e-05\n",
      "4.7789406e-05\n",
      "8.6453314e-05\n",
      "3.0541545e-05\n",
      "0.00014082094\n",
      "1.6429247e-05\n",
      "1.6009857e-05\n",
      "1.1586625e-05\n",
      "2.0637845e-05\n",
      "1.4193908e-05\n",
      "2.9130753e-05\n",
      "3.7809194e-05\n",
      "0.00010734613\n",
      "0.000116022195\n",
      "1.869363e-05\n",
      "1.3338808e-05\n",
      "2.1005291e-05\n",
      "1.5673399e-05\n",
      "3.8711565e-05\n",
      "8.83425e-06\n",
      "0.00010240238\n",
      "1.2194277e-05\n",
      "1.27039675e-05\n",
      "1.5540045e-05\n",
      "8.070559e-06\n",
      "7.766124e-06\n",
      "1.0529892e-05\n",
      "1.2879446e-05\n",
      "6.7759385e-05\n",
      "6.680983e-05\n",
      "1.0205387e-05\n",
      "7.8914745e-06\n",
      "1.9128149e-05\n",
      "1.4943131e-05\n",
      "3.1220447e-05\n",
      "5.366513e-06\n",
      "7.5509655e-05\n",
      "5.9051495e-06\n",
      "6.4314436e-06\n",
      "9.503804e-06\n",
      "1.1338546e-05\n",
      "7.3883252e-06\n",
      "1.0365799e-05\n",
      "1.1608778e-05\n",
      "6.127519e-05\n",
      "5.7387253e-05\n",
      "6.9867942e-06\n",
      "5.524985e-06\n",
      "1.8547082e-05\n",
      "1.8261559e-05\n",
      "0.0003074116\n",
      "0.00045584902\n",
      "0.0001296273\n",
      "0.00023952937\n",
      "0.00024746874\n",
      "0.00020401736\n",
      "0.00016099808\n",
      "0.00021865711\n",
      "0.00013039747\n",
      "9.6980075e-05\n",
      "0.0001301878\n",
      "0.00018674818\n",
      "7.334679e-05\n",
      "4.5141824e-05\n",
      "0.00012463644\n",
      "9.430149e-05\n",
      "8.123547e-05\n",
      "7.669143e-05\n",
      "8.4383384e-05\n",
      "0.00014935624\n",
      "0.00020032297\n",
      "0.00015627622\n",
      "0.0002139299\n",
      "6.0102273e-05\n",
      "4.120607e-05\n",
      "3.5325334e-05\n",
      "9.799161e-05\n",
      "5.933499e-05\n",
      "0.0002094859\n",
      "0.00018158139\n",
      "0.0002768357\n",
      "0.0002035244\n",
      "0.00018145239\n",
      "2.4857782e-05\n",
      "8.65062e-05\n",
      "0.0001302955\n",
      "0.00021649129\n",
      "0.00021653503\n",
      "0.00038503675\n",
      "0.0001639429\n",
      "0.00020494348\n",
      "0.00016855533\n",
      "0.00038977657\n",
      "0.0002641513\n",
      "5.3081967e-05\n",
      "9.147791e-05\n",
      "0.00025800738\n",
      "0.00038287157\n",
      "0.00052206777\n",
      "0.00016456818\n",
      "0.00047579018\n",
      "3.984319e-05\n",
      "5.2361633e-05\n",
      "9.9879144e-05\n",
      "0.00019113555\n",
      "0.00011885709\n",
      "0.00021965544\n",
      "0.00026889748\n",
      "0.00063311536\n",
      "0.00059194205\n",
      "9.7408476e-05\n",
      "5.8785496e-05\n",
      "7.11458e-05\n",
      "8.567289e-05\n",
      "0.00021483182\n",
      "8.71732e-05\n",
      "0.0005358906\n",
      "0.00011017379\n",
      "7.2386945e-05\n",
      "0.00014027959\n",
      "3.397828e-05\n",
      "2.5172454e-05\n",
      "3.9876206e-05\n",
      "7.516813e-05\n",
      "0.00035836\n",
      "0.00036975608\n",
      "6.536067e-05\n",
      "5.8520272e-05\n",
      "6.937096e-05\n",
      "3.8205748e-05\n",
      "8.950303e-05\n",
      "1.7890909e-05\n",
      "0.0002784069\n",
      "2.9655203e-05\n",
      "3.83806e-05\n",
      "9.890516e-05\n",
      "3.3686127e-05\n",
      "2.4270059e-05\n",
      "2.474435e-05\n",
      "4.350563e-05\n",
      "0.00024845655\n",
      "0.0002190283\n",
      "2.6317626e-05\n",
      "2.1448523e-05\n",
      "6.196645e-05\n",
      "6.226855e-05\n",
      "0.00012938536\n",
      "1.9584573e-05\n",
      "0.0002734154\n",
      "2.3242392e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016887335\n",
      "0.001213\n",
      "0.0020187332\n",
      "0.0008463235\n",
      "0.00097050105\n",
      "0.0008591281\n",
      "0.0012879022\n",
      "0.0012950266\n",
      "0.0007879053\n",
      "0.0009829331\n",
      "0.0014919075\n",
      "0.0013293299\n",
      "0.0011628878\n",
      "0.00025891687\n",
      "0.0006971179\n",
      "0.00025751747\n",
      "0.00066658703\n",
      "0.0006082321\n",
      "0.0012934897\n",
      "0.0006168204\n",
      "0.0008140544\n",
      "0.00073989556\n",
      "0.0018935901\n",
      "0.0012134821\n",
      "0.00014464889\n",
      "0.000237007\n",
      "0.00104676\n",
      "0.0012047901\n",
      "0.0015537797\n",
      "0.000581422\n",
      "0.0017478253\n",
      "0.0002844746\n",
      "0.00032772016\n",
      "0.00030665108\n",
      "0.00046848773\n",
      "0.0002479486\n",
      "0.0005721449\n",
      "0.00081291073\n",
      "0.0025130496\n",
      "0.0021731004\n",
      "0.00036376022\n",
      "0.00028309374\n",
      "0.00023257801\n",
      "0.00021263944\n",
      "0.0004593681\n",
      "0.00023993933\n",
      "0.0013622062\n",
      "0.0003782755\n",
      "0.0005025198\n",
      "0.00061834016\n",
      "0.00020480882\n",
      "0.0003061063\n",
      "0.00015748115\n",
      "0.00027170448\n",
      "0.0012284697\n",
      "0.0012808185\n",
      "0.00017863562\n",
      "0.00019035907\n",
      "0.00022217624\n",
      "0.00012760375\n",
      "0.00023589724\n",
      "0.00012566862\n",
      "0.0007510771\n",
      "0.00014148746\n",
      "0.00029986174\n",
      "0.00033874105\n",
      "0.00020714484\n",
      "0.00018555165\n",
      "0.0001476173\n",
      "0.00024158657\n",
      "0.0011318733\n",
      "0.0010233348\n",
      "0.00011209484\n",
      "9.4688854e-05\n",
      "0.00023691927\n",
      "0.00022671255\n",
      "0.0004038851\n",
      "0.00014287859\n",
      "0.0009045566\n",
      "0.00015489139\n",
      "0.0002925421\n",
      "0.000317942\n",
      "0.00023616775\n",
      "0.00015075358\n",
      "0.00018075276\n",
      "0.00030082755\n",
      "0.0013460215\n",
      "0.0011799869\n",
      "0.00012848667\n",
      "0.00011193367\n",
      "0.00019106147\n",
      "0.00019474728\n",
      "0.00037661256\n",
      "0.0001470889\n",
      "0.0009468849\n",
      "0.00018229392\n",
      "0.00030914013\n",
      "0.0003557548\n",
      "0.00019693575\n",
      "0.00017125889\n",
      "0.0051271273\n",
      "0.0059784907\n",
      "0.00904117\n",
      "0.0076650484\n",
      "0.0032854185\n",
      "0.002543601\n",
      "0.0020704188\n",
      "0.0029159863\n",
      "0.0038810968\n",
      "0.0026569841\n",
      "0.004186013\n",
      "0.0021704405\n",
      "0.0013252753\n",
      "0.0013239632\n",
      "0.0016894401\n",
      "0.00077587785\n",
      "0.0016264571\n",
      "0.0022778055\n",
      "0.006731346\n",
      "0.0047962544\n",
      "0.0011699137\n",
      "0.0008559363\n",
      "0.0012706278\n",
      "0.0016907252\n",
      "0.0025375308\n",
      "0.0015444786\n",
      "0.00366751\n",
      "0.0013526289\n",
      "0.00090320315\n",
      "0.0009245877\n",
      "0.0016506794\n",
      "0.0007738145\n",
      "0.0014525205\n",
      "0.0019359383\n",
      "0.006105775\n",
      "0.0044558183\n",
      "0.0011532692\n",
      "0.00082701806\n",
      "0.00076979934\n",
      "0.0011546059\n",
      "0.0018787888\n",
      "0.001299863\n",
      "0.0030662278\n",
      "0.0014217657\n",
      "0.0009243146\n",
      "0.0011635977\n",
      "0.0014185366\n",
      "0.00078501273\n",
      "0.0011838861\n",
      "0.0016867305\n",
      "0.0049999054\n",
      "0.0042049573\n",
      "0.0012171408\n",
      "0.00095013186\n",
      "0.0006495618\n",
      "0.00080367364\n",
      "0.0012685468\n",
      "0.0009958042\n",
      "0.0024180417\n",
      "0.0011464334\n",
      "0.00081429863\n",
      "0.0010125677\n",
      "0.0013341883\n",
      "0.00080207526\n",
      "0.0010592984\n",
      "0.0014477612\n",
      "0.0042447057\n",
      "0.0036458918\n",
      "0.0010562136\n",
      "0.00081179215\n",
      "0.00067719497\n",
      "0.0008296446\n",
      "0.0012758111\n",
      "0.0009591419\n",
      "0.0022795063\n",
      "0.0010275809\n",
      "0.0007465498\n",
      "0.0008769439\n",
      "0.0013564585\n",
      "0.0007380764\n",
      "0.0010872497\n",
      "0.0014670864\n",
      "0.004273386\n",
      "0.0036260085\n",
      "0.001042574\n",
      "0.0007894816\n",
      "0.0006584755\n",
      "0.0008467635\n",
      "0.0013193495\n",
      "0.0009737498\n",
      "0.0023100749\n",
      "0.0010578375\n",
      "0.0007490112\n",
      "0.0008904237\n",
      "0.0013296772\n",
      "0.0007322966\n",
      "0.0010797263\n",
      "0.0014612343\n",
      "0.0042176023\n",
      "0.0036390861\n",
      "0.013214895\n",
      "0.012787075\n",
      "0.008543407\n",
      "0.008508882\n",
      "0.009813062\n",
      "0.007759466\n",
      "0.009371583\n",
      "0.0076505914\n",
      "0.005597935\n",
      "0.0051037227\n",
      "0.0059954696\n",
      "0.0030940068\n",
      "0.0048414064\n",
      "0.0060982257\n",
      "0.016084306\n",
      "0.009146592\n",
      "0.004157002\n",
      "0.0040433705\n",
      "0.0069320025\n",
      "0.007968273\n",
      "0.009481302\n",
      "0.004379069\n",
      "0.0073669134\n",
      "0.0033045006\n",
      "0.004460593\n",
      "0.0035070179\n",
      "0.0073526967\n",
      "0.003017743\n",
      "0.0054559144\n",
      "0.005650655\n",
      "0.012303501\n",
      "0.007370232\n",
      "0.0046136384\n",
      "0.004612714\n",
      "0.0063354988\n",
      "0.0084293205\n",
      "0.01038564\n",
      "0.004597632\n",
      "0.007305327\n",
      "0.0038944778\n",
      "0.0030040706\n",
      "0.0026230249\n",
      "0.006214103\n",
      "0.003401574\n",
      "0.006071939\n",
      "0.007216093\n",
      "0.017690428\n",
      "0.010179264\n",
      "0.0037351851\n",
      "0.0031380407\n",
      "0.003944398\n",
      "0.0061680274\n",
      "0.009348864\n",
      "0.0051740413\n",
      "0.009708614\n",
      "0.0055094133\n",
      "0.0034622648\n",
      "0.0031280378\n",
      "0.004129118\n",
      "0.00222321\n",
      "0.0044604796\n",
      "0.0059659677\n",
      "0.018233404\n",
      "0.011192785\n",
      "0.0042165066\n",
      "0.003704472\n",
      "0.0026020585\n",
      "0.0036245887\n",
      "0.0059214467\n",
      "0.0035734752\n",
      "0.007930623\n",
      "0.005043953\n",
      "0.003600872\n",
      "0.0037861236\n",
      "0.0040100473\n",
      "0.0025501284\n",
      "0.0033832171\n",
      "0.004493606\n",
      "0.013474977\n",
      "0.009080183\n",
      "0.0038973428\n",
      "0.003513762\n",
      "0.0024882457\n",
      "0.0030494805\n",
      "0.004473894\n",
      "0.0027835702\n",
      "0.0062293117\n",
      "0.0038201697\n",
      "0.0028100181\n",
      "0.0029929737\n",
      "0.003810905\n",
      "0.0023629037\n",
      "0.003241299\n",
      "0.004110589\n",
      "0.011730558\n",
      "0.007802468\n",
      "0.0035440589\n",
      "0.0030179918\n",
      "0.0024876506\n",
      "0.0031661713\n",
      "0.026608916\n",
      "0.029438697\n",
      "0.023181954\n",
      "0.022490226\n",
      "0.02067706\n",
      "0.019410588\n",
      "0.019830307\n",
      "0.014938033\n",
      "0.0131781725\n",
      "0.013665244\n",
      "0.026713653\n",
      "0.015562761\n",
      "0.0152803995\n",
      "0.013606137\n",
      "0.015573825\n",
      "0.014585889\n",
      "0.014404532\n",
      "0.01613065\n",
      "0.0109959785\n",
      "0.015074849\n",
      "0.02221063\n",
      "0.010568347\n",
      "0.011312161\n",
      "0.0084750075\n",
      "0.008971638\n",
      "0.011416129\n",
      "0.012314282\n",
      "0.011885401\n",
      "0.016562842\n",
      "0.011192373\n",
      "0.010298427\n",
      "0.0097874105\n",
      "0.009381596\n",
      "0.01053818\n",
      "0.0069999034\n",
      "0.011278038\n",
      "0.015254977\n",
      "0.009060833\n",
      "0.013024927\n",
      "0.006785325\n",
      "0.0068744295\n",
      "0.008703905\n",
      "0.011333692\n",
      "0.0103492495\n",
      "0.016823465\n",
      "0.011790397\n",
      "0.011156091\n",
      "0.0107721165\n",
      "0.008927311\n",
      "0.010063848\n",
      "0.0063630147\n",
      "0.011794046\n",
      "0.018447148\n",
      "0.009920475\n",
      "0.013262327\n",
      "0.006213977\n",
      "0.0068657235\n",
      "0.008596348\n",
      "0.01149765\n",
      "0.009592626\n",
      "0.01650462\n",
      "0.012961301\n",
      "0.012629711\n",
      "0.012486715\n",
      "0.011606658\n",
      "0.007746067\n",
      "0.006963458\n",
      "0.00985558\n",
      "0.018306945\n",
      "0.012834266\n",
      "0.015580721\n",
      "0.0060600433\n",
      "0.0075196344\n",
      "0.007947309\n",
      "0.014346292\n",
      "0.009348874\n",
      "0.016626837\n",
      "0.015778087\n",
      "0.017605742\n",
      "0.018016592\n",
      "0.017966244\n",
      "0.0065561337\n",
      "0.009643886\n",
      "0.0078754425\n",
      "0.016403649\n",
      "0.014432729\n",
      "0.019740308\n",
      "0.009350458\n",
      "0.011874785\n",
      "0.011276729\n",
      "0.023206927\n",
      "0.011642372\n",
      "0.012475786\n",
      "0.01500063\n",
      "0.020647144\n",
      "0.026206821\n",
      "0.030274903\n",
      "0.011260305\n",
      "0.020981595\n",
      "0.006708766\n",
      "0.045403\n",
      "0.04426276\n",
      "0.0584581\n",
      "0.03773725\n",
      "0.045075677\n",
      "0.04367262\n",
      "0.07125223\n",
      "0.029562648\n",
      "0.046016403\n",
      "0.05226281\n",
      "0.05539015\n",
      "0.043354757\n",
      "0.040679313\n",
      "0.025094181\n",
      "0.01945155\n",
      "0.046996616\n",
      "0.08089791\n",
      "0.033863764\n",
      "0.03285133\n",
      "0.015787447\n",
      "0.016693879\n",
      "0.021606216\n",
      "0.025506303\n",
      "0.019875297\n",
      "0.04507888\n",
      "0.03191177\n",
      "0.027315482\n",
      "0.028187668\n",
      "0.023277046\n",
      "0.025230113\n",
      "0.015528028\n",
      "0.029198663\n",
      "0.0424722\n",
      "0.027902132\n",
      "0.033454727\n",
      "0.014321212\n",
      "0.012453539\n",
      "0.016257726\n",
      "0.02666838\n",
      "0.017005647\n",
      "0.042672347\n",
      "0.0328992\n",
      "0.031646203\n",
      "0.029231519\n",
      "0.027546985\n",
      "0.01697412\n",
      "0.01782962\n",
      "0.027126262\n",
      "0.04848413\n",
      "0.029283669\n",
      "0.036614228\n",
      "0.014047676\n",
      "0.013222427\n",
      "0.017727146\n",
      "0.028399399\n",
      "0.017550252\n",
      "0.042181857\n",
      "0.038364682\n",
      "0.039593156\n",
      "0.038615342\n",
      "0.04178428\n",
      "0.013349133\n",
      "0.025681252\n",
      "0.01947245\n",
      "0.042095903\n",
      "0.03254455\n",
      "0.047433823\n",
      "0.020139962\n",
      "0.023472916\n",
      "0.023743454\n",
      "0.045392275\n",
      "0.020697951\n",
      "0.037986424\n",
      "0.04265528\n",
      "0.053580314\n",
      "0.0571097\n",
      "0.0693701\n",
      "0.02256766\n",
      "0.04933425\n",
      "0.012456232\n",
      "0.024526775\n",
      "0.025651274\n",
      "0.05078845\n",
      "0.031677812\n",
      "0.047739994\n",
      "0.051468156\n",
      "0.100040615\n",
      "0.047241\n",
      "0.017548628\n",
      "0.024684781\n",
      "0.042207822\n",
      "0.06295198\n",
      "0.09445908\n",
      "0.051424976\n",
      "0.0999896\n",
      "0.03334715\n",
      "0.020428542\n",
      "0.022802975\n",
      "0.024184119\n",
      "0.0163684\n",
      "0.04908827\n",
      "0.05742468\n",
      "0.15863797\n",
      "0.10053949\n",
      "0.036216673\n",
      "0.035720635\n",
      "0.04338063\n",
      "0.058691315\n",
      "0.07829724\n",
      "0.030020814\n",
      "0.10951429\n",
      "0.020695208\n",
      "0.035733964\n",
      "0.02675928\n",
      "0.06971856\n",
      "0.028507847\n",
      "0.04783888\n",
      "0.047737673\n",
      "0.11300842\n",
      "0.036311913\n",
      "0.092969336\n",
      "0.089705154\n",
      "0.12207906\n",
      "0.10683237\n",
      "0.10851599\n",
      "0.03631748\n",
      "0.0753659\n",
      "0.032504916\n",
      "0.07832853\n",
      "0.06204554\n",
      "0.09529475\n",
      "0.05448225\n",
      "0.0723537\n",
      "0.06884738\n",
      "0.1502647\n",
      "0.07585879\n",
      "0.039511293\n",
      "0.05108047\n",
      "0.08375708\n",
      "0.1019133\n",
      "0.12705821\n",
      "0.066536196\n",
      "0.15895417\n",
      "0.0466656\n",
      "0.02928397\n",
      "0.030535853\n",
      "0.051317308\n",
      "0.03143426\n",
      "0.07060742\n",
      "0.08845422\n",
      "0.25546956\n",
      "0.13768275\n",
      "0.02830679\n",
      "0.023557948\n",
      "0.042586975\n",
      "0.06911739\n",
      "0.11010013\n",
      "0.06416386\n",
      "0.17126134\n",
      "0.06309718\n",
      "0.04431715\n",
      "0.054189265\n",
      "0.042110365\n",
      "0.03204526\n",
      "0.04310359\n",
      "0.065480396\n",
      "0.2344425\n",
      "0.14051104\n",
      "0.042409018\n",
      "0.04133988\n",
      "0.034783803\n",
      "0.039239496\n",
      "0.060528025\n",
      "0.03666009\n",
      "0.13246162\n",
      "0.051500812\n",
      "0.040583715\n",
      "0.05831264\n",
      "0.03712738\n",
      "0.037454616\n",
      "0.026630342\n",
      "0.04021488\n",
      "0.17715666\n",
      "0.100837685\n",
      "0.027823543\n",
      "0.026339583\n",
      "0.0302526\n",
      "0.03605598\n",
      "0.05324295\n",
      "0.023285965\n",
      "0.10982344\n",
      "0.03314858\n",
      "0.024874877\n",
      "0.033348944\n",
      "0.034407496\n",
      "0.02514932\n",
      "0.025910117\n",
      "0.039185353\n",
      "0.17229965\n",
      "0.08711021\n",
      "0.048246868\n",
      "0.05050221\n",
      "0.061994758\n",
      "0.08094429\n",
      "0.119824775\n",
      "0.05005559\n",
      "0.15747197\n",
      "0.04268391\n",
      "0.05193162\n",
      "0.050904956\n",
      "0.087622374\n",
      "0.050689496\n",
      "0.075303555\n",
      "0.079435185\n",
      "0.2336354\n",
      "0.09100183\n",
      "0.057676192\n",
      "0.050068025\n",
      "0.13196674\n",
      "0.13893789\n",
      "0.1772194\n",
      "0.047035858\n",
      "0.12938902\n",
      "0.028272392\n",
      "0.08866365\n",
      "0.07104667\n",
      "0.1193175\n",
      "0.058954127\n",
      "0.08825413\n",
      "0.07923561\n",
      "0.19939147\n",
      "0.08453443\n",
      "0.05864387\n",
      "0.06126759\n",
      "0.12919857\n",
      "0.13981718\n",
      "0.1830323\n",
      "0.064814\n",
      "0.1744351\n",
      "0.043383982\n",
      "0.049571566\n",
      "0.05604306\n",
      "0.09215256\n",
      "0.056194507\n",
      "0.10511209\n",
      "0.110121846\n",
      "0.29199162\n",
      "0.15339153\n",
      "0.039821796\n",
      "0.039505117\n",
      "0.08208338\n",
      "0.1139996\n",
      "0.18362676\n",
      "0.084121846\n",
      "0.22760467\n",
      "0.06986879\n",
      "0.055032887\n",
      "0.06992118\n",
      "0.068342455\n",
      "0.049732134\n",
      "0.08442283\n",
      "0.106030285\n",
      "0.3338679\n",
      "0.20699655\n",
      "0.056158468\n",
      "0.050331928\n",
      "0.048927866\n",
      "0.07157216\n",
      "0.12967965\n",
      "0.06639254\n",
      "0.21534723\n",
      "0.07852389\n",
      "0.07125783\n",
      "0.09797295\n",
      "0.06040357\n",
      "0.056013267\n",
      "0.052637983\n",
      "0.07357243\n",
      "0.2822469\n",
      "0.18003114\n",
      "0.054840688\n",
      "0.056773204\n",
      "0.045988042\n",
      "0.056178845\n",
      "0.09426809\n",
      "0.042544372\n",
      "0.17613766\n",
      "0.058857955\n",
      "0.054272942\n",
      "0.08389762\n",
      "0.052924335\n",
      "0.052787907\n",
      "0.043766964\n",
      "0.059078056\n",
      "0.25170943\n",
      "0.14431699\n",
      "0.040313475\n",
      "0.037052132\n",
      "0.041227512\n",
      "0.057688683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06090988\n",
      "0.05113896\n",
      "0.14406282\n",
      "0.041392393\n",
      "0.05152192\n",
      "0.07385323\n",
      "0.077426784\n",
      "0.059319705\n",
      "0.058206134\n",
      "0.06683113\n",
      "0.20685652\n",
      "0.13414793\n",
      "0.04284274\n",
      "0.0379216\n",
      "0.09534437\n",
      "0.08997609\n",
      "0.15691282\n",
      "0.050464936\n",
      "0.19089909\n",
      "0.032187417\n",
      "0.06753772\n",
      "0.047877472\n",
      "0.13336742\n",
      "0.06686433\n",
      "0.10471043\n",
      "0.09653008\n",
      "0.23590054\n",
      "0.13550094\n",
      "0.045882415\n",
      "0.045734413\n",
      "0.10484402\n",
      "0.11701311\n",
      "0.20271409\n",
      "0.0779244\n",
      "0.24492005\n",
      "0.048997186\n",
      "0.052637056\n",
      "0.059760097\n",
      "0.1109773\n",
      "0.083611414\n",
      "0.12454389\n",
      "0.12626408\n",
      "0.28554878\n",
      "0.18824326\n",
      "0.05783234\n",
      "0.05101305\n",
      "0.072156996\n",
      "0.08390184\n",
      "0.1747921\n",
      "0.08723285\n",
      "0.2958277\n",
      "0.08123831\n",
      "0.06337689\n",
      "0.08370131\n",
      "0.06841348\n",
      "0.0620618\n",
      "0.087929636\n",
      "0.107279286\n",
      "0.29265612\n",
      "0.20741868\n",
      "0.07627155\n",
      "0.06955236\n",
      "0.0691648\n",
      "0.05823376\n",
      "0.11245417\n",
      "0.06188094\n",
      "0.2546693\n",
      "0.073525265\n",
      "0.06442925\n",
      "0.09611429\n",
      "0.061912604\n",
      "0.06144763\n",
      "0.05941309\n",
      "0.079237\n",
      "0.25627413\n",
      "0.1878162\n",
      "0.06213375\n",
      "0.059846874\n",
      "0.06693365\n",
      "0.052378356\n",
      "0.09712599\n",
      "0.050012834\n",
      "0.22315827\n",
      "0.05637221\n",
      "0.048780065\n",
      "0.07823009\n",
      "0.05757754\n",
      "0.055923086\n",
      "0.054560367\n",
      "0.07268844\n",
      "0.24586327\n",
      "0.17877193\n",
      "0.050805148\n",
      "0.046781126\n",
      "0.061630927\n",
      "0.05231654\n",
      "0.106001295\n",
      "0.046885446\n",
      "0.22021395\n",
      "0.05058416\n",
      "0.053549137\n",
      "0.063559294\n",
      "0.0840701\n",
      "0.051038556\n",
      "0.07154532\n",
      "0.09935218\n",
      "0.23153296\n",
      "0.18944344\n",
      "0.04721176\n",
      "0.04265665\n",
      "0.07861155\n",
      "0.08618067\n",
      "0.18725125\n",
      "0.07227488\n",
      "0.26762074\n",
      "0.05756613\n",
      "0.05437844\n",
      "0.058370695\n",
      "0.10326426\n",
      "0.06675038\n",
      "0.09530866\n",
      "0.12762526\n",
      "0.2794222\n",
      "0.20595402\n",
      "0.05367986\n",
      "0.04636943\n",
      "0.07526536\n",
      "0.08535237\n",
      "0.1917798\n",
      "0.08696648\n",
      "0.3070094\n",
      "0.07844879\n",
      "0.056211516\n",
      "0.07426866\n",
      "0.08100143\n",
      "0.06476517\n",
      "0.086361215\n",
      "0.12451602\n",
      "0.2890417\n",
      "0.23127747\n",
      "0.06918518\n",
      "0.06508709\n",
      "0.06948511\n",
      "0.06591545\n",
      "0.14516324\n",
      "0.07516359\n",
      "0.29336983\n",
      "0.0873903\n",
      "0.0646594\n",
      "0.09394628\n",
      "0.06418633\n",
      "0.061864544\n",
      "0.064872324\n",
      "0.09699729\n",
      "0.25817287\n",
      "0.2145371\n",
      "0.067395516\n",
      "0.0664357\n",
      "0.06937444\n",
      "0.057006117\n",
      "0.11969898\n",
      "0.060106218\n",
      "0.25894463\n",
      "0.0729047\n",
      "0.056168094\n",
      "0.08318898\n",
      "0.061855648\n",
      "0.06126771\n",
      "0.05607121\n",
      "0.086158425\n",
      "0.24127913\n",
      "0.19694896\n",
      "0.05466141\n",
      "0.05258124\n",
      "0.063759714\n",
      "0.05592514\n",
      "0.12843406\n",
      "0.055935945\n",
      "0.25800273\n",
      "0.06403528\n",
      "0.048045024\n",
      "0.069601126\n",
      "0.06366018\n",
      "0.057180822\n",
      "0.05856225\n",
      "0.09005828\n",
      "0.24889666\n",
      "0.19167131\n",
      "0.04973057\n",
      "0.046341594\n",
      "0.062688276\n",
      "0.058423474\n",
      "0.140956\n",
      "0.05749155\n",
      "0.26687342\n",
      "0.06502322\n",
      "0.04739216\n",
      "0.06738011\n",
      "0.06430949\n",
      "0.05701211\n",
      "0.042798787\n",
      "0.056204293\n",
      "0.20061556\n",
      "0.18198812\n",
      "0.044662606\n",
      "0.04365085\n",
      "0.07278209\n",
      "0.07107235\n",
      "0.12333363\n",
      "0.04615677\n",
      "0.168246\n",
      "0.049708538\n",
      "0.053161185\n",
      "0.062864274\n",
      "0.08012922\n",
      "0.05580129\n",
      "0.05897548\n",
      "0.07887993\n",
      "0.25539684\n",
      "0.18024893\n",
      "0.040482227\n",
      "0.03621511\n",
      "0.08925739\n",
      "0.08787653\n",
      "0.16829763\n",
      "0.05896818\n",
      "0.2144051\n",
      "0.060718868\n",
      "0.0494964\n",
      "0.06078215\n",
      "0.0793362\n",
      "0.053352386\n",
      "0.07222537\n",
      "0.09335342\n",
      "0.29987642\n",
      "0.20183897\n",
      "0.045795385\n",
      "0.039412525\n",
      "0.07874504\n",
      "0.08390011\n",
      "0.16516216\n",
      "0.06632298\n",
      "0.23900077\n",
      "0.07717668\n",
      "0.057602428\n",
      "0.07326469\n",
      "0.06614654\n",
      "0.057829753\n",
      "0.06459497\n",
      "0.08969684\n",
      "0.29003388\n",
      "0.21246463\n",
      "0.05323591\n",
      "0.05030934\n",
      "0.07011378\n",
      "0.06824795\n",
      "0.13239267\n",
      "0.05779486\n",
      "0.2219563\n",
      "0.07648472\n",
      "0.061810926\n",
      "0.08323177\n",
      "0.061301373\n",
      "0.061051324\n",
      "0.052941766\n",
      "0.07185567\n",
      "0.25502625\n",
      "0.19374852\n",
      "0.0511487\n",
      "0.0498541\n",
      "0.06904965\n",
      "0.06204992\n",
      "0.11449766\n",
      "0.04754818\n",
      "0.19964246\n",
      "0.06475335\n",
      "0.053958982\n",
      "0.075404\n",
      "0.05954478\n",
      "0.059789523\n",
      "0.04887003\n",
      "0.06514809\n",
      "0.23671728\n",
      "0.17542234\n",
      "0.044863768\n",
      "0.04247473\n",
      "0.069774225\n",
      "0.06277496\n",
      "0.117427915\n",
      "0.044670396\n",
      "0.19683452\n",
      "0.058918044\n",
      "0.047975685\n",
      "0.06786077\n",
      "0.059539303\n",
      "0.05513396\n",
      "0.0504281\n",
      "0.06653899\n",
      "0.24065176\n",
      "0.17017898\n",
      "0.04450974\n",
      "0.042722173\n",
      "0.08201949\n",
      "0.077091485\n",
      "0.12642324\n",
      "0.05049192\n",
      "0.19698049\n",
      "0.058195364\n",
      "0.051133562\n",
      "0.05918745\n",
      "0.07283516\n",
      "0.055754956\n",
      "0.056518007\n",
      "0.07635542\n",
      "0.26847878\n",
      "0.19679582\n",
      "0.045646757\n",
      "0.040503185\n",
      "0.08687522\n",
      "0.07934681\n",
      "0.14088139\n",
      "0.055250384\n",
      "0.21072586\n",
      "0.06401104\n",
      "0.051521793\n",
      "0.056809492\n",
      "0.07078645\n",
      "0.057704426\n",
      "0.057242073\n",
      "0.08000346\n",
      "0.27913478\n",
      "0.19696993\n",
      "0.04631441\n",
      "0.040957227\n",
      "0.081421405\n",
      "0.075608\n",
      "0.13553593\n",
      "0.05570596\n",
      "0.21937126\n",
      "0.06780118\n",
      "0.0526251\n",
      "0.061416633\n",
      "0.06546723\n",
      "0.05834332\n",
      "0.054351557\n",
      "0.07631512\n",
      "0.27208126\n",
      "0.19500203\n",
      "0.047440436\n",
      "0.0441396\n",
      "0.078205615\n",
      "0.06936687\n",
      "0.12511948\n",
      "0.051560406\n",
      "0.21308868\n",
      "0.0669602\n",
      "0.05361151\n",
      "0.06318637\n",
      "0.06291361\n",
      "0.05883587\n",
      "0.05053877\n",
      "0.07021858\n",
      "0.25691584\n",
      "0.18487746\n",
      "0.046850856\n",
      "0.04408102\n",
      "0.076590486\n",
      "0.06584868\n",
      "0.11695357\n",
      "0.04761762\n",
      "0.20294769\n",
      "0.06394807\n",
      "0.051963985\n",
      "0.062032074\n",
      "0.061386924\n",
      "0.059472613\n",
      "0.047873046\n",
      "0.06575359\n",
      "0.2447392\n",
      "0.17446537\n",
      "0.04505652\n",
      "0.04270557\n",
      "0.07611317\n",
      "0.0642301\n",
      "0.11351498\n",
      "0.045250874\n",
      "0.19645348\n",
      "0.060906954\n",
      "0.050120816\n",
      "0.060011934\n",
      "0.060947474\n",
      "0.0584407\n",
      "0.047039192\n",
      "0.06357977\n",
      "0.237589\n",
      "0.16729756\n",
      "0.043681\n",
      "0.041567\n",
      "0.0758809\n",
      "0.063506976\n",
      "0.061890677\n",
      "0.049607575\n",
      "0.15588245\n",
      "0.0446035\n",
      "0.04998645\n",
      "0.08062094\n",
      "0.056829385\n",
      "0.069663\n",
      "0.04720082\n",
      "0.06355208\n",
      "0.1721138\n",
      "0.11960432\n",
      "0.047801442\n",
      "0.04822806\n",
      "0.08550245\n",
      "0.068026334\n",
      "0.11782823\n",
      "0.050399404\n",
      "0.22065245\n",
      "0.049233112\n",
      "0.05075331\n",
      "0.057027653\n",
      "0.07183249\n",
      "0.05894257\n",
      "0.0711549\n",
      "0.09055846\n",
      "0.22598368\n",
      "0.14190547\n",
      "0.050937373\n",
      "0.047042336\n",
      "0.08102034\n",
      "0.075160794\n",
      "0.1444303\n",
      "0.06130882\n",
      "0.2706881\n",
      "0.060167655\n",
      "0.04770337\n",
      "0.06341614\n",
      "0.06766969\n",
      "0.060523216\n",
      "0.07770468\n",
      "0.1032762\n",
      "0.25074342\n",
      "0.16357526\n",
      "0.060740277\n",
      "0.05684282\n",
      "0.0783045\n",
      "0.068918474\n",
      "0.13766666\n",
      "0.06202656\n",
      "0.28258073\n",
      "0.07047054\n",
      "0.052004408\n",
      "0.07831412\n",
      "0.061183535\n",
      "0.061849203\n",
      "0.06693258\n",
      "0.09288952\n",
      "0.24130258\n",
      "0.16537386\n",
      "0.06713538\n",
      "0.06751099\n",
      "0.0809234\n",
      "0.06250862\n",
      "0.113545366\n",
      "0.05596784\n",
      "0.25668994\n",
      "0.067951694\n",
      "0.052760996\n",
      "0.0850695\n",
      "0.057890866\n",
      "0.06287581\n",
      "0.05699686\n",
      "0.07904589\n",
      "0.22044554\n",
      "0.15216962\n",
      "0.063333854\n",
      "0.066543564\n",
      "0.08083723\n",
      "0.059946723\n",
      "0.10284362\n",
      "0.050861742\n",
      "0.23461777\n",
      "0.060555927\n",
      "0.048985116\n",
      "0.07921254\n",
      "0.057056256\n",
      "0.0616409\n",
      "0.053859707\n",
      "0.07409862\n",
      "0.2130711\n",
      "0.14267316\n",
      "0.05807005\n",
      "0.060906954\n",
      "0.0786347\n",
      "0.059541255\n",
      "0.1035428\n",
      "0.04937087\n",
      "0.22847174\n",
      "0.056856293\n",
      "0.05055572\n",
      "0.065629154\n",
      "0.06957376\n",
      "0.059737753\n",
      "0.063356\n",
      "0.08649068\n",
      "0.2257266\n",
      "0.117605045\n",
      "0.051269583\n",
      "0.05513024\n",
      "0.088738225\n",
      "0.07166481\n",
      "0.14153649\n",
      "0.061392687\n",
      "0.23839413\n",
      "0.06292188\n",
      "0.052018482\n",
      "0.07517585\n",
      "0.07174498\n",
      "0.06602246\n",
      "0.06670341\n",
      "0.09393152\n",
      "0.24848887\n",
      "0.13307317\n",
      "0.060496423\n",
      "0.06524848\n",
      "0.08344738\n",
      "0.06885324\n",
      "0.13059217\n",
      "0.063104436\n",
      "0.24089727\n",
      "0.068129115\n",
      "0.052049004\n",
      "0.08781839\n",
      "0.062288955\n",
      "0.065985225\n",
      "0.05920189\n",
      "0.0853781\n",
      "0.23580965\n",
      "0.13471992\n",
      "0.062693775\n",
      "0.07529019\n",
      "0.08286049\n",
      "0.06311112\n",
      "0.1099123\n",
      "0.05618302\n",
      "0.22428297\n",
      "0.06448714\n",
      "0.051006444\n",
      "0.0893042\n",
      "0.05946817\n",
      "0.066296384\n",
      "0.054083858\n",
      "0.076120265\n",
      "0.2227951\n",
      "0.12726265\n",
      "0.059492063\n",
      "0.072072454\n",
      "0.08314985\n",
      "0.06159284\n",
      "0.10368634\n",
      "0.054155402\n",
      "0.2155062\n",
      "0.06081321\n",
      "0.048612725\n",
      "0.08337026\n",
      "0.05900691\n",
      "0.06552635\n",
      "0.052958935\n",
      "0.07461899\n",
      "0.21924709\n",
      "0.124209724\n",
      "0.056439083\n",
      "0.068432316\n",
      "0.08175672\n",
      "0.06138845\n",
      "0.10617051\n",
      "0.053678583\n",
      "0.21513675\n",
      "0.05965813\n",
      "0.04723966\n",
      "0.08065474\n",
      "0.058681767\n",
      "0.06399226\n",
      "0.05348245\n",
      "0.075307555\n",
      "0.22085056\n",
      "0.12422483\n",
      "0.055591322\n",
      "0.06716949\n",
      "0.08069769\n",
      "0.06132392\n",
      "0.10769584\n",
      "0.05387562\n",
      "0.2160673\n",
      "0.060086057\n",
      "0.047187492\n",
      "0.08085454\n",
      "0.058252364\n",
      "0.063771725\n",
      "0.041687105\n",
      "0.060207386\n",
      "0.18770097\n",
      "0.110992834\n",
      "0.051983614\n",
      "0.06137005\n",
      "0.083013594\n",
      "0.06814635\n",
      "0.09401226\n",
      "0.047411375\n",
      "0.17099695\n",
      "0.050677575\n",
      "0.049107537\n",
      "0.074583374\n",
      "0.07391174\n",
      "0.061159097\n",
      "0.049627\n",
      "0.07729611\n",
      "0.22472322\n",
      "0.11784662\n",
      "0.049604565\n",
      "0.052194536\n",
      "0.086098604\n",
      "0.07726927\n",
      "0.123670466\n",
      "0.055375565\n",
      "0.20499584\n",
      "0.056931723\n",
      "0.046586722\n",
      "0.070835076\n",
      "0.07575266\n",
      "0.060261786\n",
      "0.058441624\n",
      "0.09207974\n",
      "0.25975877\n",
      "0.1326579\n",
      "0.051934935\n",
      "0.052788615\n",
      "0.08272427\n",
      "0.07800516\n",
      "0.13208705\n",
      "0.058856145\n",
      "0.22889918\n",
      "0.06342905\n",
      "0.047783494\n",
      "0.07716641\n",
      "0.07081388\n",
      "0.060963243\n",
      "0.05748666\n",
      "0.09317225\n",
      "0.2639147\n",
      "0.14103772\n",
      "0.05444854\n",
      "0.058348984\n",
      "0.080290355\n",
      "0.07338795\n",
      "0.12495396\n",
      "0.05791933\n",
      "0.22659677\n",
      "0.06550484\n",
      "0.048664983\n",
      "0.08333358\n",
      "0.0659857\n",
      "0.061271083\n",
      "0.053493902\n",
      "0.08688911\n",
      "0.25457892\n",
      "0.13998033\n",
      "0.055592358\n",
      "0.06119143\n",
      "0.07945794\n",
      "0.0688962\n",
      "0.114439525\n",
      "0.054242752\n",
      "0.21713793\n",
      "0.062803775\n",
      "0.04820678\n",
      "0.08423273\n",
      "0.063890584\n",
      "0.061388835\n",
      "0.05039093\n",
      "0.0803608\n",
      "0.24192539\n",
      "0.13483214\n",
      "0.05415656\n",
      "0.061004825\n",
      "0.078899786\n",
      "0.066338316\n",
      "0.10870104\n",
      "0.05147337\n",
      "0.20754966\n",
      "0.05875831\n",
      "0.046546362\n",
      "0.08153924\n",
      "0.06327809\n",
      "0.06065189\n",
      "0.049341604\n",
      "0.07782657\n",
      "0.23523057\n",
      "0.13073322\n",
      "0.04736661\n",
      "0.055917233\n",
      "0.08089342\n",
      "0.07577583\n",
      "0.11232908\n",
      "0.05157482\n",
      "0.19188051\n",
      "0.057094157\n",
      "0.049425706\n",
      "0.07114531\n",
      "0.08090435\n",
      "0.05738521\n",
      "0.051681336\n",
      "0.084276676\n",
      "0.25491008\n",
      "0.1313076\n",
      "0.051395003\n",
      "0.05335417\n",
      "0.08710505\n",
      "0.08246851\n",
      "0.12943608\n",
      "0.05851933\n",
      "0.21800427\n",
      "0.062750764\n",
      "0.048103675\n",
      "0.071205005\n",
      "0.07832053\n",
      "0.060108695\n",
      "0.054299425\n",
      "0.09207997\n",
      "0.2746556\n",
      "0.14278111\n",
      "0.05199874\n",
      "0.05475505\n",
      "0.08303545\n",
      "0.079828195\n",
      "0.13094662\n",
      "0.0598813\n",
      "0.22967291\n",
      "0.066409886\n",
      "0.049538482\n",
      "0.07651314\n",
      "0.073651165\n",
      "0.060098045\n",
      "0.05253485\n",
      "0.08956164\n",
      "0.27301413\n",
      "0.14690074\n",
      "0.05392193\n",
      "0.05882387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0811169\n",
      "0.07560834\n",
      "0.12306643\n",
      "0.057455253\n",
      "0.22388005\n",
      "0.066078104\n",
      "0.05015867\n",
      "0.079642594\n",
      "0.0711619\n",
      "0.0611337\n",
      "0.04976175\n",
      "0.08394061\n",
      "0.26000828\n",
      "0.14368813\n",
      "0.05404924\n",
      "0.06040513\n",
      "0.07983976\n",
      "0.0718933\n",
      "0.11438115\n",
      "0.05403739\n",
      "0.21381772\n",
      "0.06284804\n",
      "0.04917362\n",
      "0.07944617\n",
      "0.06931959\n",
      "0.061347652\n",
      "0.047780372\n",
      "0.07888584\n",
      "0.24741323\n",
      "0.13823363\n",
      "0.052865274\n",
      "0.059811503\n",
      "0.07886259\n",
      "0.06987515\n",
      "0.110334024\n",
      "0.05158408\n",
      "0.20633966\n",
      "0.05971923\n",
      "0.048112538\n",
      "0.07757334\n",
      "0.068724126\n",
      "0.06090498\n",
      "0.047003146\n",
      "0.07678843\n",
      "0.2408142\n",
      "0.13449787\n",
      "0.051729597\n",
      "0.058465395\n",
      "0.07845913\n",
      "0.06933193\n",
      "0.057598196\n",
      "0.058406565\n",
      "0.1288894\n",
      "0.04951649\n",
      "0.050097484\n",
      "0.08510908\n",
      "0.064656384\n",
      "0.07809564\n",
      "0.04500314\n",
      "0.061146997\n",
      "0.16615532\n",
      "0.09016059\n",
      "0.04834551\n",
      "0.06350312\n",
      "0.09081051\n",
      "0.06598055\n",
      "0.10138465\n",
      "0.055087384\n",
      "0.16843909\n",
      "0.052629866\n",
      "0.048243284\n",
      "0.06894515\n",
      "0.0689503\n",
      "0.06479626\n",
      "0.062281594\n",
      "0.081785984\n",
      "0.211005\n",
      "0.10939856\n",
      "0.050197363\n",
      "0.06097407\n",
      "0.086410396\n",
      "0.0704069\n",
      "0.12691762\n",
      "0.06157114\n",
      "0.20917939\n",
      "0.06381452\n",
      "0.049261417\n",
      "0.07319385\n",
      "0.067035556\n",
      "0.06673774\n",
      "0.06607322\n",
      "0.091539115\n",
      "0.23342279\n",
      "0.12508088\n",
      "0.056620393\n",
      "0.07170428\n",
      "0.08456207\n",
      "0.06703395\n",
      "0.11852601\n",
      "0.062411856\n",
      "0.21587028\n",
      "0.07017801\n",
      "0.053051542\n",
      "0.0828149\n",
      "0.06400886\n",
      "0.06807734\n",
      "0.05861144\n",
      "0.08351535\n",
      "0.22422269\n",
      "0.12615342\n",
      "0.060049955\n",
      "0.08018923\n",
      "0.08529836\n",
      "0.06254121\n",
      "0.10313088\n",
      "0.057864696\n",
      "0.19908567\n",
      "0.06745337\n",
      "0.052922573\n",
      "0.08543269\n",
      "0.061753176\n",
      "0.0690728\n",
      "0.05303022\n",
      "0.07466414\n",
      "0.2113009\n",
      "0.11919266\n",
      "0.05734563\n",
      "0.07758371\n",
      "0.085017964\n",
      "0.060945634\n",
      "0.09800563\n",
      "0.056173656\n",
      "0.18924502\n",
      "0.06328536\n",
      "0.05112013\n",
      "0.08119956\n",
      "0.061268713\n",
      "0.06857662\n",
      "0.052150425\n",
      "0.07300929\n",
      "0.2085546\n",
      "0.11545099\n",
      "0.05462778\n",
      "0.07362314\n",
      "0.083991155\n",
      "0.061023332\n",
      "0.09975994\n",
      "0.056014426\n",
      "0.18879253\n",
      "0.061969932\n",
      "0.053290863\n",
      "0.07287853\n",
      "0.08028671\n",
      "0.058574077\n",
      "0.07127507\n",
      "0.0911036\n",
      "0.18639617\n",
      "0.10281259\n",
      "0.050124116\n",
      "0.06356797\n",
      "0.09777465\n",
      "0.087533645\n",
      "0.14577614\n",
      "0.0666161\n",
      "0.19229105\n",
      "0.066851564\n",
      "0.05124216\n",
      "0.07938823\n",
      "0.07805941\n",
      "0.06756198\n",
      "0.0795886\n",
      "0.110978566\n",
      "0.22438404\n",
      "0.119248904\n",
      "0.053811215\n",
      "0.07113797\n",
      "0.085780345\n",
      "0.080183886\n",
      "0.13881338\n",
      "0.06980876\n",
      "0.20440802\n",
      "0.07435826\n",
      "0.05046506\n",
      "0.08691051\n",
      "0.06956277\n",
      "0.066415295\n",
      "0.07131747\n",
      "0.10138334\n",
      "0.220199\n",
      "0.12338706\n",
      "0.057094898\n",
      "0.08122997\n",
      "0.08453469\n",
      "0.07270382\n",
      "0.11979794\n",
      "0.06416821\n",
      "0.19495024\n",
      "0.074754626\n",
      "0.051062413\n",
      "0.090086326\n",
      "0.06609858\n",
      "0.06619196\n",
      "0.064345516\n",
      "0.09087639\n",
      "0.20992419\n",
      "0.11890033\n",
      "0.055756986\n",
      "0.08012716\n",
      "0.08387698\n",
      "0.07006616\n",
      "0.1134478\n",
      "0.062294398\n",
      "0.18626298\n",
      "0.07124591\n",
      "0.049799256\n",
      "0.08728546\n",
      "0.064905584\n",
      "0.06591135\n",
      "0.061740365\n",
      "0.08809843\n",
      "0.20394911\n",
      "0.11501482\n",
      "0.05345128\n",
      "0.07687217\n",
      "0.08304678\n",
      "0.07038967\n",
      "0.11467982\n",
      "0.06169244\n",
      "0.18439373\n",
      "0.0695777\n",
      "0.048817057\n",
      "0.0846173\n",
      "0.06500252\n",
      "0.0649117\n",
      "0.06212394\n",
      "0.08778764\n",
      "0.20442702\n",
      "0.11474533\n",
      "0.05313223\n",
      "0.0757813\n",
      "0.082183875\n",
      "0.070464425\n",
      "0.114919655\n",
      "0.06184153\n",
      "0.18401626\n",
      "0.0696488\n",
      "0.048471197\n",
      "0.08402465\n",
      "0.06469066\n",
      "0.06489907\n",
      "0.04562259\n",
      "0.06835276\n",
      "0.17986679\n",
      "0.10017942\n",
      "0.05187522\n",
      "0.07315758\n",
      "0.08791621\n",
      "0.07223627\n",
      "0.09297988\n",
      "0.05577778\n",
      "0.16608253\n",
      "0.06151271\n",
      "0.053547632\n",
      "0.082844585\n",
      "0.07754583\n",
      "0.06573068\n",
      "0.05763425\n",
      "0.08912542\n",
      "0.21447024\n",
      "0.10932934\n",
      "0.051751316\n",
      "0.06379672\n",
      "0.09038499\n",
      "0.08526876\n",
      "0.12598619\n",
      "0.066721074\n",
      "0.19788474\n",
      "0.067907594\n",
      "0.051838357\n",
      "0.07903053\n",
      "0.080195695\n",
      "0.065977\n",
      "0.06919968\n",
      "0.10702105\n",
      "0.2499775\n",
      "0.12367356\n",
      "0.052271493\n",
      "0.06477015\n",
      "0.08882982\n",
      "0.08904008\n",
      "0.1357208\n",
      "0.070752874\n",
      "0.22018981\n",
      "0.07709097\n",
      "0.053742506\n",
      "0.084955364\n",
      "0.0774385\n",
      "0.06643846\n",
      "0.06790353\n",
      "0.10781483\n",
      "0.2579231\n",
      "0.13437402\n",
      "0.05596672\n",
      "0.07350996\n",
      "0.08650941\n",
      "0.08204616\n",
      "0.12398895\n",
      "0.06963354\n",
      "0.21499121\n",
      "0.080650404\n",
      "0.05487254\n",
      "0.0922566\n",
      "0.07226695\n",
      "0.06656903\n",
      "0.061087646\n",
      "0.09924308\n",
      "0.2463861\n",
      "0.13281791\n",
      "0.0575615\n",
      "0.07848045\n",
      "0.085465625\n",
      "0.0751237\n",
      "0.11090372\n",
      "0.06447151\n",
      "0.20212206\n",
      "0.07752206\n",
      "0.054931864\n",
      "0.094039544\n",
      "0.069896445\n",
      "0.06631644\n",
      "0.056299154\n",
      "0.09087789\n",
      "0.2334232\n",
      "0.12604727\n",
      "0.056046553\n",
      "0.07728572\n",
      "0.08562915\n",
      "0.0725847\n",
      "0.1056214\n",
      "0.062044133\n",
      "0.19209626\n",
      "0.073133655\n",
      "0.053315777\n",
      "0.09142534\n",
      "0.06928417\n",
      "0.065732636\n",
      "0.055048075\n",
      "0.088190265\n",
      "0.2279966\n",
      "0.12165893\n",
      "0.05096504\n",
      "0.069912344\n",
      "0.08433222\n",
      "0.07926444\n",
      "0.10901195\n",
      "0.06420568\n",
      "0.19325823\n",
      "0.07339045\n",
      "0.05668667\n",
      "0.079960115\n",
      "0.075094014\n",
      "0.064126655\n",
      "0.06156368\n",
      "0.09864258\n",
      "0.24647121\n",
      "0.12542585\n",
      "0.055228673\n",
      "0.069584556\n",
      "0.08847799\n",
      "0.08231194\n",
      "0.11581254\n",
      "0.069765605\n",
      "0.20469353\n",
      "0.07881088\n",
      "0.056325022\n",
      "0.08227404\n",
      "0.07363306\n",
      "0.06664959\n",
      "0.0619224\n",
      "0.10108176\n",
      "0.25606167\n",
      "0.13117628\n",
      "0.05571992\n",
      "0.072634004\n",
      "0.08663658\n",
      "0.0795018\n",
      "0.11451258\n",
      "0.06858513\n",
      "0.20786978\n",
      "0.08067175\n",
      "0.057037994\n",
      "0.08564086\n",
      "0.07091418\n",
      "0.06565962\n",
      "0.05926709\n",
      "0.09784547\n",
      "0.25449342\n",
      "0.13146348\n",
      "0.056239225\n",
      "0.07484822\n",
      "0.08691017\n",
      "0.07689609\n",
      "0.1100876\n",
      "0.066410884\n",
      "0.2028912\n",
      "0.07963805\n",
      "0.057099573\n",
      "0.086285956\n",
      "0.06990143\n",
      "0.06601386\n",
      "0.057027813\n",
      "0.09406825\n",
      "0.24822801\n",
      "0.12945841\n",
      "0.056394614\n",
      "0.07547144\n",
      "0.08664104\n",
      "0.074555494\n",
      "0.105516575\n",
      "0.06449055\n",
      "0.1956261\n",
      "0.077624105\n",
      "0.056570575\n",
      "0.08687113\n",
      "0.06903887\n",
      "0.066100836\n",
      "0.05491312\n",
      "0.09051023\n",
      "0.2411257\n",
      "0.12624739\n",
      "0.055847574\n",
      "0.07548827\n",
      "0.08627003\n",
      "0.07275511\n",
      "0.10250513\n",
      "0.06262298\n",
      "0.18983588\n",
      "0.07517382\n",
      "0.056017283\n",
      "0.08633607\n",
      "0.068723425\n",
      "0.06587459\n",
      "0.053715453\n",
      "0.08775347\n",
      "0.23561871\n",
      "0.12332784\n",
      "0.055394784\n",
      "0.075167075\n",
      "0.08623955\n",
      "0.07175898\n",
      "0.06416551\n",
      "0.072067715\n",
      "0.120038345\n",
      "0.06621034\n",
      "0.055896718\n",
      "0.09682194\n",
      "0.070751265\n",
      "0.07527979\n",
      "0.05092722\n",
      "0.07235083\n",
      "0.1397795\n",
      "0.09751704\n",
      "0.05474684\n",
      "0.06774939\n",
      "0.09506528\n",
      "0.087282956\n",
      "0.11225623\n",
      "0.06287807\n",
      "0.14488357\n",
      "0.06504895\n",
      "0.053440355\n",
      "0.07168589\n",
      "0.08135411\n",
      "0.06601862\n",
      "0.07091353\n",
      "0.100030355\n",
      "0.19184795\n",
      "0.111281365\n",
      "0.05347056\n",
      "0.06765566\n",
      "0.08859095\n",
      "0.09270627\n",
      "0.14340137\n",
      "0.06633963\n",
      "0.18818575\n",
      "0.08033061\n",
      "0.050921336\n",
      "0.07327205\n",
      "0.080024764\n",
      "0.06929544\n",
      "0.07340145\n",
      "0.10918766\n",
      "0.21691906\n",
      "0.12519573\n",
      "0.056946926\n",
      "0.07407141\n",
      "0.08816132\n",
      "0.091373414\n",
      "0.1394021\n",
      "0.06895986\n",
      "0.1952773\n",
      "0.086718775\n",
      "0.050760157\n",
      "0.07804895\n",
      "0.07533947\n",
      "0.066237085\n",
      "0.07101707\n",
      "0.10646732\n",
      "0.21581951\n",
      "0.12974577\n",
      "0.059020564\n",
      "0.0793884\n",
      "0.08553943\n",
      "0.0848206\n",
      "0.12980205\n",
      "0.067018114\n",
      "0.18989478\n",
      "0.08718841\n",
      "0.05096869\n",
      "0.081510805\n",
      "0.0725964\n",
      "0.06483446\n",
      "0.06536194\n",
      "0.10005481\n",
      "0.20815924\n",
      "0.1263269\n",
      "0.059309296\n",
      "0.08025463\n",
      "0.08500599\n",
      "0.08186473\n",
      "0.122887775\n",
      "0.065508984\n",
      "0.18139029\n",
      "0.08432424\n",
      "0.050606743\n",
      "0.08148174\n",
      "0.072131634\n",
      "0.06340978\n",
      "0.06261668\n",
      "0.09627122\n",
      "0.20137364\n",
      "0.122961976\n",
      "0.058539987\n",
      "0.079157494\n",
      "0.08445197\n",
      "0.08059653\n",
      "0.1194684\n",
      "0.06484221\n",
      "0.17528874\n",
      "0.08206897\n",
      "0.06307003\n",
      "0.07981067\n",
      "0.091533676\n",
      "0.06395185\n",
      "0.07056698\n",
      "0.10302572\n",
      "0.17808016\n",
      "0.10638134\n",
      "0.058642782\n",
      "0.06785761\n",
      "0.105949014\n",
      "0.11542792\n",
      "0.16915523\n",
      "0.06960205\n",
      "0.16852\n",
      "0.08265373\n",
      "0.05673041\n",
      "0.07917222\n",
      "0.091671295\n",
      "0.076051906\n",
      "0.081505306\n",
      "0.12575361\n",
      "0.21282661\n",
      "0.11987788\n",
      "0.05682247\n",
      "0.07501065\n",
      "0.09429612\n",
      "0.10519301\n",
      "0.17090641\n",
      "0.07271547\n",
      "0.18844235\n",
      "0.094728\n",
      "0.053444616\n",
      "0.08417312\n",
      "0.08190154\n",
      "0.0713525\n",
      "0.07478175\n",
      "0.11871743\n",
      "0.2112444\n",
      "0.124135315\n",
      "0.059211314\n",
      "0.082680136\n",
      "0.0917659\n",
      "0.096374765\n",
      "0.15185715\n",
      "0.069615714\n",
      "0.18112826\n",
      "0.09421768\n",
      "0.05253175\n",
      "0.0870621\n",
      "0.07738584\n",
      "0.06726557\n",
      "0.06846799\n",
      "0.10965395\n",
      "0.2032637\n",
      "0.12000938\n",
      "0.05765401\n",
      "0.08117168\n",
      "0.08995808\n",
      "0.09448694\n",
      "0.14718425\n",
      "0.067874804\n",
      "0.17337692\n",
      "0.08940925\n",
      "0.051322535\n",
      "0.08606399\n",
      "0.0768722\n",
      "0.06654583\n",
      "0.06665251\n",
      "0.10716751\n",
      "0.19927086\n",
      "0.116074614\n",
      "0.056630198\n",
      "0.0784362\n",
      "0.089983076\n",
      "0.09602581\n",
      "0.14893426\n",
      "0.06753327\n",
      "0.17237765\n",
      "0.08755833\n",
      "0.050678536\n",
      "0.08457647\n",
      "0.07783169\n",
      "0.06614049\n",
      "0.06723866\n",
      "0.10800667\n",
      "0.19945921\n",
      "0.11561447\n",
      "0.056368105\n",
      "0.07771221\n",
      "0.08942085\n",
      "0.09597196\n",
      "0.14943334\n",
      "0.06782533\n",
      "0.17337073\n",
      "0.08757344\n",
      "0.050439775\n",
      "0.08482692\n",
      "0.0773478\n",
      "0.06556217\n",
      "0.050747603\n",
      "0.08119258\n",
      "0.16164863\n",
      "0.10472574\n",
      "0.058219\n",
      "0.07236037\n",
      "0.090404436\n",
      "0.08510357\n",
      "0.11165309\n",
      "0.06252769\n",
      "0.15511718\n",
      "0.073588096\n",
      "0.05868195\n",
      "0.08872498\n",
      "0.08098286\n",
      "0.0635836\n",
      "0.06163306\n",
      "0.10164877\n",
      "0.19580844\n",
      "0.116008356\n",
      "0.058144607\n",
      "0.06891176\n",
      "0.090925\n",
      "0.09666456\n",
      "0.13494422\n",
      "0.067085125\n",
      "0.1761296\n",
      "0.08272265\n",
      "0.055954184\n",
      "0.08835285\n",
      "0.08125986\n",
      "0.063933544\n",
      "0.06671281\n",
      "0.111386284\n",
      "0.21718533\n",
      "0.12581837\n",
      "0.059588123\n",
      "0.07116935\n",
      "0.089087844\n",
      "0.09746851\n",
      "0.14000522\n",
      "0.06928356\n",
      "0.1881208\n",
      "0.08840157\n",
      "0.05569363\n",
      "0.090606265\n",
      "0.07924907\n",
      "0.063337475\n",
      "0.06653253\n",
      "0.11280592\n",
      "0.22291675\n",
      "0.13115367\n",
      "0.061000496\n",
      "0.07437227\n",
      "0.08813222\n",
      "0.09423189\n",
      "0.13618281\n",
      "0.068846785\n",
      "0.18798284\n",
      "0.090394\n",
      "0.05537573\n",
      "0.092925124\n",
      "0.07662326\n",
      "0.06229885\n",
      "0.06424539\n",
      "0.10985354\n",
      "0.22105822\n",
      "0.13136967\n",
      "0.06184228\n",
      "0.07601044\n",
      "0.087426886\n",
      "0.090727895\n",
      "0.13080011\n",
      "0.06779503\n",
      "0.18455389\n",
      "0.09022587\n",
      "0.055316284\n",
      "0.09421318\n",
      "0.075480916\n",
      "0.061540175\n",
      "0.061785117\n",
      "0.10627594\n",
      "0.2166292\n",
      "0.12929043\n",
      "0.06179233\n",
      "0.076467715\n",
      "0.08690825\n",
      "0.088578\n",
      "0.12690718\n",
      "0.066730216\n",
      "0.1801152\n",
      "0.08901628\n",
      "0.054864917\n",
      "0.09456816\n",
      "0.07464449\n",
      "0.060993023\n",
      "0.06026744\n",
      "0.1035422\n",
      "0.21292987\n",
      "0.12715381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05742033\n",
      "0.07515885\n",
      "0.08840629\n",
      "0.08841903\n",
      "0.12823237\n",
      "0.069838904\n",
      "0.17627372\n",
      "0.08261747\n",
      "0.05992149\n",
      "0.09324312\n",
      "0.07891507\n",
      "0.062978715\n",
      "0.063096486\n",
      "0.10699054\n",
      "0.21207497\n",
      "0.12750141\n",
      "0.060864165\n",
      "0.0720714\n",
      "0.09148504\n",
      "0.092732914\n",
      "0.13148677\n",
      "0.069581985\n",
      "0.17860806\n",
      "0.08508695\n",
      "0.05875161\n",
      "0.0913436\n",
      "0.07887274\n",
      "0.06419095\n",
      "0.064026035\n",
      "0.110622704\n",
      "0.2186927\n",
      "0.12919296\n",
      "0.059415653\n",
      "0.07408281\n",
      "0.08891147\n",
      "0.092279844\n",
      "0.13353829\n",
      "0.07031174\n",
      "0.18166137\n",
      "0.0865765\n",
      "0.05821436\n",
      "0.09320757\n",
      "0.0776695\n",
      "0.063084185\n",
      "0.06380992\n",
      "0.10967867\n",
      "0.22195743\n",
      "0.13087486\n",
      "0.06016158\n",
      "0.07439625\n",
      "0.08815837\n",
      "0.091387935\n",
      "0.13303314\n",
      "0.06969426\n",
      "0.1824563\n",
      "0.08735726\n",
      "0.057817914\n",
      "0.092744514\n",
      "0.07703259\n",
      "0.06286275\n",
      "0.06273827\n",
      "0.108544424\n",
      "0.22161344\n",
      "0.1309317\n",
      "0.060555637\n",
      "0.074642085\n",
      "0.08758158\n",
      "0.09007342\n",
      "0.13049886\n",
      "0.06947847\n",
      "0.18061303\n",
      "0.08702524\n",
      "0.05762701\n",
      "0.09350844\n",
      "0.07633135\n",
      "0.06222584\n",
      "0.06159433\n",
      "0.10672085\n",
      "0.21936002\n",
      "0.12972564\n",
      "0.060484167\n",
      "0.07495813\n",
      "0.08717576\n",
      "0.08853167\n",
      "0.12795293\n",
      "0.06888761\n",
      "0.17832369\n",
      "0.08663204\n",
      "0.057365082\n",
      "0.09362177\n",
      "0.07555128\n",
      "0.061701592\n",
      "0.060522035\n",
      "0.10472374\n",
      "0.21699552\n",
      "0.12844327\n",
      "0.060655683\n",
      "0.07468714\n",
      "0.08680071\n",
      "0.087401204\n",
      "0.08147404\n",
      "0.07745493\n",
      "0.10637452\n",
      "0.075300485\n",
      "0.05578702\n",
      "0.10230936\n",
      "0.076610886\n",
      "0.075154506\n",
      "0.048466645\n",
      "0.07317194\n",
      "0.12829956\n",
      "0.09085002\n",
      "0.06354309\n",
      "0.07273522\n",
      "0.10493424\n",
      "0.092587255\n",
      "0.13258183\n",
      "0.06338952\n",
      "0.12531239\n",
      "0.073240384\n",
      "0.056526646\n",
      "0.07639023\n",
      "0.09498796\n",
      "0.072473764\n",
      "0.06056116\n",
      "0.09647714\n",
      "0.1729905\n",
      "0.09521038\n",
      "0.061738376\n",
      "0.071643844\n",
      "0.10743059\n",
      "0.11080933\n",
      "0.17730367\n",
      "0.06724671\n",
      "0.15877748\n",
      "0.08253453\n",
      "0.05271212\n",
      "0.0786329\n",
      "0.09677933\n",
      "0.0800887\n",
      "0.070471734\n",
      "0.115303285\n",
      "0.21021256\n",
      "0.10763433\n",
      "0.060089897\n",
      "0.07308428\n",
      "0.10471701\n",
      "0.11257759\n",
      "0.19128236\n",
      "0.07264644\n",
      "0.18205175\n",
      "0.092727914\n",
      "0.051080547\n",
      "0.079540476\n",
      "0.09213734\n",
      "0.077188015\n",
      "0.0732867\n",
      "0.12295067\n",
      "0.22838332\n",
      "0.11610529\n",
      "0.061841503\n",
      "0.07568629\n",
      "0.1013962\n",
      "0.108929984\n",
      "0.1882587\n",
      "0.073754326\n",
      "0.18986015\n",
      "0.09764556\n",
      "0.05081255\n",
      "0.08452241\n",
      "0.08671363\n",
      "0.07426551\n",
      "0.0710758\n",
      "0.12235792\n",
      "0.23017192\n",
      "0.1190812\n",
      "0.06291717\n",
      "0.079343796\n",
      "0.09806561\n",
      "0.1024777\n",
      "0.17869525\n",
      "0.07237659\n",
      "0.18898389\n",
      "0.09980546\n",
      "0.0506712\n",
      "0.08777606\n",
      "0.08231474\n",
      "0.07133782\n",
      "0.06741255\n",
      "0.118290395\n",
      "0.22530013\n",
      "0.11879033\n",
      "0.063491456\n",
      "0.08126093\n",
      "0.096598074\n",
      "0.09796191\n",
      "0.16942248\n",
      "0.0705669\n",
      "0.18435629\n",
      "0.09898074\n",
      "0.06537817\n",
      "0.09462481\n",
      "0.10948861\n",
      "0.076148085\n",
      "0.071739435\n",
      "0.1178316\n",
      "0.20405874\n",
      "0.11594802\n",
      "0.06570226\n",
      "0.07340381\n",
      "0.11447127\n",
      "0.11574653\n",
      "0.1915643\n",
      "0.078923486\n",
      "0.17790188\n",
      "0.091821514\n",
      "0.05679388\n",
      "0.0946696\n",
      "0.099679746\n",
      "0.08505843\n",
      "0.071116656\n",
      "0.12771757\n",
      "0.2259494\n",
      "0.12205263\n",
      "0.06356213\n",
      "0.08280848\n",
      "0.10433611\n",
      "0.11082014\n",
      "0.19034918\n",
      "0.07956325\n",
      "0.18815514\n",
      "0.09760613\n",
      "0.05170775\n",
      "0.097053714\n",
      "0.09060191\n",
      "0.080552734\n",
      "0.068042465\n",
      "0.124585554\n",
      "0.22935958\n",
      "0.12396768\n",
      "0.06463759\n",
      "0.08658288\n",
      "0.101735055\n",
      "0.10556465\n",
      "0.17921068\n",
      "0.07756518\n",
      "0.18836142\n",
      "0.10016796\n",
      "0.051696185\n",
      "0.09728544\n",
      "0.08742129\n",
      "0.07933396\n",
      "0.06541163\n",
      "0.12081584\n",
      "0.22594939\n",
      "0.12395466\n",
      "0.06510459\n",
      "0.08726725\n",
      "0.09936416\n",
      "0.10051172\n",
      "0.17171416\n",
      "0.076351225\n",
      "0.1840769\n",
      "0.09875904\n",
      "0.05132504\n",
      "0.09779048\n",
      "0.084766194\n",
      "0.077846035\n",
      "0.06340806\n",
      "0.11769908\n",
      "0.2199586\n",
      "0.12209046\n",
      "0.065015905\n",
      "0.087541744\n",
      "0.09837696\n",
      "0.09773188\n",
      "0.16579163\n",
      "0.07477084\n",
      "0.18072706\n",
      "0.09759374\n",
      "0.051306564\n",
      "0.097580835\n",
      "0.08336012\n",
      "0.07649874\n",
      "0.062196653\n",
      "0.11504817\n",
      "0.216242\n",
      "0.12004913\n",
      "0.06507069\n",
      "0.08718105\n",
      "0.09753228\n",
      "0.095881894\n",
      "0.16233647\n",
      "0.07376728\n",
      "0.17832656\n",
      "0.09637319\n",
      "0.0511274\n",
      "0.09727028\n",
      "0.08240223\n",
      "0.07595753\n",
      "0.049283557\n",
      "0.08815168\n",
      "0.16797909\n",
      "0.107355095\n",
      "0.06879465\n",
      "0.08029597\n",
      "0.0985477\n",
      "0.088199995\n",
      "0.13272619\n",
      "0.06774066\n",
      "0.15311739\n",
      "0.08405587\n",
      "0.061453093\n",
      "0.09518735\n",
      "0.087952435\n",
      "0.07234182\n",
      "0.05848311\n",
      "0.106792256\n",
      "0.18828574\n",
      "0.10966699\n",
      "0.06874183\n",
      "0.0745782\n",
      "0.103922844\n",
      "0.10468249\n",
      "0.16476424\n",
      "0.07342632\n",
      "0.16948053\n",
      "0.088786945\n",
      "0.058442295\n",
      "0.09104794\n",
      "0.090422824\n",
      "0.07539911\n",
      "0.06637467\n",
      "0.12136294\n",
      "0.21503049\n",
      "0.117161565\n",
      "0.067563504\n",
      "0.07373375\n",
      "0.10274275\n",
      "0.11010511\n",
      "0.18031345\n",
      "0.079154484\n",
      "0.18639281\n",
      "0.09545046\n",
      "0.057944763\n",
      "0.09288845\n",
      "0.09023923\n",
      "0.07592889\n",
      "0.06852938\n",
      "0.12753278\n",
      "0.2293874\n",
      "0.12442687\n",
      "0.06797137\n",
      "0.07599595\n",
      "0.10113932\n",
      "0.10850924\n",
      "0.17888238\n",
      "0.080695644\n",
      "0.1920723\n",
      "0.099827155\n",
      "0.057076335\n",
      "0.09673487\n",
      "0.08685918\n",
      "0.07438719\n",
      "0.06716428\n",
      "0.1266107\n",
      "0.23189552\n",
      "0.12723754\n",
      "0.06838862\n",
      "0.07826416\n",
      "0.09996754\n",
      "0.10409711\n",
      "0.1727523\n",
      "0.07960383\n",
      "0.19194752\n",
      "0.10197383\n",
      "0.057153065\n",
      "0.09850874\n",
      "0.08398463\n",
      "0.07262403\n",
      "0.06470866\n",
      "0.12359184\n",
      "0.22877026\n",
      "0.12702903\n",
      "0.06869966\n",
      "0.07936144\n",
      "0.099337116\n",
      "0.10018256\n",
      "0.16625887\n",
      "0.077669874\n",
      "0.18848303\n",
      "0.10155196\n",
      "0.05705585\n",
      "0.09949251\n",
      "0.0817197\n",
      "0.07143653\n",
      "0.06271168\n",
      "0.12006849\n",
      "0.2233813\n",
      "0.12555647\n",
      "0.070344806\n",
      "0.08237593\n",
      "0.09549531\n",
      "0.09609058\n",
      "0.16464002\n",
      "0.08042016\n",
      "0.19075663\n",
      "0.10270956\n",
      "0.0665247\n",
      "0.100192726\n",
      "0.08784419\n",
      "0.072077334\n",
      "0.06471929\n",
      "0.12137437\n",
      "0.22063702\n",
      "0.13298003\n",
      "0.07427221\n",
      "0.08095538\n",
      "0.10155707\n",
      "0.10117896\n",
      "0.16531259\n",
      "0.0799045\n",
      "0.18713672\n",
      "0.09986923\n",
      "0.06345602\n",
      "0.09726595\n",
      "0.087822266\n",
      "0.074510604\n",
      "0.06494707\n",
      "0.12077757\n",
      "0.22101504\n",
      "0.12978789\n",
      "0.071074754\n",
      "0.081310116\n",
      "0.09881559\n",
      "0.0988432\n",
      "0.16662455\n",
      "0.079559565\n",
      "0.18845889\n",
      "0.100819536\n",
      "0.062246554\n",
      "0.09842205\n",
      "0.08555131\n",
      "0.07334687\n",
      "0.06418245\n",
      "0.11990416\n",
      "0.22147232\n",
      "0.13038264\n",
      "0.07188702\n",
      "0.080965966\n",
      "0.09886145\n",
      "0.09830706\n",
      "0.16479799\n",
      "0.079287305\n",
      "0.18800023\n",
      "0.10116177\n",
      "0.062486056\n",
      "0.09805011\n",
      "0.085053384\n",
      "0.07270233\n",
      "0.063820854\n",
      "0.119595096\n",
      "0.22073916\n",
      "0.13044189\n",
      "0.07168744\n",
      "0.081488825\n",
      "0.098040946\n",
      "0.09604807\n",
      "0.16156833\n",
      "0.07833433\n",
      "0.18614182\n",
      "0.101245165\n",
      "0.06194587\n",
      "0.09925348\n",
      "0.08308111\n",
      "0.07200737\n",
      "0.06254361\n",
      "0.11747551\n",
      "0.2179701\n",
      "0.1297305\n",
      "0.071990795\n",
      "0.08153027\n",
      "0.09789947\n",
      "0.09441593\n",
      "0.15811276\n",
      "0.077299595\n",
      "0.18339348\n",
      "0.100673415\n",
      "0.061723232\n",
      "0.09932502\n",
      "0.08190168\n",
      "0.07140319\n",
      "0.061580166\n",
      "0.11536241\n",
      "0.21480608\n",
      "0.12832431\n",
      "0.071923554\n",
      "0.08133242\n",
      "0.09743363\n",
      "0.0926583\n",
      "0.091909796\n",
      "0.07712141\n",
      "0.10103691\n",
      "0.07096392\n",
      "0.06421672\n",
      "0.102767214\n",
      "0.079349354\n",
      "0.095098265\n",
      "0.05689674\n",
      "0.0774782\n",
      "0.12030981\n",
      "0.102722265\n",
      "0.062463656\n",
      "0.08845846\n",
      "0.10493359\n",
      "0.09469515\n",
      "0.1315078\n",
      "0.07062656\n",
      "0.123575866\n",
      "0.068353415\n",
      "0.061796255\n",
      "0.08320223\n",
      "0.08378931\n",
      "0.07832455\n",
      "0.059631832\n",
      "0.09661807\n",
      "0.16130795\n",
      "0.10645155\n",
      "0.06144601\n",
      "0.0857612\n",
      "0.09930868\n",
      "0.09904076\n",
      "0.17051679\n",
      "0.0781004\n",
      "0.15640306\n",
      "0.07399726\n",
      "0.06016367\n",
      "0.08413685\n",
      "0.085146606\n",
      "0.0801042\n",
      "0.06401794\n",
      "0.11092205\n",
      "0.19001445\n",
      "0.11944676\n",
      "0.06348291\n",
      "0.0869796\n",
      "0.10110257\n",
      "0.101968765\n",
      "0.17821027\n",
      "0.08482162\n",
      "0.17280823\n",
      "0.08049755\n",
      "0.058706738\n",
      "0.084735036\n",
      "0.08341927\n",
      "0.07770357\n",
      "0.06589288\n",
      "0.11658609\n",
      "0.20354645\n",
      "0.1257669\n",
      "0.06529763\n",
      "0.08877644\n",
      "0.09867619\n",
      "0.09873851\n",
      "0.17752491\n",
      "0.08540895\n",
      "0.1794231\n",
      "0.084320225\n",
      "0.05876104\n",
      "0.08557466\n",
      "0.08057352\n",
      "0.07750289\n",
      "0.06468777\n",
      "0.11527842\n",
      "0.20378736\n",
      "0.12690973\n",
      "0.06681552\n",
      "0.09081717\n",
      "0.09799504\n",
      "0.095343806\n",
      "0.1705445\n",
      "0.08425185\n",
      "0.17825846\n",
      "0.08497179\n",
      "0.05866348\n",
      "0.08668987\n",
      "0.0788202\n",
      "0.07710186\n",
      "0.06323666\n",
      "0.11294742\n",
      "0.20099676\n",
      "0.12604551\n",
      "0.06703435\n",
      "0.091696255\n",
      "0.09740262\n",
      "0.092936784\n",
      "0.16500683\n",
      "0.08279135\n",
      "0.17603014\n",
      "0.08455271\n",
      "0.06964225\n",
      "0.09809628\n",
      "0.101318486\n",
      "0.081070796\n",
      "0.07161521\n",
      "0.11625032\n",
      "0.17885387\n",
      "0.12141731\n",
      "0.06652457\n",
      "0.08651193\n",
      "0.10812285\n",
      "0.10666562\n",
      "0.18151093\n",
      "0.08931248\n",
      "0.15584528\n",
      "0.079293646\n",
      "0.06361464\n",
      "0.10041259\n",
      "0.09112511\n",
      "0.08876496\n",
      "0.06862645\n",
      "0.116678745\n",
      "0.19109902\n",
      "0.12681563\n",
      "0.06970667\n",
      "0.09758597\n",
      "0.10077292\n",
      "0.09890833\n",
      "0.17439808\n",
      "0.0900153\n",
      "0.16146721\n",
      "0.082016\n",
      "0.060737714\n",
      "0.10043511\n",
      "0.084211014\n",
      "0.08692703\n",
      "0.065953724\n",
      "0.112517715\n",
      "0.19063796\n",
      "0.12821572\n",
      "0.07109402\n",
      "0.100100726\n",
      "0.10132601\n",
      "0.09462479\n",
      "0.16348043\n",
      "0.086836115\n",
      "0.15879941\n",
      "0.08202189\n",
      "0.0608034\n",
      "0.10082188\n",
      "0.08213015\n",
      "0.08686432\n",
      "0.06431804\n",
      "0.10892772\n",
      "0.18571807\n",
      "0.1265307\n",
      "0.071730524\n",
      "0.10006626\n",
      "0.100735486\n",
      "0.09132174\n",
      "0.15740994\n",
      "0.08476021\n",
      "0.15430172\n",
      "0.08076843\n",
      "0.060426913\n",
      "0.10013017\n",
      "0.08020656\n",
      "0.08704804\n",
      "0.062487707\n",
      "0.1051141\n",
      "0.17770709\n",
      "0.122492164\n",
      "0.0710347\n",
      "0.10136093\n",
      "0.10026239\n",
      "0.08966948\n",
      "0.15108228\n",
      "0.08232114\n",
      "0.14833549\n",
      "0.07855058\n",
      "0.059845302\n",
      "0.099531025\n",
      "0.079453476\n",
      "0.086314835\n",
      "0.062085256\n",
      "0.103459634\n",
      "0.17537421\n",
      "0.12041276\n",
      "0.070462696\n",
      "0.09893659\n",
      "0.09970525\n",
      "0.088568486\n",
      "0.15023854\n",
      "0.08174166\n",
      "0.14798486\n",
      "0.077736\n",
      "0.060148716\n",
      "0.09784554\n",
      "0.07935109\n",
      "0.08577643\n",
      "0.060684077\n",
      "0.09532215\n",
      "0.14442863\n",
      "0.11021702\n",
      "0.071883745\n",
      "0.09632153\n",
      "0.10304939\n",
      "0.087819874\n",
      "0.13445663\n",
      "0.075353764\n",
      "0.13263546\n",
      "0.07503461\n",
      "0.06884948\n",
      "0.09975128\n",
      "0.0885344\n",
      "0.083283484\n",
      "0.062136706\n",
      "0.10728516\n",
      "0.15629438\n",
      "0.11306732\n",
      "0.06906502\n",
      "0.09189\n",
      "0.10628756\n",
      "0.097495824\n",
      "0.15555362\n",
      "0.08125753\n",
      "0.14386998\n",
      "0.07634447\n",
      "0.06570672\n",
      "0.09687617\n",
      "0.08855974\n",
      "0.082618\n",
      "0.06613539\n",
      "0.115934275\n",
      "0.17286679\n",
      "0.117301665\n",
      "0.06942386\n",
      "0.0906983\n",
      "0.10545514\n",
      "0.09977689\n",
      "0.1664095\n",
      "0.08598223\n",
      "0.15540883\n",
      "0.08002082\n",
      "0.06584119\n",
      "0.096743554\n",
      "0.08872892\n",
      "0.08271846\n",
      "0.06774207\n",
      "0.12056978\n",
      "0.18084268\n",
      "0.12217259\n",
      "0.069915704\n",
      "0.09106806\n",
      "0.1045929\n",
      "0.09912096\n",
      "0.16795808\n",
      "0.08754996\n",
      "0.16031444\n",
      "0.0819878\n",
      "0.065546066\n",
      "0.09695346\n",
      "0.08762496\n",
      "0.08196366\n",
      "0.06813844\n",
      "0.12183234\n",
      "0.18385972\n",
      "0.12434159\n",
      "0.07046867\n",
      "0.09183339\n",
      "0.10372431\n",
      "0.09722294\n",
      "0.16556919\n",
      "0.08742114\n",
      "0.16200368\n",
      "0.08295996\n",
      "0.06535046\n",
      "0.09741267\n",
      "0.08644921\n",
      "0.081707925\n",
      "0.067692086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12084999\n",
      "0.1839369\n",
      "0.12459171\n",
      "0.0707087\n",
      "0.09237997\n",
      "0.10325609\n",
      "0.09562625\n",
      "0.16294551\n",
      "0.086812556\n",
      "0.16161233\n",
      "0.083108194\n",
      "0.065090045\n",
      "0.0974317\n",
      "0.08542827\n",
      "0.081525035\n",
      "0.06716244\n",
      "0.119632214\n",
      "0.18284345\n",
      "0.124350294\n",
      "0.072166376\n",
      "0.090283036\n",
      "0.10205118\n",
      "0.097015776\n",
      "0.16840443\n",
      "0.0860924\n",
      "0.16372973\n",
      "0.08556707\n",
      "0.07092777\n",
      "0.09776151\n",
      "0.092944145\n",
      "0.0809544\n",
      "0.06996753\n",
      "0.1255033\n",
      "0.18410312\n",
      "0.12646441\n",
      "0.07359713\n",
      "0.09119123\n",
      "0.1070747\n",
      "0.100418866\n",
      "0.16861613\n",
      "0.08799539\n",
      "0.16468409\n",
      "0.08363587\n",
      "0.0687629\n",
      "0.096892156\n",
      "0.092582695\n",
      "0.08246632\n",
      "0.0705212\n",
      "0.12588748\n",
      "0.18711758\n",
      "0.12673396\n",
      "0.0723439\n",
      "0.09185554\n",
      "0.10550046\n",
      "0.098361015\n",
      "0.16846737\n",
      "0.08823728\n",
      "0.16585866\n",
      "0.08435662\n",
      "0.06793157\n",
      "0.09675598\n",
      "0.09062781\n",
      "0.082020774\n",
      "0.07042122\n",
      "0.12539378\n",
      "0.1873569\n",
      "0.1272033\n",
      "0.07303816\n",
      "0.09149498\n",
      "0.10529413\n",
      "0.09772564\n",
      "0.16726232\n",
      "0.08820828\n",
      "0.16619395\n",
      "0.084659\n",
      "0.06792036\n",
      "0.096496984\n",
      "0.09014519\n",
      "0.08143382\n",
      "0.07020664\n",
      "0.1250222\n",
      "0.18692392\n",
      "0.12735477\n",
      "0.07296871\n",
      "0.091735065\n",
      "0.10469775\n",
      "0.09647633\n",
      "0.16505021\n",
      "0.087758265\n",
      "0.16563983\n",
      "0.084940635\n",
      "0.067538716\n",
      "0.09664823\n",
      "0.088716224\n",
      "0.0808687\n",
      "0.069707364\n",
      "0.12353459\n",
      "0.18585886\n",
      "0.12696268\n",
      "0.07307146\n",
      "0.09179876\n",
      "0.10423781\n",
      "0.09499258\n",
      "0.16299526\n",
      "0.08689943\n",
      "0.1648111\n",
      "0.08462621\n",
      "0.06747663\n",
      "0.096507564\n",
      "0.0880957\n",
      "0.08055231\n",
      "0.06921454\n",
      "0.12222868\n",
      "0.18440694\n",
      "0.12622987\n",
      "0.07288526\n",
      "0.09191678\n",
      "0.10399548\n",
      "0.09406139\n",
      "0.0992894\n",
      "0.07648515\n",
      "0.09246446\n",
      "0.07180946\n",
      "0.066878125\n",
      "0.104793675\n",
      "0.08523643\n",
      "0.1086058\n",
      "0.06464119\n",
      "0.08298269\n",
      "0.11634749\n",
      "0.1085601\n",
      "0.0653883\n",
      "0.09942732\n",
      "0.10224084\n",
      "0.098510675\n",
      "0.14472544\n",
      "0.07839767\n",
      "0.11102441\n",
      "0.06889028\n",
      "0.06561976\n",
      "0.08617202\n",
      "0.08667689\n",
      "0.09544501\n",
      "0.06727317\n",
      "0.093805894\n",
      "0.14074436\n",
      "0.104997195\n",
      "0.06444059\n",
      "0.09316225\n",
      "0.10048155\n",
      "0.10655174\n",
      "0.18444204\n",
      "0.08657454\n",
      "0.12726538\n",
      "0.07097998\n",
      "0.06417071\n",
      "0.085218415\n",
      "0.09112341\n",
      "0.09829092\n",
      "0.07213055\n",
      "0.106487095\n",
      "0.16093275\n",
      "0.11236454\n",
      "0.06510562\n",
      "0.09067331\n",
      "0.103218794\n",
      "0.11270179\n",
      "0.20137604\n",
      "0.09363644\n",
      "0.13943782\n",
      "0.073645994\n",
      "0.0634685\n",
      "0.08550384\n",
      "0.09327395\n",
      "0.09626908\n",
      "0.07693991\n",
      "0.11432506\n",
      "0.17355153\n",
      "0.11724083\n",
      "0.066167146\n",
      "0.089874126\n",
      "0.102259524\n",
      "0.11296791\n",
      "0.20850903\n",
      "0.09713267\n",
      "0.14730947\n",
      "0.07687547\n",
      "0.0624041\n",
      "0.085739315\n",
      "0.09226989\n",
      "0.0956418\n",
      "0.07795909\n",
      "0.11693662\n",
      "0.17953025\n",
      "0.12004397\n",
      "0.067745574\n",
      "0.091315314\n",
      "0.10116042\n",
      "0.11074696\n",
      "0.20674077\n",
      "0.09826039\n",
      "0.15040171\n",
      "0.079035245\n",
      "0.061841656\n",
      "0.0868088\n",
      "0.09037824\n",
      "0.094163656\n",
      "0.07702857\n",
      "0.11678325\n",
      "0.18141136\n",
      "0.121887274\n",
      "0.06907151\n",
      "0.09336664\n",
      "0.10014035\n",
      "0.107540496\n",
      "0.20135373\n",
      "0.09744169\n",
      "0.15087238\n",
      "0.080407344\n",
      "0.06744355\n",
      "0.092204854\n",
      "0.0994202\n",
      "0.10400058\n",
      "0.08227183\n",
      "0.11998038\n",
      "0.16871047\n",
      "0.11676093\n",
      "0.064806126\n",
      "0.0905209\n",
      "0.1115329\n",
      "0.11474702\n",
      "0.22234178\n",
      "0.09618794\n",
      "0.14265819\n",
      "0.077958345\n",
      "0.06638152\n",
      "0.09277084\n",
      "0.10003384\n",
      "0.110570446\n",
      "0.08247517\n",
      "0.12066114\n",
      "0.17570625\n",
      "0.11664271\n",
      "0.06491677\n",
      "0.09592563\n",
      "0.10703669\n",
      "0.11272153\n",
      "0.22384639\n",
      "0.09830791\n",
      "0.14682165\n",
      "0.081177115\n",
      "0.0633408\n",
      "0.09179902\n",
      "0.09576647\n",
      "0.105679736\n",
      "0.081135\n",
      "0.121237874\n",
      "0.18019369\n",
      "0.12059565\n",
      "0.06605165\n",
      "0.09617048\n",
      "0.10525896\n",
      "0.108078614\n",
      "0.21750824\n",
      "0.09674148\n",
      "0.14916481\n",
      "0.0827805\n",
      "0.06288203\n",
      "0.09254398\n",
      "0.09385778\n",
      "0.10314378\n",
      "0.0801103\n",
      "0.120406464\n",
      "0.18051198\n",
      "0.122262165\n",
      "0.067440264\n",
      "0.09733274\n",
      "0.10374774\n",
      "0.104983464\n",
      "0.21276589\n",
      "0.095291235\n",
      "0.14870963\n",
      "0.083401576\n",
      "0.062239356\n",
      "0.09262046\n",
      "0.09207618\n",
      "0.10204644\n",
      "0.07860607\n",
      "0.11872179\n",
      "0.17906022\n",
      "0.122016095\n",
      "0.0678394\n",
      "0.09796765\n",
      "0.10266502\n",
      "0.10261982\n",
      "0.20883174\n",
      "0.09382516\n",
      "0.14810206\n",
      "0.08363936\n",
      "0.06191595\n",
      "0.09296309\n",
      "0.09084275\n",
      "0.10042335\n",
      "0.07767542\n",
      "0.11738554\n",
      "0.17795694\n",
      "0.12194959\n",
      "0.06835817\n",
      "0.098346725\n",
      "0.102023706\n",
      "0.10089323\n",
      "0.20557484\n",
      "0.0926188\n",
      "0.14729978\n",
      "0.083867304\n",
      "0.061652616\n",
      "0.093041226\n",
      "0.08969088\n",
      "0.099344485\n",
      "0.06526182\n",
      "0.09213106\n",
      "0.1347768\n",
      "0.10844528\n",
      "0.074190736\n",
      "0.0975142\n",
      "0.09894439\n",
      "0.08549653\n",
      "0.15310587\n",
      "0.08161992\n",
      "0.121295154\n",
      "0.0751148\n",
      "0.06689732\n",
      "0.093732074\n",
      "0.09338802\n",
      "0.085832745\n",
      "0.068245426\n",
      "0.10944456\n",
      "0.15296322\n",
      "0.11477617\n",
      "0.07054959\n",
      "0.09333548\n",
      "0.10383964\n",
      "0.09905644\n",
      "0.17854697\n",
      "0.08854923\n",
      "0.1328961\n",
      "0.077098295\n",
      "0.06482569\n",
      "0.09216601\n",
      "0.09548726\n",
      "0.08547363\n",
      "0.07455361\n",
      "0.1195717\n",
      "0.17198889\n",
      "0.12041709\n",
      "0.07075247\n",
      "0.09155766\n",
      "0.10394153\n",
      "0.10303875\n",
      "0.19446078\n",
      "0.09502838\n",
      "0.14476213\n",
      "0.08001119\n",
      "0.06452961\n",
      "0.09167977\n",
      "0.09621877\n",
      "0.08744096\n",
      "0.07748623\n",
      "0.12558311\n",
      "0.17963691\n",
      "0.12502879\n",
      "0.07137143\n",
      "0.09268582\n",
      "0.10288231\n",
      "0.10236712\n",
      "0.19626923\n",
      "0.09653728\n",
      "0.14951834\n",
      "0.082534954\n",
      "0.064344384\n",
      "0.09309977\n",
      "0.09484303\n",
      "0.08602142\n",
      "0.077653706\n",
      "0.12621742\n",
      "0.18264996\n",
      "0.12783119\n",
      "0.072363675\n",
      "0.094997786\n",
      "0.10186639\n",
      "0.09965131\n",
      "0.19223318\n",
      "0.09596088\n",
      "0.15095255\n",
      "0.08396482\n",
      "0.064093344\n",
      "0.09406153\n",
      "0.09320955\n",
      "0.08558738\n",
      "0.07621923\n",
      "0.124446735\n",
      "0.18197045\n",
      "0.1285747\n",
      "0.073004335\n",
      "0.096417226\n",
      "0.101247385\n",
      "0.096788675\n",
      "0.18731365\n",
      "0.094343446\n",
      "0.14983825\n",
      "0.08426827\n",
      "0.06394197\n",
      "0.09464908\n",
      "0.09178176\n",
      "0.08472091\n",
      "0.07486884\n",
      "0.122186765\n",
      "0.17968166\n",
      "0.12822531\n",
      "0.073681355\n",
      "0.09307736\n",
      "0.098782614\n",
      "0.100719504\n",
      "0.18900403\n",
      "0.09425347\n",
      "0.15099403\n",
      "0.084330074\n",
      "0.0685751\n",
      "0.09243003\n",
      "0.09592343\n",
      "0.084462054\n",
      "0.078649595\n",
      "0.12574205\n",
      "0.18170848\n",
      "0.1279743\n",
      "0.07633491\n",
      "0.09179433\n",
      "0.10668606\n",
      "0.10699816\n",
      "0.1942929\n",
      "0.09788387\n",
      "0.15270394\n",
      "0.083905585\n",
      "0.06748926\n",
      "0.09035459\n",
      "0.09690996\n",
      "0.08671141\n",
      "0.079645954\n",
      "0.12790298\n",
      "0.18451877\n",
      "0.12873955\n",
      "0.07440948\n",
      "0.09329203\n",
      "0.10398832\n",
      "0.1037574\n",
      "0.19325775\n",
      "0.097593926\n",
      "0.15486485\n",
      "0.08513422\n",
      "0.067010224\n",
      "0.09146695\n",
      "0.094458446\n",
      "0.085302435\n",
      "0.0790259\n",
      "0.12716217\n",
      "0.18534668\n",
      "0.13058092\n",
      "0.07555619\n",
      "0.093606055\n",
      "0.104182094\n",
      "0.102029905\n",
      "0.18967783\n",
      "0.09680599\n",
      "0.15461034\n",
      "0.08560316\n",
      "0.067076765\n",
      "0.09204324\n",
      "0.094226584\n",
      "0.08488673\n",
      "0.07793834\n",
      "0.12589054\n",
      "0.18393011\n",
      "0.1305512\n",
      "0.0754945\n",
      "0.094519645\n",
      "0.103358865\n",
      "0.09968017\n",
      "0.18551925\n",
      "0.09547986\n",
      "0.15361287\n",
      "0.08566514\n",
      "0.06667912\n",
      "0.092689686\n",
      "0.09269377\n",
      "0.08412147\n",
      "0.07680485\n",
      "0.12383823\n",
      "0.18227607\n",
      "0.13018507\n",
      "0.07550599\n",
      "0.09474614\n",
      "0.10304323\n",
      "0.0978874\n",
      "0.18237285\n",
      "0.094127774\n",
      "0.15249069\n",
      "0.085315436\n",
      "0.066804826\n",
      "0.09283397\n",
      "0.09208739\n",
      "0.08367421\n",
      "0.07584149\n",
      "0.12230668\n",
      "0.180683\n",
      "0.12960602\n",
      "0.07535209\n",
      "0.09489438\n",
      "0.102803975\n",
      "0.096680924\n",
      "0.10422355\n",
      "0.074782446\n",
      "0.094005294\n",
      "0.07716768\n",
      "0.06564902\n",
      "0.10356036\n",
      "0.08759539\n",
      "0.114167\n",
      "0.069304585\n",
      "0.09206033\n",
      "0.123050034\n",
      "0.109086685\n",
      "0.06200759\n",
      "0.10508906\n",
      "0.10927118\n",
      "0.09768868\n",
      "0.1648967\n",
      "0.076295726\n",
      "0.112660535\n",
      "0.07483112\n",
      "0.06498941\n",
      "0.08656473\n",
      "0.09017404\n",
      "0.10535309\n",
      "0.07742728\n",
      "0.10700126\n",
      "0.14875191\n",
      "0.10626796\n",
      "0.060209665\n",
      "0.09682224\n",
      "0.106833935\n",
      "0.10750994\n",
      "0.20825872\n",
      "0.085139945\n",
      "0.12826447\n",
      "0.07828861\n",
      "0.06480211\n",
      "0.087499835\n",
      "0.0956582\n",
      "0.10628134\n",
      "0.084703244\n",
      "0.12067289\n",
      "0.16955368\n",
      "0.11321422\n",
      "0.061706327\n",
      "0.09409252\n",
      "0.10952128\n",
      "0.11410836\n",
      "0.22513738\n",
      "0.09200619\n",
      "0.13855687\n",
      "0.08114338\n",
      "0.06418118\n",
      "0.08641094\n",
      "0.097089335\n",
      "0.106151976\n",
      "0.08959453\n",
      "0.12880465\n",
      "0.17972058\n",
      "0.11655872\n",
      "0.06252204\n",
      "0.09348318\n",
      "0.108177185\n",
      "0.11459218\n",
      "0.23353077\n",
      "0.09457297\n",
      "0.14496115\n",
      "0.08330373\n",
      "0.06379407\n",
      "0.08629432\n",
      "0.09670749\n",
      "0.1059438\n",
      "0.09062713\n",
      "0.13157949\n",
      "0.18505439\n",
      "0.11879063\n",
      "0.06373027\n",
      "0.09410211\n",
      "0.10755435\n",
      "0.11377629\n",
      "0.23439763\n",
      "0.09566372\n",
      "0.14803262\n",
      "0.08489461\n",
      "0.06328927\n",
      "0.08656322\n",
      "0.09549339\n",
      "0.104798846\n",
      "0.09021759\n",
      "0.132593\n",
      "0.1879528\n",
      "0.12027224\n",
      "0.06464475\n",
      "0.09525542\n",
      "0.10654655\n",
      "0.11166722\n",
      "0.23268268\n",
      "0.095586345\n",
      "0.14999071\n",
      "0.08618351\n",
      "0.0630907\n",
      "0.0913359\n",
      "0.10111306\n",
      "0.108614355\n",
      "0.09034002\n",
      "0.1331698\n",
      "0.18353656\n",
      "0.11637965\n",
      "0.06211496\n",
      "0.0959585\n",
      "0.11523332\n",
      "0.11181134\n",
      "0.23177978\n",
      "0.090860665\n",
      "0.14191589\n",
      "0.086504236\n",
      "0.064895056\n",
      "0.09377048\n",
      "0.100896336\n",
      "0.11346966\n",
      "0.08991486\n",
      "0.13249835\n",
      "0.18715307\n",
      "0.11693338\n",
      "0.06313589\n",
      "0.10021503\n",
      "0.11102574\n",
      "0.10955199\n",
      "0.2290942\n",
      "0.0940628\n",
      "0.14349458\n",
      "0.08826119\n",
      "0.062535\n",
      "0.09372795\n",
      "0.096805446\n",
      "0.109430596\n",
      "0.08793156\n",
      "0.13149923\n",
      "0.18808466\n",
      "0.11833422\n",
      "0.06413268\n",
      "0.102665156\n",
      "0.10896042\n",
      "0.1051464\n",
      "0.22324947\n",
      "0.09203127\n",
      "0.14526564\n",
      "0.09004302\n",
      "0.06296389\n",
      "0.09430722\n",
      "0.09443115\n",
      "0.10730495\n",
      "0.08617591\n",
      "0.13004969\n",
      "0.18714097\n",
      "0.11941851\n",
      "0.065416425\n",
      "0.10409053\n",
      "0.10794856\n",
      "0.102397926\n",
      "0.21794492\n",
      "0.09109596\n",
      "0.144865\n",
      "0.09055601\n",
      "0.062814444\n",
      "0.0942607\n",
      "0.09268307\n",
      "0.10626533\n",
      "0.084598795\n",
      "0.12821695\n",
      "0.18520308\n",
      "0.11917603\n",
      "0.06582293\n",
      "0.10514523\n",
      "0.10693139\n",
      "0.10012179\n",
      "0.21368766\n",
      "0.089671284\n",
      "0.14427216\n",
      "0.09079048\n",
      "0.062714756\n",
      "0.094604865\n",
      "0.091359\n",
      "0.10499358\n",
      "0.08337796\n",
      "0.12640503\n",
      "0.18333377\n",
      "0.1187198\n",
      "0.0663416\n",
      "0.10573126\n",
      "0.106275015\n",
      "0.098571524\n",
      "0.21023792\n",
      "0.08851847\n",
      "0.1436983\n",
      "0.090755574\n",
      "0.062755324\n",
      "0.09458854\n",
      "0.09052637\n",
      "0.10440306\n",
      "0.06868385\n",
      "0.09219353\n",
      "0.13025391\n",
      "0.10886611\n",
      "0.072072685\n",
      "0.10890373\n",
      "0.1027646\n",
      "0.08496229\n",
      "0.16449133\n",
      "0.082948215\n",
      "0.11542336\n",
      "0.0778209\n",
      "0.06782481\n",
      "0.093358636\n",
      "0.09344459\n",
      "0.09116326\n",
      "0.071787134\n",
      "0.10933463\n",
      "0.15277798\n",
      "0.116452225\n",
      "0.068726726\n",
      "0.10000695\n",
      "0.10551274\n",
      "0.09876526\n",
      "0.19660729\n",
      "0.09016339\n",
      "0.12641296\n",
      "0.07991517\n",
      "0.06576737\n",
      "0.09139551\n",
      "0.095131926\n",
      "0.09171955\n",
      "0.07844039\n",
      "0.1205046\n",
      "0.1702094\n",
      "0.1210978\n",
      "0.06994459\n",
      "0.098519154\n",
      "0.10549818\n",
      "0.10280291\n",
      "0.21190603\n",
      "0.0959661\n",
      "0.13677591\n",
      "0.08307837\n",
      "0.0657752\n",
      "0.09070271\n",
      "0.09597072\n",
      "0.09352331\n",
      "0.080855265\n",
      "0.12605633\n",
      "0.17861241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12541181\n",
      "0.07099344\n",
      "0.09969969\n",
      "0.105308756\n",
      "0.10270364\n",
      "0.21437684\n",
      "0.09728609\n",
      "0.14079128\n",
      "0.085269146\n",
      "0.06564235\n",
      "0.0914946\n",
      "0.09522193\n",
      "0.092348844\n",
      "0.08122046\n",
      "0.12742856\n",
      "0.18179524\n",
      "0.12744261\n",
      "0.071911104\n",
      "0.10128402\n",
      "0.10444452\n",
      "0.10089583\n",
      "0.21204495\n",
      "0.096826166\n",
      "0.1423618\n",
      "0.08635823\n",
      "0.06556913\n",
      "0.09204829\n",
      "0.094203606\n",
      "0.09193924\n",
      "0.08038991\n",
      "0.12648347\n",
      "0.18178853\n",
      "0.1280519\n",
      "0.07256006\n",
      "0.10213742\n",
      "0.10390007\n",
      "0.09876882\n",
      "0.20834804\n",
      "0.095607504\n",
      "0.14183994\n",
      "0.08660743\n",
      "0.06566243\n",
      "0.09216497\n",
      "0.0935418\n",
      "0.091291316\n",
      "0.07942501\n",
      "0.12493069\n",
      "0.18025438\n",
      "0.12761834\n",
      "0.069746934\n",
      "0.09614428\n",
      "0.099372685\n",
      "0.10528932\n",
      "0.2077112\n",
      "0.09723122\n",
      "0.13779569\n",
      "0.083491996\n",
      "0.068616994\n",
      "0.08732608\n",
      "0.09441918\n",
      "0.09059987\n",
      "0.08223702\n",
      "0.1273613\n",
      "0.18236381\n",
      "0.13034542\n",
      "0.075375855\n",
      "0.09780411\n",
      "0.10694071\n",
      "0.109353326\n",
      "0.21120515\n",
      "0.101474546\n",
      "0.14197251\n",
      "0.08530222\n",
      "0.06836523\n",
      "0.08602535\n",
      "0.095154375\n",
      "0.092373155\n",
      "0.0823757\n",
      "0.12879327\n",
      "0.18536949\n",
      "0.13071015\n",
      "0.07371125\n",
      "0.099515304\n",
      "0.103904806\n",
      "0.105794325\n",
      "0.20902172\n",
      "0.100845896\n",
      "0.14450264\n",
      "0.086881444\n",
      "0.06795988\n",
      "0.08735622\n",
      "0.09288217\n",
      "0.09043371\n",
      "0.0814919\n",
      "0.12792073\n",
      "0.18595676\n",
      "0.13223204\n",
      "0.075207554\n",
      "0.10027226\n",
      "0.10414385\n",
      "0.103846595\n",
      "0.20567705\n",
      "0.09953211\n",
      "0.14424373\n",
      "0.087160915\n",
      "0.068315044\n",
      "0.08742621\n",
      "0.09320856\n",
      "0.090403594\n",
      "0.08051976\n",
      "0.12661561\n",
      "0.18427762\n",
      "0.13148019\n",
      "0.075233586\n",
      "0.10096249\n",
      "0.10351975\n",
      "0.1018496\n",
      "0.20166275\n",
      "0.09805386\n",
      "0.14301348\n",
      "0.087264076\n",
      "0.06796343\n",
      "0.087922245\n",
      "0.09203269\n",
      "0.089785695\n",
      "0.07954635\n",
      "0.124732286\n",
      "0.18231615\n",
      "0.13083035\n",
      "0.07540293\n",
      "0.101339236\n",
      "0.103213735\n",
      "0.10001544\n",
      "0.1981755\n",
      "0.09637627\n",
      "0.1418475\n",
      "0.08683352\n",
      "0.06808025\n",
      "0.08791409\n",
      "0.09184662\n",
      "0.08956353\n",
      "0.07871986\n",
      "0.12310743\n",
      "0.18011734\n",
      "0.1299521\n",
      "0.07533467\n",
      "0.1014334\n",
      "0.103176534\n",
      "0.09883729\n",
      "0.10568124\n",
      "0.07530019\n",
      "0.096890315\n",
      "0.081729375\n",
      "0.06297749\n",
      "0.10803957\n",
      "0.08721529\n",
      "0.11798538\n",
      "0.06579378\n",
      "0.08959037\n",
      "0.13378656\n",
      "0.10735984\n",
      "0.06284174\n",
      "0.114732906\n",
      "0.10763318\n",
      "0.09914229\n",
      "0.16327332\n",
      "0.07826149\n",
      "0.11787575\n",
      "0.081253946\n",
      "0.06216171\n",
      "0.09146641\n",
      "0.089901485\n",
      "0.106759906\n",
      "0.07676233\n",
      "0.109538704\n",
      "0.16043776\n",
      "0.10440171\n",
      "0.060010772\n",
      "0.102793284\n",
      "0.1051053\n",
      "0.10968519\n",
      "0.20678987\n",
      "0.08621686\n",
      "0.1318375\n",
      "0.08498555\n",
      "0.06133613\n",
      "0.09144549\n",
      "0.09495844\n",
      "0.10714173\n",
      "0.08420941\n",
      "0.12178229\n",
      "0.18054087\n",
      "0.112218924\n",
      "0.06292794\n",
      "0.0997453\n",
      "0.10772895\n",
      "0.11596286\n",
      "0.22450417\n",
      "0.09335852\n",
      "0.14254981\n",
      "0.088974655\n",
      "0.06147653\n",
      "0.09026014\n",
      "0.09607123\n",
      "0.107196696\n",
      "0.08894404\n",
      "0.12966834\n",
      "0.19188121\n",
      "0.11662185\n",
      "0.06442152\n",
      "0.1010377\n",
      "0.10570117\n",
      "0.11468578\n",
      "0.22933531\n",
      "0.09547323\n",
      "0.14848611\n",
      "0.09164368\n",
      "0.061547276\n",
      "0.09136191\n",
      "0.09448936\n",
      "0.1064561\n",
      "0.08891498\n",
      "0.13066037\n",
      "0.1955084\n",
      "0.11917617\n",
      "0.06671164\n",
      "0.10438612\n",
      "0.10466291\n",
      "0.11101141\n",
      "0.22499801\n",
      "0.09505743\n",
      "0.15078296\n",
      "0.09395638\n",
      "0.061903648\n",
      "0.09207798\n",
      "0.09238119\n",
      "0.10549798\n",
      "0.0870005\n",
      "0.1293315\n",
      "0.196024\n",
      "0.1203346\n",
      "0.068185285\n",
      "0.10729234\n",
      "0.10364807\n",
      "0.107414104\n",
      "0.21878491\n",
      "0.09366959\n",
      "0.1509851\n",
      "0.094976954\n",
      "0.05898136\n",
      "0.0940176\n",
      "0.101522595\n",
      "0.106081784\n",
      "0.08735836\n",
      "0.12535764\n",
      "0.17585132\n",
      "0.111166075\n",
      "0.06324435\n",
      "0.10227087\n",
      "0.11340173\n",
      "0.113168836\n",
      "0.22136535\n",
      "0.09155746\n",
      "0.14621729\n",
      "0.09165406\n",
      "0.061755978\n",
      "0.09570657\n",
      "0.10257086\n",
      "0.113021754\n",
      "0.08862452\n",
      "0.13025126\n",
      "0.18712272\n",
      "0.1128013\n",
      "0.06575914\n",
      "0.10679247\n",
      "0.11043632\n",
      "0.11206726\n",
      "0.22248082\n",
      "0.09754426\n",
      "0.15051533\n",
      "0.09502382\n",
      "0.060229074\n",
      "0.09556008\n",
      "0.09911496\n",
      "0.109936625\n",
      "0.08784609\n",
      "0.13114607\n",
      "0.1920015\n",
      "0.115880474\n",
      "0.06743409\n",
      "0.11062577\n",
      "0.10757074\n",
      "0.10679691\n",
      "0.2173112\n",
      "0.095471166\n",
      "0.15401301\n",
      "0.09791449\n",
      "0.060976613\n",
      "0.09703271\n",
      "0.09613237\n",
      "0.10845337\n",
      "0.085603625\n",
      "0.12926923\n",
      "0.19266938\n",
      "0.1178972\n",
      "0.0696147\n",
      "0.11306061\n",
      "0.10715173\n",
      "0.10369506\n",
      "0.2119021\n",
      "0.09458193\n",
      "0.15374269\n",
      "0.09887588\n",
      "0.061329648\n",
      "0.09735979\n",
      "0.09434412\n",
      "0.10830104\n",
      "0.08350096\n",
      "0.12710816\n",
      "0.1909832\n",
      "0.11805486\n",
      "0.07030994\n",
      "0.11466786\n",
      "0.10636094\n",
      "0.10108936\n",
      "0.20702331\n",
      "0.093286715\n",
      "0.15245143\n",
      "0.098993726\n",
      "0.061291687\n",
      "0.09786935\n",
      "0.092829905\n",
      "0.1072839\n",
      "0.081935\n",
      "0.12477858\n",
      "0.1887386\n",
      "0.117657796\n",
      "0.07085152\n",
      "0.115432866\n",
      "0.10588345\n",
      "0.0990723\n",
      "0.20316488\n",
      "0.09187794\n",
      "0.15109362\n",
      "0.09888777\n",
      "0.06146597\n",
      "0.09800695\n",
      "0.09197078\n",
      "0.10711397\n",
      "0.06557794\n",
      "0.089188725\n",
      "0.13630275\n",
      "0.10517209\n",
      "0.072110064\n",
      "0.1242387\n",
      "0.104059175\n",
      "0.08113857\n",
      "0.15883364\n",
      "0.08158191\n",
      "0.113579944\n",
      "0.08233008\n",
      "0.06576408\n",
      "0.099881776\n",
      "0.09113733\n",
      "0.0972311\n",
      "0.06878375\n",
      "0.105400965\n",
      "0.16000198\n",
      "0.113336466\n",
      "0.06937335\n",
      "0.110291466\n",
      "0.10653742\n",
      "0.09260197\n",
      "0.1881142\n",
      "0.087646626\n",
      "0.124274015\n",
      "0.08540514\n",
      "0.06391192\n",
      "0.09737225\n",
      "0.09198118\n",
      "0.09816887\n",
      "0.07437797\n",
      "0.1153814\n",
      "0.17548779\n",
      "0.117616214\n",
      "0.07042743\n",
      "0.10912337\n",
      "0.10652828\n",
      "0.09630303\n",
      "0.2032639\n",
      "0.09377751\n",
      "0.13391653\n",
      "0.088752374\n",
      "0.06419112\n",
      "0.09686595\n",
      "0.09243676\n",
      "0.09860984\n",
      "0.07682265\n",
      "0.120948985\n",
      "0.18394232\n",
      "0.1215436\n",
      "0.07155578\n",
      "0.11052695\n",
      "0.106305\n",
      "0.09662875\n",
      "0.2066417\n",
      "0.09505796\n",
      "0.13807039\n",
      "0.0914091\n",
      "0.06460942\n",
      "0.097653\n",
      "0.09177366\n",
      "0.097795874\n",
      "0.07706413\n",
      "0.12211287\n",
      "0.18624713\n",
      "0.12325414\n",
      "0.072930366\n",
      "0.11242165\n",
      "0.10556984\n",
      "0.09499525\n",
      "0.20495147\n",
      "0.09478752\n",
      "0.1392587\n",
      "0.09254527\n",
      "0.064937584\n",
      "0.09841317\n",
      "0.09113921\n",
      "0.097662896\n",
      "0.076237105\n",
      "0.120961234\n",
      "0.18577501\n",
      "0.123548046\n",
      "0.07365604\n",
      "0.113463804\n",
      "0.10525381\n",
      "0.09332275\n",
      "0.20169234\n",
      "0.093407765\n",
      "0.1383647\n",
      "0.09255985\n",
      "0.0650031\n",
      "0.09873366\n",
      "0.090596005\n",
      "0.09742189\n",
      "0.0754214\n",
      "0.11942524\n",
      "0.18392125\n",
      "0.12313475\n",
      "0.06653877\n",
      "0.100002736\n",
      "0.10091517\n",
      "0.09856614\n",
      "0.20627798\n",
      "0.09350532\n",
      "0.12925987\n",
      "0.08387125\n",
      "0.06591204\n",
      "0.09250872\n",
      "0.092631295\n",
      "0.09323095\n",
      "0.07837094\n",
      "0.12303495\n",
      "0.18566377\n",
      "0.12744658\n",
      "0.07549819\n",
      "0.10542015\n",
      "0.10884539\n",
      "0.10242673\n",
      "0.21092783\n",
      "0.10009048\n",
      "0.13718729\n",
      "0.089431085\n",
      "0.06608968\n",
      "0.09373643\n",
      "0.092078574\n",
      "0.096646994\n",
      "0.07847653\n",
      "0.12620184\n",
      "0.19273503\n",
      "0.1314405\n",
      "0.075051405\n",
      "0.10915153\n",
      "0.105105914\n",
      "0.097544886\n",
      "0.20774037\n",
      "0.09914622\n",
      "0.14038523\n",
      "0.09181014\n",
      "0.06639613\n",
      "0.09533592\n",
      "0.09015355\n",
      "0.09397637\n",
      "0.07696714\n",
      "0.12393764\n",
      "0.1924676\n",
      "0.13289727\n",
      "0.077295825\n",
      "0.11063097\n",
      "0.10586321\n",
      "0.096066535\n",
      "0.20258605\n",
      "0.09785463\n",
      "0.13935813\n",
      "0.09209947\n",
      "0.06662934\n",
      "0.095759474\n",
      "0.09037784\n",
      "0.094866455\n",
      "0.0756868\n",
      "0.12194921\n",
      "0.1897661\n",
      "0.13173732\n",
      "0.07685117\n",
      "0.111314274\n",
      "0.10477121\n",
      "0.09373692\n",
      "0.19841392\n",
      "0.09585197\n",
      "0.13751306\n",
      "0.09158243\n",
      "0.06648982\n",
      "0.09586717\n",
      "0.08987713\n",
      "0.09412663\n",
      "0.07480087\n",
      "0.119885355\n",
      "0.18697314\n",
      "0.13074794\n",
      "0.076911345\n",
      "0.110963926\n",
      "0.104971215\n",
      "0.09292361\n",
      "0.19525328\n",
      "0.09440654\n",
      "0.13565984\n",
      "0.09095864\n",
      "0.06627614\n",
      "0.095581874\n",
      "0.08990861\n",
      "0.094314024\n",
      "0.07421963\n",
      "0.1182587\n",
      "0.18469547\n",
      "0.12947296\n",
      "0.07645133\n",
      "0.11047767\n",
      "0.104581036\n",
      "0.09198658\n",
      "0.11054199\n",
      "0.0752768\n",
      "0.0990792\n",
      "0.08286682\n",
      "0.063214764\n",
      "0.10809632\n",
      "0.09147155\n",
      "0.11880963\n",
      "0.06283154\n",
      "0.08789126\n",
      "0.13158925\n",
      "0.10487694\n",
      "0.06725716\n",
      "0.116616555\n",
      "0.10935381\n",
      "0.09807244\n",
      "0.16135933\n",
      "0.08150664\n",
      "0.11755237\n",
      "0.08286507\n",
      "0.060220674\n",
      "0.090275615\n",
      "0.093689375\n",
      "0.105099924\n",
      "0.07568769\n",
      "0.107790455\n",
      "0.15607151\n",
      "0.10405531\n",
      "0.064824\n",
      "0.10740534\n",
      "0.106089815\n",
      "0.10460543\n",
      "0.19884282\n",
      "0.087570764\n",
      "0.13352953\n",
      "0.08715907\n",
      "0.060929157\n",
      "0.09049422\n",
      "0.098877445\n",
      "0.10448799\n",
      "0.08408429\n",
      "0.118780434\n",
      "0.17089228\n",
      "0.11021333\n",
      "0.06732594\n",
      "0.10600561\n",
      "0.10856951\n",
      "0.11021313\n",
      "0.21126677\n",
      "0.094648354\n",
      "0.14190122\n",
      "0.09060684\n",
      "0.06051689\n",
      "0.089249685\n",
      "0.098621756\n",
      "0.104661696\n",
      "0.08736631\n",
      "0.12552495\n",
      "0.18077387\n",
      "0.11468492\n",
      "0.069117576\n",
      "0.10814297\n",
      "0.107294664\n",
      "0.10813367\n",
      "0.21468578\n",
      "0.0958479\n",
      "0.14768746\n",
      "0.09401766\n",
      "0.06115319\n",
      "0.089942455\n",
      "0.09692774\n",
      "0.103606366\n",
      "0.08710233\n",
      "0.12658782\n",
      "0.18457916\n",
      "0.11733783\n",
      "0.071269296\n",
      "0.11074887\n",
      "0.10640904\n",
      "0.10600008\n",
      "0.21236326\n",
      "0.09592523\n",
      "0.14991216\n",
      "0.095948026\n",
      "0.06142337\n",
      "0.09113436\n",
      "0.09555884\n",
      "0.10329647\n",
      "0.085563615\n",
      "0.1259531\n",
      "0.18575728\n",
      "0.1184805\n",
      "0.072473705\n",
      "0.112985015\n",
      "0.105840385\n",
      "0.1039724\n",
      "0.2086866\n",
      "0.09528828\n",
      "0.14997225\n",
      "0.096944906\n",
      "0.05820598\n",
      "0.093420975\n",
      "0.1062174\n",
      "0.101571746\n",
      "0.090503104\n",
      "0.123766094\n",
      "0.16212304\n",
      "0.10564577\n",
      "0.065237954\n",
      "0.10240693\n",
      "0.112825066\n",
      "0.110037744\n",
      "0.2041905\n",
      "0.09162363\n",
      "0.14058572\n",
      "0.090231925\n",
      "0.061321974\n",
      "0.09573665\n",
      "0.10661949\n",
      "0.10870124\n",
      "0.08941556\n",
      "0.12678073\n",
      "0.169413\n",
      "0.10619959\n",
      "0.06796465\n",
      "0.10741975\n",
      "0.11002765\n",
      "0.108534455\n",
      "0.2039808\n",
      "0.0969453\n",
      "0.14343283\n",
      "0.09283829\n",
      "0.059859335\n",
      "0.095949955\n",
      "0.10242149\n",
      "0.105266035\n",
      "0.08869268\n",
      "0.1273294\n",
      "0.17273752\n",
      "0.10838109\n",
      "0.06931633\n",
      "0.11098156\n",
      "0.10735463\n",
      "0.104708165\n",
      "0.2007414\n",
      "0.09522149\n",
      "0.14608903\n",
      "0.09490429\n",
      "0.06046831\n",
      "0.09732338\n",
      "0.100145146\n",
      "0.10465561\n",
      "0.08674517\n",
      "0.1260995\n",
      "0.17327406\n",
      "0.109292604\n",
      "0.07060937\n",
      "0.11298569\n",
      "0.107184246\n",
      "0.10264926\n",
      "0.19736812\n",
      "0.09493207\n",
      "0.1457471\n",
      "0.095666066\n",
      "0.060636718\n",
      "0.09765483\n",
      "0.098619066\n",
      "0.10451752\n",
      "0.08527475\n",
      "0.12453348\n",
      "0.17276992\n",
      "0.10930182\n",
      "0.071094036\n",
      "0.11448923\n",
      "0.10632113\n",
      "0.10061823\n",
      "0.19426093\n",
      "0.09384844\n",
      "0.14506117\n",
      "0.09570949\n",
      "0.060687818\n",
      "0.098031044\n",
      "0.097579286\n",
      "0.103797905\n",
      "0.084090516\n",
      "0.12292147\n",
      "0.17176346\n",
      "0.10891503\n",
      "0.07146267\n",
      "0.11521959\n",
      "0.1061097\n",
      "0.099670336\n",
      "0.19140777\n",
      "0.09321412\n",
      "0.14397344\n",
      "0.09557848\n",
      "0.060588434\n",
      "0.09801785\n",
      "0.09670802\n",
      "0.10365406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.062854715\n",
      "0.083602905\n",
      "0.13128804\n",
      "0.09934285\n",
      "0.07342457\n",
      "0.12487075\n",
      "0.10326804\n",
      "0.079305515\n",
      "0.15499839\n",
      "0.07950437\n",
      "0.11144707\n",
      "0.08155927\n",
      "0.0657886\n",
      "0.09636252\n",
      "0.09265805\n",
      "0.09899214\n",
      "0.06806864\n",
      "0.09980243\n",
      "0.15678394\n",
      "0.108150385\n",
      "0.07246706\n",
      "0.11182997\n",
      "0.10647619\n",
      "0.08967797\n",
      "0.18313143\n",
      "0.08782963\n",
      "0.12549406\n",
      "0.08705932\n",
      "0.06386549\n",
      "0.09442964\n",
      "0.093081445\n",
      "0.099295355\n",
      "0.0749299\n",
      "0.11040294\n",
      "0.17313904\n",
      "0.115082465\n",
      "0.0744967\n",
      "0.11277021\n",
      "0.105999224\n",
      "0.09134168\n",
      "0.1950199\n",
      "0.09246793\n",
      "0.13537619\n",
      "0.0910932\n",
      "0.06465912\n",
      "0.09467189\n",
      "0.09323878\n",
      "0.09924714\n",
      "0.077133685\n",
      "0.11478223\n",
      "0.18035771\n",
      "0.11889525\n",
      "0.0761598\n",
      "0.11423466\n",
      "0.10548551\n",
      "0.09108695\n",
      "0.196677\n",
      "0.0933546\n",
      "0.13885283\n",
      "0.09339328\n",
      "0.065067746\n",
      "0.09585527\n",
      "0.09256867\n",
      "0.09905943\n",
      "0.077003606\n",
      "0.11555632\n",
      "0.18240654\n",
      "0.12056957\n",
      "0.07738364\n",
      "0.11611006\n",
      "0.10479129\n",
      "0.08959757\n",
      "0.1945147\n",
      "0.09277339\n",
      "0.13969064\n",
      "0.094375566\n",
      "0.06540762\n",
      "0.096648976\n",
      "0.091848426\n",
      "0.09904079\n",
      "0.076049805\n",
      "0.11457546\n",
      "0.18182157\n",
      "0.12078045\n",
      "0.07777293\n",
      "0.117030054\n",
      "0.1045618\n",
      "0.08828758\n",
      "0.19146192\n",
      "0.09159055\n",
      "0.13882776\n",
      "0.09434823\n",
      "0.0655899\n",
      "0.097050525\n",
      "0.09142818\n",
      "0.09896895\n",
      "0.07507042\n",
      "0.113302484\n",
      "0.1803115\n",
      "0.12030401\n",
      "0.06797567\n",
      "0.10157928\n",
      "0.10151363\n",
      "0.09106372\n",
      "0.20421961\n",
      "0.090092786\n",
      "0.12657556\n",
      "0.08518418\n",
      "0.0665302\n",
      "0.08988126\n",
      "0.094693355\n",
      "0.09492265\n",
      "0.078138165\n",
      "0.11599446\n",
      "0.18405056\n",
      "0.12583427\n",
      "0.07898273\n",
      "0.10940861\n",
      "0.10805486\n",
      "0.094681546\n",
      "0.20841664\n",
      "0.09831309\n",
      "0.13712165\n",
      "0.09218763\n",
      "0.06738403\n",
      "0.09203966\n",
      "0.09402659\n",
      "0.09846843\n",
      "0.07766164\n",
      "0.119556636\n",
      "0.19197479\n",
      "0.13167913\n",
      "0.08001308\n",
      "0.114219904\n",
      "0.10478293\n",
      "0.08960415\n",
      "0.20374148\n",
      "0.09699471\n",
      "0.13986352\n",
      "0.094729625\n",
      "0.06788753\n",
      "0.09433022\n",
      "0.091895044\n",
      "0.096100576\n",
      "0.07602024\n",
      "0.11694295\n",
      "0.19081005\n",
      "0.13254221\n",
      "0.08232469\n",
      "0.11690591\n",
      "0.105258666\n",
      "0.087694734\n",
      "0.19677798\n",
      "0.09462868\n",
      "0.13709275\n",
      "0.09423075\n",
      "0.067931995\n",
      "0.09506481\n",
      "0.0917676\n",
      "0.09682565\n",
      "0.07436903\n",
      "0.11426671\n",
      "0.18709125\n",
      "0.13098308\n",
      "0.08165507\n",
      "0.11721943\n",
      "0.10464364\n",
      "0.08582106\n",
      "0.19227046\n",
      "0.09253899\n",
      "0.13440064\n",
      "0.09319062\n",
      "0.0675951\n",
      "0.095044345\n",
      "0.09120418\n",
      "0.09629474\n",
      "0.07344918\n",
      "0.11218673\n",
      "0.18397194\n",
      "0.12947085\n",
      "0.081252344\n",
      "0.1166386\n",
      "0.10450229\n",
      "0.0849825\n",
      "0.18979073\n",
      "0.091027796\n",
      "0.13258289\n",
      "0.09225674\n",
      "0.06754015\n",
      "0.09482696\n",
      "0.09140844\n",
      "0.09652907\n",
      "0.07294845\n",
      "0.110979825\n",
      "0.18183278\n",
      "0.12824571\n",
      "0.08067665\n",
      "0.11596858\n",
      "0.104561046\n",
      "0.0846424\n",
      "0.11367866\n",
      "0.07598445\n",
      "0.09793703\n",
      "0.08211302\n",
      "0.06596235\n",
      "0.10479365\n",
      "0.094246894\n",
      "0.11471456\n",
      "0.06526019\n",
      "0.08705237\n",
      "0.12419122\n",
      "0.09988978\n",
      "0.06873759\n",
      "0.11189753\n",
      "0.10850221\n",
      "0.09580726\n",
      "0.1563335\n",
      "0.08186908\n",
      "0.111543\n",
      "0.08127264\n",
      "0.06178441\n",
      "0.08727003\n",
      "0.09656317\n",
      "0.1013985\n",
      "0.076989666\n",
      "0.104507394\n",
      "0.14274636\n",
      "0.09768095\n",
      "0.06647286\n",
      "0.103694245\n",
      "0.106654935\n",
      "0.10062915\n",
      "0.18816306\n",
      "0.08620488\n",
      "0.12452699\n",
      "0.084858775\n",
      "0.06207838\n",
      "0.087393776\n",
      "0.10117516\n",
      "0.10050057\n",
      "0.084948696\n",
      "0.115310416\n",
      "0.15534809\n",
      "0.10265967\n",
      "0.0683455\n",
      "0.10296345\n",
      "0.108362615\n",
      "0.10555508\n",
      "0.20060088\n",
      "0.09194409\n",
      "0.133103\n",
      "0.08827419\n",
      "0.061690398\n",
      "0.08691343\n",
      "0.100958884\n",
      "0.10012963\n",
      "0.08828577\n",
      "0.12173966\n",
      "0.16299944\n",
      "0.10541588\n",
      "0.06913908\n",
      "0.103858694\n",
      "0.107183374\n",
      "0.10532719\n",
      "0.2060582\n",
      "0.093288295\n",
      "0.13799489\n",
      "0.09036955\n",
      "0.06186212\n",
      "0.087558694\n",
      "0.10015526\n",
      "0.099820845\n",
      "0.08896172\n",
      "0.12361947\n",
      "0.16662405\n",
      "0.107574046\n",
      "0.070413925\n",
      "0.10543503\n",
      "0.10675509\n",
      "0.10457688\n",
      "0.20641027\n",
      "0.0940024\n",
      "0.1401225\n",
      "0.0918358\n",
      "0.061984353\n",
      "0.08797279\n",
      "0.09922752\n",
      "0.099559836\n",
      "0.088519424\n",
      "0.12390539\n",
      "0.1683174\n",
      "0.10862314\n",
      "0.071149826\n",
      "0.10698765\n",
      "0.10613039\n",
      "0.10341687\n",
      "0.2047815\n",
      "0.0938736\n",
      "0.14089331\n",
      "0.09279511\n",
      "0.060082406\n",
      "0.09202518\n",
      "0.110472254\n",
      "0.09735362\n",
      "0.09419503\n",
      "0.12227673\n",
      "0.15123473\n",
      "0.09997908\n",
      "0.06579344\n",
      "0.09764785\n",
      "0.11226757\n",
      "0.10687111\n",
      "0.19795835\n",
      "0.09125164\n",
      "0.13445966\n",
      "0.08884248\n",
      "0.06346371\n",
      "0.093702\n",
      "0.10959058\n",
      "0.10417863\n",
      "0.091098204\n",
      "0.124147296\n",
      "0.15543394\n",
      "0.10074293\n",
      "0.068038985\n",
      "0.10180158\n",
      "0.10906102\n",
      "0.10528482\n",
      "0.19566643\n",
      "0.09501205\n",
      "0.1352562\n",
      "0.09030999\n",
      "0.061594725\n",
      "0.094072685\n",
      "0.10487783\n",
      "0.10067693\n",
      "0.08989288\n",
      "0.12348032\n",
      "0.1570341\n",
      "0.10241397\n",
      "0.06914254\n",
      "0.10543804\n",
      "0.10724588\n",
      "0.10209609\n",
      "0.19092603\n",
      "0.09313025\n",
      "0.13597389\n",
      "0.09134433\n",
      "0.06171322\n",
      "0.09471476\n",
      "0.10268649\n",
      "0.10018498\n",
      "0.08783406\n",
      "0.12176338\n",
      "0.1568058\n",
      "0.10279592\n",
      "0.0696792\n",
      "0.106771775\n",
      "0.10657285\n",
      "0.100341976\n",
      "0.18764904\n",
      "0.09241831\n",
      "0.13488987\n",
      "0.09141308\n",
      "0.061484396\n",
      "0.09462713\n",
      "0.10127974\n",
      "0.100051135\n",
      "0.08660839\n",
      "0.12041869\n",
      "0.15587392\n",
      "0.10256737\n",
      "0.06966727\n",
      "0.107326925\n",
      "0.10593393\n",
      "0.09916539\n",
      "0.18579102\n",
      "0.09180968\n",
      "0.13422546\n",
      "0.0911607\n",
      "0.061186813\n",
      "0.09464107\n",
      "0.10036837\n",
      "0.09946011\n",
      "0.08603685\n",
      "0.1193793\n",
      "0.15530673\n",
      "0.102238216\n",
      "0.06977261\n",
      "0.10765009\n",
      "0.10553727\n",
      "0.09847749\n",
      "0.18462321\n",
      "0.09141767\n",
      "0.1338611\n",
      "0.09110134\n",
      "0.061083097\n",
      "0.09452702\n",
      "0.09982032\n",
      "0.09937675\n",
      "0.06474045\n",
      "0.08049677\n",
      "0.12330415\n",
      "0.092409514\n",
      "0.07290992\n",
      "0.11539215\n",
      "0.10305402\n",
      "0.07929731\n",
      "0.15471548\n",
      "0.07904651\n",
      "0.108184814\n",
      "0.07889466\n",
      "0.06565389\n",
      "0.0915785\n",
      "0.09543778\n",
      "0.097864576\n",
      "0.071864866\n",
      "0.09775687\n",
      "0.1492708\n",
      "0.101235665\n",
      "0.073459975\n",
      "0.10546753\n",
      "0.10640203\n",
      "0.087565675\n",
      "0.18153942\n",
      "0.08678571\n",
      "0.121820144\n",
      "0.085128665\n",
      "0.063761204\n",
      "0.090529785\n",
      "0.09647586\n",
      "0.0977586\n",
      "0.079674564\n",
      "0.10900785\n",
      "0.16443999\n",
      "0.10796317\n",
      "0.07530912\n",
      "0.10601919\n",
      "0.10617481\n",
      "0.09014799\n",
      "0.19500087\n",
      "0.092074655\n",
      "0.1320969\n",
      "0.08907275\n",
      "0.0645416\n",
      "0.09015579\n",
      "0.09667774\n",
      "0.09719491\n",
      "0.08234832\n",
      "0.11403072\n",
      "0.17162156\n",
      "0.11231553\n",
      "0.077331394\n",
      "0.10845162\n",
      "0.10564867\n",
      "0.08956472\n",
      "0.19619435\n",
      "0.093136534\n",
      "0.13616772\n",
      "0.09204125\n",
      "0.06544219\n",
      "0.09121697\n",
      "0.095779926\n",
      "0.09699007\n",
      "0.08178197\n",
      "0.114672706\n",
      "0.17359252\n",
      "0.11434155\n",
      "0.07884021\n",
      "0.11034101\n",
      "0.10520248\n",
      "0.08837952\n",
      "0.19345436\n",
      "0.092718355\n",
      "0.13660434\n",
      "0.09309752\n",
      "0.065518185\n",
      "0.092335686\n",
      "0.0948902\n",
      "0.09709574\n",
      "0.08059902\n",
      "0.11354345\n",
      "0.17319386\n",
      "0.11471904\n",
      "0.07945817\n",
      "0.11139058\n",
      "0.10487985\n",
      "0.08711856\n",
      "0.1909446\n",
      "0.09160866\n",
      "0.13553986\n",
      "0.09303203\n",
      "0.065625146\n",
      "0.09247833\n",
      "0.09458111\n",
      "0.09714054\n",
      "0.07956465\n",
      "0.112413906\n",
      "0.17177922\n",
      "0.114452414\n",
      "0.07098208\n",
      "0.09953976\n",
      "0.10286597\n",
      "0.08767225\n",
      "0.20445397\n",
      "0.08900477\n",
      "0.12581304\n",
      "0.08499519\n",
      "0.065178074\n",
      "0.08693024\n",
      "0.10049612\n",
      "0.09482283\n",
      "0.08171478\n",
      "0.11219156\n",
      "0.17639217\n",
      "0.117555104\n",
      "0.07960682\n",
      "0.10482379\n",
      "0.10959291\n",
      "0.09169756\n",
      "0.20857823\n",
      "0.096891105\n",
      "0.1348257\n",
      "0.09127529\n",
      "0.06582509\n",
      "0.08792664\n",
      "0.09935018\n",
      "0.098572925\n",
      "0.08186499\n",
      "0.117093384\n",
      "0.18426585\n",
      "0.12449954\n",
      "0.0810336\n",
      "0.108184494\n",
      "0.106241964\n",
      "0.08805125\n",
      "0.20702899\n",
      "0.09681538\n",
      "0.13825692\n",
      "0.09390796\n",
      "0.066275805\n",
      "0.089684784\n",
      "0.09680335\n",
      "0.095493004\n",
      "0.0810486\n",
      "0.11575215\n",
      "0.18511635\n",
      "0.12605222\n",
      "0.08361018\n",
      "0.110803306\n",
      "0.1060835\n",
      "0.08651984\n",
      "0.2016159\n",
      "0.095390216\n",
      "0.13751933\n",
      "0.094849035\n",
      "0.06679374\n",
      "0.09075287\n",
      "0.096172854\n",
      "0.09663743\n",
      "0.0789843\n",
      "0.11390814\n",
      "0.18309867\n",
      "0.12587586\n",
      "0.08377421\n",
      "0.11183052\n",
      "0.10569277\n",
      "0.085036695\n",
      "0.19760159\n",
      "0.09392742\n",
      "0.13574441\n",
      "0.094515935\n",
      "0.06674674\n",
      "0.091135465\n",
      "0.09553955\n",
      "0.09618614\n",
      "0.077838436\n",
      "0.11207074\n",
      "0.18093282\n",
      "0.12514412\n",
      "0.08380926\n",
      "0.11183959\n",
      "0.10567507\n",
      "0.08467274\n",
      "0.1949031\n",
      "0.09277038\n",
      "0.13419059\n",
      "0.0941413\n",
      "0.06652123\n",
      "0.091315985\n",
      "0.095133126\n",
      "0.09648442\n",
      "0.07709207\n",
      "0.110864565\n",
      "0.17917621\n",
      "0.12434459\n",
      "0.08351688\n",
      "0.11150566\n",
      "0.10536638\n",
      "0.08403716\n",
      "0.11376114\n",
      "0.07768758\n",
      "0.097408354\n",
      "0.08374212\n",
      "0.06488221\n",
      "0.105269454\n",
      "0.09794258\n",
      "0.10983011\n",
      "0.06803329\n",
      "0.08850592\n",
      "0.11869193\n",
      "0.09845475\n",
      "0.06963873\n",
      "0.10580552\n",
      "0.109007835\n",
      "0.09404624\n",
      "0.1520612\n",
      "0.08260041\n",
      "0.10842961\n",
      "0.08290303\n",
      "0.0612019\n",
      "0.08773789\n",
      "0.10087586\n",
      "0.09723733\n",
      "0.07906013\n",
      "0.10436344\n",
      "0.13450475\n",
      "0.09494835\n",
      "0.06698472\n",
      "0.097033486\n",
      "0.10907078\n",
      "0.098726265\n",
      "0.1822165\n",
      "0.085935526\n",
      "0.11930651\n",
      "0.08473801\n",
      "0.060730778\n",
      "0.08717976\n",
      "0.105726674\n",
      "0.09639147\n",
      "0.087383814\n",
      "0.11472529\n",
      "0.14587913\n",
      "0.09891652\n",
      "0.06807663\n",
      "0.09631793\n",
      "0.11024274\n",
      "0.10265687\n",
      "0.19463721\n",
      "0.090129584\n",
      "0.12781566\n",
      "0.087718815\n",
      "0.060773075\n",
      "0.08632907\n",
      "0.10613534\n",
      "0.09649509\n",
      "0.09089942\n",
      "0.120923385\n",
      "0.15227497\n",
      "0.102034934\n",
      "0.0688709\n",
      "0.09769447\n",
      "0.10859163\n",
      "0.10142972\n",
      "0.19802609\n",
      "0.091252156\n",
      "0.13265428\n",
      "0.09003309\n",
      "0.06115468\n",
      "0.086793974\n",
      "0.104698166\n",
      "0.0961774\n",
      "0.09134981\n",
      "0.12239325\n",
      "0.154873\n",
      "0.1037475\n",
      "0.07008552\n",
      "0.099666364\n",
      "0.10771041\n",
      "0.100592814\n",
      "0.19655944\n",
      "0.09175802\n",
      "0.13395078\n",
      "0.091416374\n",
      "0.061186172\n",
      "0.087254934\n",
      "0.10327147\n",
      "0.096042946\n",
      "0.090508\n",
      "0.12225611\n",
      "0.15574305\n",
      "0.10443682\n",
      "0.070755795\n",
      "0.10111457\n",
      "0.10676669\n",
      "0.09925332\n",
      "0.19468316\n",
      "0.09118846\n",
      "0.13430053\n",
      "0.09204412\n",
      "0.05853775\n",
      "0.094539106\n",
      "0.112751536\n",
      "0.09445388\n",
      "0.09378136\n",
      "0.12172839\n",
      "0.14460169\n",
      "0.1011551\n",
      "0.065933496\n",
      "0.09219101\n",
      "0.11266347\n",
      "0.10589707\n",
      "0.19323494\n",
      "0.08987814\n",
      "0.1306152\n",
      "0.08994779\n",
      "0.061670594\n",
      "0.096521825\n",
      "0.11331503\n",
      "0.10108188\n",
      "0.092174016\n",
      "0.12470875\n",
      "0.14890113\n",
      "0.10144138\n",
      "0.06779412\n",
      "0.09492406\n",
      "0.10906783\n",
      "0.104281865\n",
      "0.19349416\n",
      "0.09354313\n",
      "0.13287483\n",
      "0.09152403\n",
      "0.060573645\n",
      "0.09653021\n",
      "0.10943008\n",
      "0.09756004\n",
      "0.092170015\n",
      "0.124976225\n",
      "0.150951\n",
      "0.102827094\n",
      "0.06884592\n",
      "0.09788064\n",
      "0.10731576\n",
      "0.10227178\n",
      "0.19113375\n",
      "0.09243458\n",
      "0.13489859\n",
      "0.09311722\n",
      "0.060889214\n",
      "0.0970406\n",
      "0.10781794\n",
      "0.09773934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09085922\n",
      "0.12423915\n",
      "0.15121107\n",
      "0.103365615\n",
      "0.06957836\n",
      "0.098844096\n",
      "0.10643345\n",
      "0.10102168\n",
      "0.18890011\n",
      "0.09237132\n",
      "0.1343999\n",
      "0.093637675\n",
      "0.06078422\n",
      "0.09703484\n",
      "0.10621059\n",
      "0.09721555\n",
      "0.090017825\n",
      "0.12310389\n",
      "0.15056065\n",
      "0.10322952\n",
      "0.06989734\n",
      "0.099581175\n",
      "0.10578381\n",
      "0.10001312\n",
      "0.18698314\n",
      "0.091731735\n",
      "0.13371173\n",
      "0.0934649\n",
      "0.060641877\n",
      "0.09703599\n",
      "0.10530551\n",
      "0.09689891\n",
      "0.089310616\n",
      "0.12206802\n",
      "0.14982283\n",
      "0.10282896\n",
      "0.070023194\n",
      "0.09981082\n",
      "0.105241716\n",
      "0.09930732\n",
      "0.18527576\n",
      "0.09115281\n",
      "0.13263114\n",
      "0.09330174\n",
      "0.060540304\n",
      "0.09688304\n",
      "0.1046339\n",
      "0.096728794\n",
      "0.06631841\n",
      "0.0812466\n",
      "0.11796821\n",
      "0.09110808\n",
      "0.07540024\n",
      "0.108300254\n",
      "0.10208657\n",
      "0.081572555\n",
      "0.15106158\n",
      "0.08038716\n",
      "0.10590278\n",
      "0.077353776\n",
      "0.06340312\n",
      "0.091848314\n",
      "0.09988342\n",
      "0.09725498\n",
      "0.07379094\n",
      "0.09731255\n",
      "0.14168456\n",
      "0.09889287\n",
      "0.07504904\n",
      "0.09834893\n",
      "0.105860606\n",
      "0.08844715\n",
      "0.17743254\n",
      "0.08744463\n",
      "0.11953375\n",
      "0.083139874\n",
      "0.061939046\n",
      "0.09028584\n",
      "0.10142877\n",
      "0.09717941\n",
      "0.082109176\n",
      "0.108356014\n",
      "0.15601532\n",
      "0.10534573\n",
      "0.07689897\n",
      "0.09827626\n",
      "0.10623412\n",
      "0.091441706\n",
      "0.19168724\n",
      "0.0927103\n",
      "0.12921812\n",
      "0.086336315\n",
      "0.06265273\n",
      "0.09038362\n",
      "0.102208525\n",
      "0.09688908\n",
      "0.08542154\n",
      "0.11412384\n",
      "0.16310409\n",
      "0.10891186\n",
      "0.07781668\n",
      "0.09932658\n",
      "0.10585019\n",
      "0.09232853\n",
      "0.19459529\n",
      "0.094620116\n",
      "0.13337843\n",
      "0.08881825\n",
      "0.06306728\n",
      "0.0912368\n",
      "0.101129256\n",
      "0.09660958\n",
      "0.0856184\n",
      "0.11554656\n",
      "0.16538051\n",
      "0.11033718\n",
      "0.078759834\n",
      "0.10072178\n",
      "0.1050936\n",
      "0.09124532\n",
      "0.19325063\n",
      "0.09440852\n",
      "0.13446823\n",
      "0.089838475\n",
      "0.06337633\n",
      "0.09174946\n",
      "0.10028581\n",
      "0.096620284\n",
      "0.084751844\n",
      "0.11500734\n",
      "0.16528855\n",
      "0.11083445\n",
      "0.0792417\n",
      "0.101636276\n",
      "0.10477048\n",
      "0.09057591\n",
      "0.19050393\n",
      "0.093621545\n",
      "0.13381547\n",
      "0.0900401\n",
      "0.06328238\n",
      "0.09214291\n",
      "0.09946301\n",
      "0.096624374\n",
      "0.083831176\n",
      "0.11394254\n",
      "0.1643419\n",
      "0.110651694\n",
      "0.074093506\n",
      "0.09369457\n",
      "0.103503264\n",
      "0.08991742\n",
      "0.20310128\n",
      "0.09074159\n",
      "0.12659103\n",
      "0.084138684\n",
      "0.0628367\n",
      "0.08627958\n",
      "0.10372988\n",
      "0.09567299\n",
      "0.084975794\n",
      "0.113855414\n",
      "0.17132299\n",
      "0.11579532\n",
      "0.0820399\n",
      "0.099724665\n",
      "0.10950115\n",
      "0.09166705\n",
      "0.2036395\n",
      "0.097222134\n",
      "0.13504809\n",
      "0.09041654\n",
      "0.06417853\n",
      "0.086771175\n",
      "0.10147469\n",
      "0.09735101\n",
      "0.08399077\n",
      "0.1168193\n",
      "0.17740554\n",
      "0.120761685\n",
      "0.08309289\n",
      "0.102840066\n",
      "0.10588086\n",
      "0.08795343\n",
      "0.20049815\n",
      "0.09625778\n",
      "0.13756464\n",
      "0.092541754\n",
      "0.06470634\n",
      "0.08874039\n",
      "0.09910683\n",
      "0.095516875\n",
      "0.0823675\n",
      "0.114731975\n",
      "0.17667612\n",
      "0.12162744\n",
      "0.08483094\n",
      "0.104771316\n",
      "0.10598098\n",
      "0.08684204\n",
      "0.19610523\n",
      "0.094990775\n",
      "0.13645403\n",
      "0.09302912\n",
      "0.06516735\n",
      "0.08917807\n",
      "0.09833183\n",
      "0.096323445\n",
      "0.08076973\n",
      "0.11317792\n",
      "0.17504323\n",
      "0.121071465\n",
      "0.08488289\n",
      "0.10531866\n",
      "0.10565341\n",
      "0.08589318\n",
      "0.19289005\n",
      "0.09400548\n",
      "0.13481285\n",
      "0.092607945\n",
      "0.06500454\n",
      "0.089447744\n",
      "0.097383685\n",
      "0.09577815\n",
      "0.079796806\n",
      "0.11163312\n",
      "0.17315324\n",
      "0.12031248\n",
      "0.0849162\n",
      "0.10530991\n",
      "0.10554062\n",
      "0.08526197\n",
      "0.19077271\n",
      "0.09284029\n",
      "0.13333507\n",
      "0.092096455\n",
      "0.06502882\n",
      "0.08943128\n",
      "0.09715267\n",
      "0.09602117\n",
      "0.079078875\n",
      "0.110570826\n",
      "0.17160437\n",
      "0.11944229\n",
      "0.08474329\n",
      "0.105055735\n",
      "0.105494104\n",
      "0.08500662\n",
      "0.11457852\n",
      "0.07712696\n",
      "0.09679256\n",
      "0.08514503\n",
      "0.06327768\n",
      "0.10786992\n",
      "0.09747924\n",
      "0.10628388\n",
      "0.070343554\n",
      "0.09051637\n",
      "0.115507014\n",
      "0.10153112\n",
      "0.06757518\n",
      "0.09767314\n",
      "0.11260516\n",
      "0.0943726\n",
      "0.15745755\n",
      "0.08161113\n",
      "0.11286066\n",
      "0.085900195\n",
      "0.06043485\n",
      "0.0911053\n",
      "0.10300875\n",
      "0.09587193\n",
      "0.083274096\n",
      "0.109135374\n",
      "0.13311692\n",
      "0.098878436\n",
      "0.06584506\n",
      "0.0904299\n",
      "0.11074264\n",
      "0.099262156\n",
      "0.18623343\n",
      "0.08629108\n",
      "0.12173635\n",
      "0.087300695\n",
      "0.05979557\n",
      "0.090644695\n",
      "0.10625664\n",
      "0.09534556\n",
      "0.090476505\n",
      "0.117942154\n",
      "0.14305855\n",
      "0.10338691\n",
      "0.06715894\n",
      "0.09017028\n",
      "0.111880764\n",
      "0.10149098\n",
      "0.19589104\n",
      "0.089420624\n",
      "0.12903796\n",
      "0.09001554\n",
      "0.060153954\n",
      "0.08999699\n",
      "0.10720235\n",
      "0.095367685\n",
      "0.09384533\n",
      "0.12298716\n",
      "0.1483216\n",
      "0.10557607\n",
      "0.06723731\n",
      "0.09050046\n",
      "0.11051398\n",
      "0.10189815\n",
      "0.20010804\n",
      "0.090694144\n",
      "0.13303204\n",
      "0.09156602\n",
      "0.060282998\n",
      "0.09025286\n",
      "0.106454924\n",
      "0.09525114\n",
      "0.09474847\n",
      "0.124401525\n",
      "0.15100195\n",
      "0.10684544\n",
      "0.06804042\n",
      "0.091681734\n",
      "0.10985516\n",
      "0.10132319\n",
      "0.20058213\n",
      "0.091352224\n",
      "0.13461722\n",
      "0.092601776\n",
      "0.06039805\n",
      "0.090758674\n",
      "0.10580677\n",
      "0.095151454\n",
      "0.09473067\n",
      "0.124814734\n",
      "0.15217605\n",
      "0.10750042\n",
      "0.06859641\n",
      "0.09256298\n",
      "0.108965784\n",
      "0.100514576\n",
      "0.19982992\n",
      "0.09139258\n",
      "0.13539493\n",
      "0.09319426\n",
      "0.058233142\n",
      "0.09825931\n",
      "0.1127028\n",
      "0.094297186\n",
      "0.096956566\n",
      "0.12524371\n",
      "0.14335352\n",
      "0.10812541\n",
      "0.064879574\n",
      "0.08531907\n",
      "0.11533238\n",
      "0.10721662\n",
      "0.20089749\n",
      "0.08968405\n",
      "0.13279638\n",
      "0.09046583\n",
      "0.06076885\n",
      "0.100171104\n",
      "0.114445455\n",
      "0.10133322\n",
      "0.09661675\n",
      "0.12902778\n",
      "0.14868656\n",
      "0.107973024\n",
      "0.06642023\n",
      "0.08740121\n",
      "0.112818964\n",
      "0.106079675\n",
      "0.20270334\n",
      "0.09403978\n",
      "0.13487166\n",
      "0.091822304\n",
      "0.059849598\n",
      "0.09946289\n",
      "0.11079388\n",
      "0.09790255\n",
      "0.09663765\n",
      "0.1292906\n",
      "0.15085419\n",
      "0.1099693\n",
      "0.067428276\n",
      "0.08969288\n",
      "0.11102293\n",
      "0.1037378\n",
      "0.20004551\n",
      "0.0925785\n",
      "0.13655624\n",
      "0.09319558\n",
      "0.060134083\n",
      "0.10014441\n",
      "0.10936532\n",
      "0.09763785\n",
      "0.0955246\n",
      "0.12872952\n",
      "0.1513362\n",
      "0.110510156\n",
      "0.06805266\n",
      "0.09065418\n",
      "0.11004423\n",
      "0.10253048\n",
      "0.19740433\n",
      "0.09262487\n",
      "0.13602449\n",
      "0.09378994\n",
      "0.060119286\n",
      "0.10012834\n",
      "0.107865326\n",
      "0.09716944\n",
      "0.09460186\n",
      "0.12775302\n",
      "0.15108076\n",
      "0.11047831\n",
      "0.0684306\n",
      "0.09165329\n",
      "0.109027356\n",
      "0.10108991\n",
      "0.19537324\n",
      "0.09204693\n",
      "0.13584447\n",
      "0.09397186\n",
      "0.060117394\n",
      "0.100274235\n",
      "0.10698441\n",
      "0.09649721\n",
      "0.093966305\n",
      "0.12695575\n",
      "0.15085958\n",
      "0.11030957\n",
      "0.06864187\n",
      "0.09212397\n",
      "0.10841671\n",
      "0.10043556\n",
      "0.19369785\n",
      "0.09179115\n",
      "0.13541627\n",
      "0.09420537\n",
      "0.060252972\n",
      "0.10014126\n",
      "0.106284335\n",
      "0.09614952\n",
      "0.06874892\n",
      "0.08200216\n",
      "0.11622849\n",
      "0.09400508\n",
      "0.07538049\n",
      "0.101705134\n",
      "0.10341115\n",
      "0.082436144\n",
      "0.14964539\n",
      "0.08184857\n",
      "0.10471416\n",
      "0.07654464\n",
      "0.06230084\n",
      "0.094247505\n",
      "0.100003995\n",
      "0.09544268\n",
      "0.0770853\n",
      "0.09690711\n",
      "0.13945694\n",
      "0.10126467\n",
      "0.07536108\n",
      "0.092955574\n",
      "0.106920294\n",
      "0.088267565\n",
      "0.17759599\n",
      "0.08757331\n",
      "0.121417135\n",
      "0.08469749\n",
      "0.061876487\n",
      "0.093602955\n",
      "0.10300566\n",
      "0.09748042\n",
      "0.08507252\n",
      "0.1088763\n",
      "0.15400112\n",
      "0.10895589\n",
      "0.0777902\n",
      "0.0932962\n",
      "0.10612212\n",
      "0.09083137\n",
      "0.1908986\n",
      "0.093220964\n",
      "0.13071093\n",
      "0.0881791\n",
      "0.06260102\n",
      "0.09338962\n",
      "0.10225077\n",
      "0.09593263\n",
      "0.087923706\n",
      "0.11368109\n",
      "0.16120525\n",
      "0.11262394\n",
      "0.07870902\n",
      "0.09452474\n",
      "0.105799414\n",
      "0.090925455\n",
      "0.1934243\n",
      "0.094155766\n",
      "0.13410649\n",
      "0.09038535\n",
      "0.06336628\n",
      "0.09415684\n",
      "0.10194835\n",
      "0.09611111\n",
      "0.08774233\n",
      "0.11450495\n",
      "0.16270947\n",
      "0.11369196\n",
      "0.07959219\n",
      "0.09565096\n",
      "0.10504453\n",
      "0.090302214\n",
      "0.19241291\n",
      "0.09387653\n",
      "0.1343515\n",
      "0.091010526\n",
      "0.06363891\n",
      "0.09455075\n",
      "0.10126508\n",
      "0.09597278\n",
      "0.087071374\n",
      "0.11385149\n",
      "0.16247264\n",
      "0.113855496\n",
      "0.08010254\n",
      "0.0963074\n",
      "0.10501391\n",
      "0.089612424\n",
      "0.19040903\n",
      "0.0931564\n",
      "0.13373822\n",
      "0.09115703\n",
      "0.063773334\n",
      "0.09464434\n",
      "0.10067702\n",
      "0.09589261\n",
      "0.086329296\n",
      "0.11295033\n",
      "0.16151842\n",
      "0.11346646\n",
      "0.07537684\n",
      "0.08876727\n",
      "0.10384433\n",
      "0.09055899\n",
      "0.20116524\n",
      "0.091539204\n",
      "0.12667938\n",
      "0.08440998\n",
      "0.0634363\n",
      "0.088119686\n",
      "0.10354196\n",
      "0.09565665\n",
      "0.087496616\n",
      "0.11220157\n",
      "0.1681869\n",
      "0.11806626\n",
      "0.082339704\n",
      "0.09436974\n",
      "0.10906796\n",
      "0.09229295\n",
      "0.20290554\n",
      "0.09804128\n",
      "0.13561116\n",
      "0.090708576\n",
      "0.06463388\n",
      "0.088645235\n",
      "0.101432756\n",
      "0.09746438\n",
      "0.08696744\n",
      "0.115525305\n",
      "0.17451586\n",
      "0.12417633\n",
      "0.08416529\n",
      "0.09783184\n",
      "0.10618643\n",
      "0.0888724\n",
      "0.19984011\n",
      "0.09700529\n",
      "0.13788548\n",
      "0.09289684\n",
      "0.06546711\n",
      "0.09075759\n",
      "0.099304974\n",
      "0.095303625\n",
      "0.08540444\n",
      "0.113542795\n",
      "0.17389655\n",
      "0.1247888\n",
      "0.08581763\n",
      "0.09984251\n",
      "0.10634956\n",
      "0.08773413\n",
      "0.1952548\n",
      "0.09566938\n",
      "0.13649096\n",
      "0.09337881\n",
      "0.065901\n",
      "0.09125318\n",
      "0.098628804\n",
      "0.096015856\n",
      "0.08360872\n",
      "0.111656964\n",
      "0.17191763\n",
      "0.12394251\n",
      "0.08588612\n",
      "0.10047992\n",
      "0.10601858\n",
      "0.08678845\n",
      "0.19149423\n",
      "0.09422679\n",
      "0.1343614\n",
      "0.092930086\n",
      "0.065689\n",
      "0.091457404\n",
      "0.09783315\n",
      "0.095533706\n",
      "0.0824993\n",
      "0.10969177\n",
      "0.16960792\n",
      "0.122757465\n",
      "0.085815996\n",
      "0.10026313\n",
      "0.10583134\n",
      "0.08618891\n",
      "0.18936642\n",
      "0.09301916\n",
      "0.13257553\n",
      "0.092284136\n",
      "0.065674536\n",
      "0.09133037\n",
      "0.097783044\n",
      "0.09558271\n",
      "0.0818422\n",
      "0.10846769\n",
      "0.1679064\n",
      "0.12161562\n",
      "0.08545688\n",
      "0.09991956\n",
      "0.10578861\n",
      "0.08600559\n",
      "0.116507635\n",
      "0.078172766\n",
      "0.096340254\n",
      "0.086108506\n",
      "0.06421989\n",
      "0.11068827\n",
      "0.09718938\n",
      "0.10549156\n",
      "0.07276797\n",
      "0.090907395\n",
      "0.11467152\n",
      "0.10671083\n",
      "0.06739617\n",
      "0.09452958\n",
      "0.114573106\n",
      "0.09153963\n",
      "0.15864694\n",
      "0.08294142\n",
      "0.11181701\n",
      "0.08619896\n",
      "0.0612913\n",
      "0.093545556\n",
      "0.10132804\n",
      "0.094922066\n",
      "0.08564937\n",
      "0.1092167\n",
      "0.13192897\n",
      "0.1039613\n",
      "0.065127514\n",
      "0.086873665\n",
      "0.11468471\n",
      "0.09789222\n",
      "0.19157691\n",
      "0.08765317\n",
      "0.123172924\n",
      "0.08799872\n",
      "0.06025075\n",
      "0.09188372\n",
      "0.10650557\n",
      "0.095923945\n",
      "0.09394091\n",
      "0.119644284\n",
      "0.1433268\n",
      "0.109365605\n",
      "0.06644308\n",
      "0.08653773\n",
      "0.11592896\n",
      "0.10131307\n",
      "0.20170881\n",
      "0.092015415\n",
      "0.13105544\n",
      "0.09173835\n",
      "0.060424738\n",
      "0.091827884\n",
      "0.10734391\n",
      "0.095934585\n",
      "0.09724308\n",
      "0.12561862\n",
      "0.14989243\n",
      "0.11224247\n",
      "0.067115456\n",
      "0.08741391\n",
      "0.11423538\n",
      "0.10068794\n",
      "0.20626488\n",
      "0.09334612\n",
      "0.13547707\n",
      "0.093566485\n",
      "0.06081217\n",
      "0.09189854\n",
      "0.106425256\n",
      "0.09564583\n",
      "0.098035045\n",
      "0.12744676\n",
      "0.1528742\n",
      "0.113697246\n",
      "0.068215996\n",
      "0.08877807\n",
      "0.113418505\n",
      "0.10014673\n",
      "0.20666604\n",
      "0.09418603\n",
      "0.13708681\n",
      "0.09472596\n",
      "0.061030276\n",
      "0.09218117\n",
      "0.105752334\n",
      "0.09532455\n",
      "0.09787941\n",
      "0.12788802\n",
      "0.15430373\n",
      "0.11444777\n",
      "0.068921104\n",
      "0.08998902\n",
      "0.1124188\n",
      "0.09901683\n",
      "0.20532982\n",
      "0.09399441\n",
      "0.13785915\n",
      "0.095673636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059096042\n",
      "0.10102563\n",
      "0.11189561\n",
      "0.09474948\n",
      "0.09735\n",
      "0.1276191\n",
      "0.14475378\n",
      "0.11328997\n",
      "0.06578375\n",
      "0.084146574\n",
      "0.119551286\n",
      "0.10535783\n",
      "0.20682791\n",
      "0.09293963\n",
      "0.13420503\n",
      "0.09199159\n",
      "0.061140053\n",
      "0.10281556\n",
      "0.112333216\n",
      "0.1023686\n",
      "0.09698236\n",
      "0.13184308\n",
      "0.15045205\n",
      "0.11334394\n",
      "0.06766976\n",
      "0.08590296\n",
      "0.11621602\n",
      "0.10448403\n",
      "0.20976956\n",
      "0.09694566\n",
      "0.13677931\n",
      "0.09337538\n",
      "0.06048816\n",
      "0.101892486\n",
      "0.10965615\n",
      "0.098875634\n",
      "0.097091496\n",
      "0.13250718\n",
      "0.15295267\n",
      "0.11592611\n",
      "0.06871581\n",
      "0.0882388\n",
      "0.11489528\n",
      "0.10239824\n",
      "0.20763254\n",
      "0.095806286\n",
      "0.13959451\n",
      "0.09514176\n",
      "0.061034568\n",
      "0.10253252\n",
      "0.10825467\n",
      "0.09870396\n",
      "0.09625954\n",
      "0.1323582\n",
      "0.15439717\n",
      "0.117063634\n",
      "0.06953493\n",
      "0.089560665\n",
      "0.113426566\n",
      "0.10066843\n",
      "0.20560765\n",
      "0.09582698\n",
      "0.13986818\n",
      "0.095942646\n",
      "0.06117476\n",
      "0.10289457\n",
      "0.106744684\n",
      "0.09808041\n",
      "0.09540117\n",
      "0.1313235\n",
      "0.1542098\n",
      "0.11734593\n",
      "0.07015807\n",
      "0.09082679\n",
      "0.112528354\n",
      "0.09939488\n",
      "0.20301855\n",
      "0.09524432\n",
      "0.13973975\n",
      "0.09626235\n",
      "0.061348803\n",
      "0.10310776\n",
      "0.10579981\n",
      "0.09753244\n",
      "0.09469211\n",
      "0.13014065\n",
      "0.15388693\n",
      "0.117205545\n",
      "0.07042353\n",
      "0.09168147\n",
      "0.11185311\n",
      "0.0981098\n",
      "0.2008211\n",
      "0.0945348\n",
      "0.1390757\n",
      "0.09642508\n",
      "0.06153856\n",
      "0.10335007\n",
      "0.104973264\n",
      "0.09710723\n",
      "0.07218203\n",
      "0.08323229\n",
      "0.114845246\n",
      "0.10077814\n",
      "0.07499159\n",
      "0.10006522\n",
      "0.10467786\n",
      "0.083064616\n",
      "0.15583661\n",
      "0.08296203\n",
      "0.10986955\n",
      "0.07906654\n",
      "0.06528342\n",
      "0.09967583\n",
      "0.10091074\n",
      "0.096012875\n",
      "0.081045136\n",
      "0.10017413\n",
      "0.13978925\n",
      "0.1094942\n",
      "0.07435204\n",
      "0.09191646\n",
      "0.10609229\n",
      "0.08816157\n",
      "0.17938405\n",
      "0.08912547\n",
      "0.12178127\n",
      "0.08517563\n",
      "0.063597\n",
      "0.09832394\n",
      "0.10131462\n",
      "0.094944574\n",
      "0.087897405\n",
      "0.10900669\n",
      "0.1526741\n",
      "0.11695892\n",
      "0.07679735\n",
      "0.09210649\n",
      "0.106523715\n",
      "0.09055489\n",
      "0.19246727\n",
      "0.09349632\n",
      "0.13040142\n",
      "0.08789654\n",
      "0.06449841\n",
      "0.09848513\n",
      "0.10218688\n",
      "0.09473041\n",
      "0.090577506\n",
      "0.11376925\n",
      "0.15883124\n",
      "0.120237276\n",
      "0.07762415\n",
      "0.09328994\n",
      "0.10616265\n",
      "0.090950534\n",
      "0.19493495\n",
      "0.0948078\n",
      "0.1333806\n",
      "0.089882255\n",
      "0.064988256\n",
      "0.09938182\n",
      "0.101252824\n",
      "0.09424032\n",
      "0.09043553\n",
      "0.11445466\n",
      "0.1605574\n",
      "0.121453285\n",
      "0.07866081\n",
      "0.09439625\n",
      "0.10574764\n",
      "0.090104505\n",
      "0.19431195\n",
      "0.09443761\n",
      "0.13401484\n",
      "0.09050985\n",
      "0.065383725\n",
      "0.09989362\n",
      "0.10082625\n",
      "0.09436132\n",
      "0.08982761\n",
      "0.11387673\n",
      "0.16049552\n",
      "0.12163616\n",
      "0.079017684\n",
      "0.09521796\n",
      "0.10571426\n",
      "0.08957271\n",
      "0.1924034\n",
      "0.09374882\n",
      "0.1332492\n",
      "0.09047947\n",
      "0.06540796\n",
      "0.10019768\n",
      "0.100189686\n",
      "0.094287\n",
      "0.089000955\n",
      "0.11282843\n",
      "0.15970875\n",
      "0.12138357\n",
      "0.074733734\n",
      "0.086800925\n",
      "0.10383854\n",
      "0.091186434\n",
      "0.20094344\n",
      "0.09154341\n",
      "0.12631309\n",
      "0.08369094\n",
      "0.06538086\n",
      "0.09438358\n",
      "0.10223344\n",
      "0.095239304\n",
      "0.08994813\n",
      "0.110981174\n",
      "0.16372257\n",
      "0.123416506\n",
      "0.08060172\n",
      "0.092115335\n",
      "0.10846913\n",
      "0.0927879\n",
      "0.2025969\n",
      "0.09750099\n",
      "0.13372116\n",
      "0.089361995\n",
      "0.06638933\n",
      "0.095173255\n",
      "0.09967659\n",
      "0.09622038\n",
      "0.08934615\n",
      "0.11395365\n",
      "0.17000397\n",
      "0.12993404\n",
      "0.08250783\n",
      "0.09535788\n",
      "0.10616283\n",
      "0.0894265\n",
      "0.20004529\n",
      "0.09657726\n",
      "0.13611898\n",
      "0.09143128\n",
      "0.06710556\n",
      "0.09713892\n",
      "0.097995885\n",
      "0.09435704\n",
      "0.088107675\n",
      "0.112216584\n",
      "0.16976029\n",
      "0.13059486\n",
      "0.08381305\n",
      "0.096752524\n",
      "0.10608782\n",
      "0.088556975\n",
      "0.19644383\n",
      "0.095496766\n",
      "0.13491637\n",
      "0.09164956\n",
      "0.067569636\n",
      "0.0976607\n",
      "0.09740481\n",
      "0.09494051\n",
      "0.08649421\n",
      "0.11055102\n",
      "0.16816057\n",
      "0.12974697\n",
      "0.08381434\n",
      "0.09739853\n",
      "0.10592299\n",
      "0.08767342\n",
      "0.19327022\n",
      "0.09428542\n",
      "0.13304287\n",
      "0.091156274\n",
      "0.06742139\n",
      "0.09796937\n",
      "0.09668146\n",
      "0.094308585\n",
      "0.08554931\n",
      "0.10876806\n",
      "0.16631812\n",
      "0.12861215\n",
      "0.08378474\n",
      "0.09735121\n",
      "0.10566384\n",
      "0.086993225\n",
      "0.19135198\n",
      "0.09311887\n",
      "0.13153383\n",
      "0.09050149\n",
      "0.06750438\n",
      "0.097957626\n",
      "0.09651458\n",
      "0.09440468\n",
      "0.08487117\n",
      "0.107624516\n",
      "0.16487801\n",
      "0.1276206\n",
      "0.083478734\n",
      "0.097086854\n",
      "0.10574449\n",
      "0.08673721\n",
      "0.11948917\n",
      "0.08002613\n",
      "0.09587925\n",
      "0.08656048\n",
      "0.06496474\n",
      "0.113840096\n",
      "0.096145466\n",
      "0.103920735\n",
      "0.07340111\n",
      "0.09094884\n",
      "0.11368547\n",
      "0.11072103\n",
      "0.06764932\n",
      "0.09271997\n",
      "0.118320666\n",
      "0.08968067\n",
      "0.16349702\n",
      "0.08562663\n",
      "0.111765176\n",
      "0.087415926\n",
      "0.061761536\n",
      "0.0969853\n",
      "0.09930009\n",
      "0.09537034\n",
      "0.08561171\n",
      "0.109284855\n",
      "0.13256995\n",
      "0.109024286\n",
      "0.06576978\n",
      "0.08551914\n",
      "0.11871143\n",
      "0.09528717\n",
      "0.1973893\n",
      "0.09102574\n",
      "0.12400101\n",
      "0.08921001\n",
      "0.061195247\n",
      "0.0949742\n",
      "0.104518265\n",
      "0.09685719\n",
      "0.093512766\n",
      "0.12030637\n",
      "0.1448097\n",
      "0.11507462\n",
      "0.06748044\n",
      "0.08502121\n",
      "0.11979529\n",
      "0.09829735\n",
      "0.20922291\n",
      "0.09563173\n",
      "0.13267682\n",
      "0.09251767\n",
      "0.061510585\n",
      "0.09479903\n",
      "0.105373815\n",
      "0.096862\n",
      "0.097040705\n",
      "0.12633845\n",
      "0.1510826\n",
      "0.1182284\n",
      "0.06821009\n",
      "0.08576665\n",
      "0.118650556\n",
      "0.09851696\n",
      "0.21409063\n",
      "0.09721393\n",
      "0.13671468\n",
      "0.0940873\n",
      "0.061848775\n",
      "0.09508465\n",
      "0.1046724\n",
      "0.096825555\n",
      "0.09787328\n",
      "0.12808648\n",
      "0.15357618\n",
      "0.120042406\n",
      "0.069100164\n",
      "0.08698729\n",
      "0.11768994\n",
      "0.09798639\n",
      "0.21370526\n",
      "0.09783988\n",
      "0.13836995\n",
      "0.09518844\n",
      "0.062094063\n",
      "0.09544972\n",
      "0.103901856\n",
      "0.09636561\n",
      "0.09760287\n",
      "0.12825683\n",
      "0.15475541\n",
      "0.12099113\n",
      "0.069698766\n",
      "0.08806816\n",
      "0.11682352\n",
      "0.09700645\n",
      "0.21218157\n",
      "0.09767617\n",
      "0.13887899\n",
      "0.095597625\n",
      "0.060302682\n",
      "0.10180182\n",
      "0.110395715\n",
      "0.09586005\n",
      "0.094887264\n",
      "0.12754774\n",
      "0.14607804\n",
      "0.11926847\n",
      "0.06653698\n",
      "0.08231677\n",
      "0.120838776\n",
      "0.10377226\n",
      "0.21570241\n",
      "0.0968071\n",
      "0.13560444\n",
      "0.09242992\n",
      "0.06218249\n",
      "0.103784464\n",
      "0.111533225\n",
      "0.10388093\n",
      "0.095760584\n",
      "0.13188434\n",
      "0.15231992\n",
      "0.119443476\n",
      "0.06813747\n",
      "0.08392758\n",
      "0.118576996\n",
      "0.104438625\n",
      "0.21897034\n",
      "0.10137894\n",
      "0.13738967\n",
      "0.092888445\n",
      "0.061010707\n",
      "0.10338454\n",
      "0.10904214\n",
      "0.101162605\n",
      "0.09629138\n",
      "0.13342242\n",
      "0.1551961\n",
      "0.122380376\n",
      "0.06916191\n",
      "0.0858136\n",
      "0.11685923\n",
      "0.10195513\n",
      "0.21770383\n",
      "0.10000303\n",
      "0.140707\n",
      "0.09444451\n",
      "0.061594017\n",
      "0.10377249\n",
      "0.10798323\n",
      "0.10054365\n",
      "0.09588482\n",
      "0.13379411\n",
      "0.15683751\n",
      "0.12374133\n",
      "0.06989075\n",
      "0.08685519\n",
      "0.11565256\n",
      "0.10069276\n",
      "0.21553656\n",
      "0.10003343\n",
      "0.14107412\n",
      "0.095132135\n",
      "0.06167697\n",
      "0.10395162\n",
      "0.10643203\n",
      "0.10029698\n",
      "0.09514279\n",
      "0.13305001\n",
      "0.1571513\n",
      "0.12442216\n",
      "0.07057548\n",
      "0.08810334\n",
      "0.11455592\n",
      "0.09916661\n",
      "0.21290347\n",
      "0.09946774\n",
      "0.14116956\n",
      "0.09534696\n",
      "0.061782118\n",
      "0.104170315\n",
      "0.105442755\n",
      "0.099526584\n",
      "0.094525635\n",
      "0.13205403\n",
      "0.15698087\n",
      "0.12458883\n",
      "0.07095766\n",
      "0.08882705\n",
      "0.11362592\n",
      "0.09805973\n",
      "0.21051982\n",
      "0.09883323\n",
      "0.14098564\n",
      "0.095471136\n",
      "0.06194521\n",
      "0.10442023\n",
      "0.104504265\n",
      "0.09909045\n",
      "0.074681744\n",
      "0.084806785\n",
      "0.11350308\n",
      "0.10699176\n",
      "0.074002795\n",
      "0.09750344\n",
      "0.10495159\n",
      "0.08313236\n",
      "0.16041934\n",
      "0.082820185\n",
      "0.10985711\n",
      "0.07877573\n",
      "0.06557469\n",
      "0.10233089\n",
      "0.09800555\n",
      "0.09491014\n",
      "0.0837132\n",
      "0.10238586\n",
      "0.14016962\n",
      "0.117236376\n",
      "0.074087806\n",
      "0.090211965\n",
      "0.107079595\n",
      "0.08786254\n",
      "0.18595949\n",
      "0.08924514\n",
      "0.12414468\n",
      "0.08536948\n",
      "0.06484114\n",
      "0.10083156\n",
      "0.098930426\n",
      "0.095197834\n",
      "0.08991785\n",
      "0.11165871\n",
      "0.15313381\n",
      "0.12543957\n",
      "0.0763357\n",
      "0.09084347\n",
      "0.106461026\n",
      "0.08987974\n",
      "0.19843483\n",
      "0.093352735\n",
      "0.13268773\n",
      "0.087826736\n",
      "0.06578988\n",
      "0.101478994\n",
      "0.09904772\n",
      "0.09443989\n",
      "0.09228817\n",
      "0.115919925\n",
      "0.15898329\n",
      "0.12933715\n",
      "0.077435955\n",
      "0.0925903\n",
      "0.10622747\n",
      "0.09004387\n",
      "0.19954434\n",
      "0.09444849\n",
      "0.13513711\n",
      "0.08977386\n",
      "0.0662025\n",
      "0.10278511\n",
      "0.09803101\n",
      "0.094278015\n",
      "0.09175573\n",
      "0.11620633\n",
      "0.16073495\n",
      "0.13071556\n",
      "0.07846427\n",
      "0.09396073\n",
      "0.10586427\n",
      "0.08882897\n",
      "0.19777517\n",
      "0.09385669\n",
      "0.13527876\n",
      "0.09025517\n",
      "0.06653631\n",
      "0.10348343\n",
      "0.09734413\n",
      "0.09409479\n",
      "0.09093155\n",
      "0.11521463\n",
      "0.16030058\n",
      "0.13070232\n",
      "0.0789041\n",
      "0.09474006\n",
      "0.105874635\n",
      "0.088116206\n",
      "0.1951568\n",
      "0.0930184\n",
      "0.13409123\n",
      "0.09007005\n",
      "0.06651985\n",
      "0.10386087\n",
      "0.09672877\n",
      "0.09403388\n",
      "0.09000078\n",
      "0.113924615\n",
      "0.15925159\n",
      "0.13000768\n",
      "0.074481554\n",
      "0.085018\n",
      "0.10323997\n",
      "0.089865424\n",
      "0.20245683\n",
      "0.09141606\n",
      "0.12679428\n",
      "0.083856784\n",
      "0.06603399\n",
      "0.09837122\n",
      "0.09874207\n",
      "0.0945779\n",
      "0.09270256\n",
      "0.112138435\n",
      "0.16396654\n",
      "0.1317205\n",
      "0.080747254\n",
      "0.09044328\n",
      "0.108336635\n",
      "0.09173745\n",
      "0.2045929\n",
      "0.097363666\n",
      "0.1352359\n",
      "0.0894558\n",
      "0.06764226\n",
      "0.099646196\n",
      "0.09613084\n",
      "0.09608233\n",
      "0.09171279\n",
      "0.11494516\n",
      "0.16961145\n",
      "0.13865353\n",
      "0.082580425\n",
      "0.09384554\n",
      "0.10595026\n",
      "0.08840814\n",
      "0.2013265\n",
      "0.09649573\n",
      "0.13765866\n",
      "0.09166831\n",
      "0.06831075\n",
      "0.10238546\n",
      "0.09421204\n",
      "0.09359573\n",
      "0.09013617\n",
      "0.112882435\n",
      "0.16922766\n",
      "0.1393441\n",
      "0.08413496\n",
      "0.095624\n",
      "0.105903596\n",
      "0.08734472\n",
      "0.1971838\n",
      "0.09514726\n",
      "0.13642015\n",
      "0.09179254\n",
      "0.06874223\n",
      "0.10296939\n",
      "0.09374657\n",
      "0.09456678\n",
      "0.08853419\n",
      "0.11117088\n",
      "0.16759011\n",
      "0.1382891\n",
      "0.08414035\n",
      "0.09624131\n",
      "0.10575387\n",
      "0.086410604\n",
      "0.19364023\n",
      "0.093849644\n",
      "0.13439271\n",
      "0.091137886\n",
      "0.068494536\n",
      "0.103340134\n",
      "0.09295729\n",
      "0.0938538\n",
      "0.087519\n",
      "0.10934518\n",
      "0.16571854\n",
      "0.13686298\n",
      "0.08414777\n",
      "0.09608075\n",
      "0.105639294\n",
      "0.08588391\n",
      "0.19134541\n",
      "0.09263389\n",
      "0.13258564\n",
      "0.09039128\n",
      "0.06847609\n",
      "0.10326829\n",
      "0.093006864\n",
      "0.09400864\n",
      "0.086795166\n",
      "0.108154\n",
      "0.16429627\n",
      "0.13549781\n",
      "0.08383223\n",
      "0.095737524\n",
      "0.1056061\n",
      "0.08579321\n",
      "0.122484535\n",
      "0.082481734\n",
      "0.09643428\n",
      "0.08711515\n",
      "0.065468535\n",
      "0.117271155\n",
      "0.09276953\n",
      "0.10384213\n",
      "0.07392358\n",
      "0.090610825\n",
      "0.115384534\n",
      "0.11600534\n",
      "0.06873422\n",
      "0.091158584\n",
      "0.11809105\n",
      "0.0868907\n",
      "0.16611508\n",
      "0.08852103\n",
      "0.111646645\n",
      "0.08720891\n",
      "0.061980672\n",
      "0.099870436\n",
      "0.09542326\n",
      "0.09630817\n",
      "0.08563369\n",
      "0.10782727\n",
      "0.13435398\n",
      "0.11414632\n",
      "0.06666882\n",
      "0.08439321\n",
      "0.118974045\n",
      "0.09252387\n",
      "0.20182638\n",
      "0.09340209\n",
      "0.12380509\n",
      "0.08878018\n",
      "0.061880607\n",
      "0.09708212\n",
      "0.10075438\n",
      "0.09784424\n",
      "0.09355248\n",
      "0.11893752\n",
      "0.14725588\n",
      "0.120462574\n",
      "0.067744754\n",
      "0.08324146\n",
      "0.12098028\n",
      "0.09621267\n",
      "0.21395828\n",
      "0.09821203\n",
      "0.13215327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09184045\n",
      "0.062027704\n",
      "0.096643895\n",
      "0.101748854\n",
      "0.09836675\n",
      "0.09720528\n",
      "0.12520099\n",
      "0.15392789\n",
      "0.12408378\n",
      "0.06852469\n",
      "0.084150955\n",
      "0.119566225\n",
      "0.09579153\n",
      "0.21876845\n",
      "0.099656634\n",
      "0.13640985\n",
      "0.09349467\n",
      "0.062447887\n",
      "0.09700737\n",
      "0.101382405\n",
      "0.098458335\n",
      "0.09802173\n",
      "0.12701046\n",
      "0.15680228\n",
      "0.12604219\n",
      "0.069629215\n",
      "0.08550703\n",
      "0.118584335\n",
      "0.095167115\n",
      "0.21796827\n",
      "0.10006635\n",
      "0.13798395\n",
      "0.09454094\n",
      "0.06276818\n",
      "0.09754613\n",
      "0.1005807\n",
      "0.09808224\n",
      "0.09780152\n",
      "0.12723824\n",
      "0.15798466\n",
      "0.12700057\n",
      "0.07028122\n",
      "0.08671863\n",
      "0.11751576\n",
      "0.093951836\n",
      "0.21610746\n",
      "0.09974074\n",
      "0.13851607\n",
      "0.095037974\n",
      "0.061973132\n",
      "0.101101786\n",
      "0.10755104\n",
      "0.09652308\n",
      "0.09436728\n",
      "0.12607524\n",
      "0.14920832\n",
      "0.121896856\n",
      "0.066407695\n",
      "0.08155485\n",
      "0.1197227\n",
      "0.10112328\n",
      "0.21885224\n",
      "0.09857666\n",
      "0.13573141\n",
      "0.09233824\n",
      "0.063224226\n",
      "0.103833534\n",
      "0.10799508\n",
      "0.104272865\n",
      "0.095389575\n",
      "0.12962888\n",
      "0.15527119\n",
      "0.122776754\n",
      "0.06807327\n",
      "0.08308924\n",
      "0.11780903\n",
      "0.10131208\n",
      "0.22198477\n",
      "0.10295457\n",
      "0.13776585\n",
      "0.0927736\n",
      "0.06243404\n",
      "0.103362106\n",
      "0.10592348\n",
      "0.10195123\n",
      "0.09553951\n",
      "0.13088837\n",
      "0.15799406\n",
      "0.12546179\n",
      "0.06901497\n",
      "0.08466806\n",
      "0.11630998\n",
      "0.099169776\n",
      "0.22007263\n",
      "0.10137095\n",
      "0.14063828\n",
      "0.09386364\n",
      "0.06281876\n",
      "0.104122415\n",
      "0.104651615\n",
      "0.10147937\n",
      "0.094929926\n",
      "0.13097146\n",
      "0.1592953\n",
      "0.1267952\n",
      "0.06989947\n",
      "0.08566491\n",
      "0.115150765\n",
      "0.097873986\n",
      "0.21781793\n",
      "0.101603836\n",
      "0.14105542\n",
      "0.094319224\n",
      "0.06295334\n",
      "0.10433567\n",
      "0.10330416\n",
      "0.10132265\n",
      "0.094254464\n",
      "0.13027136\n",
      "0.15925053\n",
      "0.12735154\n",
      "0.0705407\n",
      "0.08665526\n",
      "0.11404358\n",
      "0.096316054\n",
      "0.21546343\n",
      "0.10105003\n",
      "0.14121993\n",
      "0.09445869\n",
      "0.063273035\n",
      "0.10455852\n",
      "0.10250119\n",
      "0.1005214\n",
      "0.093644574\n",
      "0.12942542\n",
      "0.15898378\n",
      "0.12746747\n",
      "0.070923254\n",
      "0.08747083\n",
      "0.113333546\n",
      "0.09534407\n",
      "0.21292892\n",
      "0.10046242\n",
      "0.14092943\n",
      "0.09458829\n",
      "0.06328231\n",
      "0.10483027\n",
      "0.10153127\n",
      "0.100297384\n",
      "0.07628109\n",
      "0.08454617\n",
      "0.11325483\n",
      "0.111185394\n",
      "0.07312297\n",
      "0.09474869\n",
      "0.10344307\n",
      "0.08250699\n",
      "0.16370076\n",
      "0.08435856\n",
      "0.1110408\n",
      "0.07878674\n",
      "0.06625193\n",
      "0.103029944\n",
      "0.09578649\n",
      "0.09358369\n",
      "0.08570905\n",
      "0.10300368\n",
      "0.14153625\n",
      "0.122940436\n",
      "0.07356207\n",
      "0.088283926\n",
      "0.105940714\n",
      "0.087487504\n",
      "0.19082546\n",
      "0.09087709\n",
      "0.12675038\n",
      "0.08565237\n",
      "0.06558748\n",
      "0.10225092\n",
      "0.09637916\n",
      "0.09411713\n",
      "0.092159644\n",
      "0.11287388\n",
      "0.15587161\n",
      "0.13272345\n",
      "0.07629034\n",
      "0.08933611\n",
      "0.10504648\n",
      "0.08953145\n",
      "0.20283209\n",
      "0.09516026\n",
      "0.13594642\n",
      "0.08876459\n",
      "0.066819\n",
      "0.10320465\n",
      "0.0964795\n",
      "0.0934748\n",
      "0.09418284\n",
      "0.11727388\n",
      "0.1618894\n",
      "0.13653398\n",
      "0.077475384\n",
      "0.0913091\n",
      "0.104843065\n",
      "0.08955055\n",
      "0.2036073\n",
      "0.09608059\n",
      "0.1385146\n",
      "0.0907908\n",
      "0.067430295\n",
      "0.10462524\n",
      "0.09523284\n",
      "0.093149334\n",
      "0.0933875\n",
      "0.117271915\n",
      "0.16338786\n",
      "0.1378464\n",
      "0.07866364\n",
      "0.09291432\n",
      "0.10444757\n",
      "0.088076845\n",
      "0.20123647\n",
      "0.09495407\n",
      "0.13853645\n",
      "0.09140333\n",
      "0.06807046\n",
      "0.1053967\n",
      "0.094668195\n",
      "0.09305571\n",
      "0.09229892\n",
      "0.11600831\n",
      "0.1627077\n",
      "0.13753225\n",
      "0.079109624\n",
      "0.09375593\n",
      "0.10449481\n",
      "0.087506145\n",
      "0.19830418\n",
      "0.094046496\n",
      "0.1371536\n",
      "0.0912403\n",
      "0.068011165\n",
      "0.10587361\n",
      "0.094073586\n",
      "0.09293824\n",
      "0.091276914\n",
      "0.11442684\n",
      "0.16141522\n",
      "0.13648297\n",
      "0.07470764\n",
      "0.083271004\n",
      "0.10218518\n",
      "0.08932446\n",
      "0.20373039\n",
      "0.092873044\n",
      "0.12883438\n",
      "0.0848018\n",
      "0.067520514\n",
      "0.10054157\n",
      "0.095570005\n",
      "0.09281565\n",
      "0.09521701\n",
      "0.11361857\n",
      "0.16666366\n",
      "0.13933046\n",
      "0.0821883\n",
      "0.0901512\n",
      "0.10727825\n",
      "0.091221325\n",
      "0.20507954\n",
      "0.09892308\n",
      "0.13783002\n",
      "0.09064263\n",
      "0.06914716\n",
      "0.10244552\n",
      "0.093447335\n",
      "0.09447256\n",
      "0.09373465\n",
      "0.11561358\n",
      "0.17191324\n",
      "0.14510241\n",
      "0.0832212\n",
      "0.0936975\n",
      "0.10410303\n",
      "0.08745739\n",
      "0.20037176\n",
      "0.096780546\n",
      "0.13912505\n",
      "0.09230765\n",
      "0.070034966\n",
      "0.10524349\n",
      "0.09170974\n",
      "0.09225743\n",
      "0.09158105\n",
      "0.11282003\n",
      "0.17037298\n",
      "0.14557773\n",
      "0.084554695\n",
      "0.0952953\n",
      "0.10456057\n",
      "0.08670491\n",
      "0.1956951\n",
      "0.09553543\n",
      "0.13707082\n",
      "0.09221614\n",
      "0.070194565\n",
      "0.105759464\n",
      "0.09132297\n",
      "0.092833295\n",
      "0.09001656\n",
      "0.11069404\n",
      "0.16819872\n",
      "0.14361057\n",
      "0.08431958\n",
      "0.09554274\n",
      "0.1041497\n",
      "0.08589445\n",
      "0.19243822\n",
      "0.09400587\n",
      "0.13470232\n",
      "0.09134101\n",
      "0.06984083\n",
      "0.10608948\n",
      "0.09074387\n",
      "0.09234989\n",
      "0.088933125\n",
      "0.10881816\n",
      "0.1659747\n",
      "0.14191005\n",
      "0.08393533\n",
      "0.095279455\n",
      "0.104171686\n",
      "0.08556598\n",
      "0.19048063\n",
      "0.09288375\n",
      "0.13286543\n",
      "0.09054528\n",
      "0.06965977\n",
      "0.10580192\n",
      "0.09059031\n",
      "0.092322566\n",
      "0.08825292\n",
      "0.10743964\n",
      "0.16424936\n",
      "0.14010254\n",
      "0.083481535\n",
      "0.0946888\n",
      "0.10419978\n",
      "0.08563448\n",
      "0.123366\n",
      "0.085187376\n",
      "0.09714916\n",
      "0.08740233\n",
      "0.06688274\n",
      "0.11957154\n",
      "0.09083162\n",
      "0.101905644\n",
      "0.07549091\n",
      "0.08847952\n",
      "0.116106614\n",
      "0.11588668\n",
      "0.069673575\n",
      "0.09004611\n",
      "0.116953254\n",
      "0.08576961\n",
      "0.16620302\n",
      "0.09096815\n",
      "0.1115285\n",
      "0.08726646\n",
      "0.06328416\n",
      "0.10254403\n",
      "0.09262441\n",
      "0.095163494\n",
      "0.085289545\n",
      "0.10467664\n",
      "0.13461697\n",
      "0.11386746\n",
      "0.06706701\n",
      "0.083034225\n",
      "0.118921146\n",
      "0.09096602\n",
      "0.20185915\n",
      "0.0954162\n",
      "0.12355451\n",
      "0.08890087\n",
      "0.06304061\n",
      "0.09961697\n",
      "0.097803585\n",
      "0.097109355\n",
      "0.093094155\n",
      "0.116125785\n",
      "0.14855945\n",
      "0.12065595\n",
      "0.068583705\n",
      "0.08223391\n",
      "0.12051742\n",
      "0.0939484\n",
      "0.21408746\n",
      "0.09998417\n",
      "0.13324969\n",
      "0.09185179\n",
      "0.06342733\n",
      "0.09888536\n",
      "0.09868785\n",
      "0.09739502\n",
      "0.09671144\n",
      "0.122284494\n",
      "0.15537164\n",
      "0.12441277\n",
      "0.06989414\n",
      "0.08332055\n",
      "0.11893346\n",
      "0.09337552\n",
      "0.21749336\n",
      "0.10152081\n",
      "0.13687645\n",
      "0.093513936\n",
      "0.06392216\n",
      "0.099737845\n",
      "0.09792127\n",
      "0.09747566\n",
      "0.09689147\n",
      "0.123631604\n",
      "0.15756063\n",
      "0.12657295\n",
      "0.071500264\n",
      "0.084922895\n",
      "0.11783792\n",
      "0.092147835\n",
      "0.21510218\n",
      "0.101944745\n",
      "0.1382013\n",
      "0.09456661\n",
      "0.06424892\n",
      "0.1002429\n",
      "0.09679253\n",
      "0.09661844\n",
      "0.09629224\n",
      "0.12354724\n",
      "0.15828033\n",
      "0.12739234\n",
      "0.07225059\n",
      "0.08627276\n",
      "0.11672133\n",
      "0.090765946\n",
      "0.21237805\n",
      "0.101310365\n",
      "0.13825276\n",
      "0.094951816\n",
      "0.06436597\n",
      "0.1012342\n",
      "0.10608569\n",
      "0.0953054\n",
      "0.09280347\n",
      "0.121327974\n",
      "0.14865316\n",
      "0.11917577\n",
      "0.06753103\n",
      "0.081035666\n",
      "0.11778246\n",
      "0.09802592\n",
      "0.21636063\n",
      "0.0994097\n",
      "0.13565782\n",
      "0.09241179\n",
      "0.065270945\n",
      "0.10436922\n",
      "0.106034756\n",
      "0.10279921\n",
      "0.09416387\n",
      "0.1247305\n",
      "0.15494701\n",
      "0.11968981\n",
      "0.06910882\n",
      "0.082439\n",
      "0.11576842\n",
      "0.09837258\n",
      "0.21987668\n",
      "0.10364129\n",
      "0.13724044\n",
      "0.09260836\n",
      "0.064307556\n",
      "0.10391718\n",
      "0.103792645\n",
      "0.100683935\n",
      "0.094326004\n",
      "0.12593517\n",
      "0.15750828\n",
      "0.12249508\n",
      "0.07022286\n",
      "0.083882295\n",
      "0.11446633\n",
      "0.09639275\n",
      "0.21795064\n",
      "0.10224167\n",
      "0.14008442\n",
      "0.09373327\n",
      "0.06470719\n",
      "0.10451436\n",
      "0.102670684\n",
      "0.100134484\n",
      "0.09377144\n",
      "0.12607889\n",
      "0.1588318\n",
      "0.123698056\n",
      "0.07105174\n",
      "0.08465149\n",
      "0.11338547\n",
      "0.09522983\n",
      "0.2160459\n",
      "0.10244753\n",
      "0.1403221\n",
      "0.09410405\n",
      "0.06487328\n",
      "0.104620084\n",
      "0.10138142\n",
      "0.10003763\n",
      "0.093107745\n",
      "0.12545651\n",
      "0.15866089\n",
      "0.124317266\n",
      "0.07175657\n",
      "0.08557341\n",
      "0.11242516\n",
      "0.09394584\n",
      "0.21364725\n",
      "0.10206356\n",
      "0.14038569\n",
      "0.094175756\n",
      "0.06502128\n",
      "0.10473339\n",
      "0.100470394\n",
      "0.09928797\n",
      "0.092538\n",
      "0.1247243\n",
      "0.15846236\n",
      "0.124532975\n",
      "0.072232924\n",
      "0.086301036\n",
      "0.11170406\n",
      "0.09293121\n",
      "0.21148123\n",
      "0.10146633\n",
      "0.13989933\n",
      "0.09416485\n",
      "0.06510884\n",
      "0.10497337\n",
      "0.09961572\n",
      "0.09903373\n",
      "0.07734461\n",
      "0.08268484\n",
      "0.11391264\n",
      "0.11036784\n",
      "0.07252932\n",
      "0.09201833\n",
      "0.10175997\n",
      "0.08254676\n",
      "0.16575481\n",
      "0.087420605\n",
      "0.11158513\n",
      "0.07877107\n",
      "0.066729374\n",
      "0.10312304\n",
      "0.09495888\n",
      "0.092370346\n",
      "0.087027855\n",
      "0.10159395\n",
      "0.1423037\n",
      "0.12289135\n",
      "0.07323323\n",
      "0.086214736\n",
      "0.104437634\n",
      "0.0869151\n",
      "0.19437546\n",
      "0.09349015\n",
      "0.12870082\n",
      "0.08584051\n",
      "0.065639816\n",
      "0.1021111\n",
      "0.09466494\n",
      "0.093102604\n",
      "0.0939791\n",
      "0.11264327\n",
      "0.15876006\n",
      "0.13442527\n",
      "0.076659754\n",
      "0.08711763\n",
      "0.1048881\n",
      "0.08967265\n",
      "0.20788544\n",
      "0.09937566\n",
      "0.13905379\n",
      "0.08967528\n",
      "0.06644002\n",
      "0.10345956\n",
      "0.094747245\n",
      "0.09236419\n",
      "0.09665855\n",
      "0.1177801\n",
      "0.16653943\n",
      "0.13799094\n",
      "0.07745114\n",
      "0.088819504\n",
      "0.10403966\n",
      "0.08959973\n",
      "0.2102899\n",
      "0.10022403\n",
      "0.14220308\n",
      "0.09190604\n",
      "0.067098886\n",
      "0.10495708\n",
      "0.093708195\n",
      "0.092218384\n",
      "0.09587678\n",
      "0.11809939\n",
      "0.1681791\n",
      "0.13971463\n",
      "0.078597374\n",
      "0.09035812\n",
      "0.10390171\n",
      "0.08874512\n",
      "0.20845892\n",
      "0.09956941\n",
      "0.14246775\n",
      "0.09257686\n",
      "0.0676019\n",
      "0.10574807\n",
      "0.09329956\n",
      "0.09192385\n",
      "0.095019326\n",
      "0.11712827\n",
      "0.16757804\n",
      "0.1392239\n",
      "0.07905415\n",
      "0.09121744\n",
      "0.10368457\n",
      "0.087927304\n",
      "0.20589505\n",
      "0.098516844\n",
      "0.14131097\n",
      "0.09245068\n",
      "0.06775035\n",
      "0.106187455\n",
      "0.092737615\n",
      "0.091664605\n",
      "0.09393588\n",
      "0.11568877\n",
      "0.16614522\n",
      "0.13813324\n",
      "0.07410166\n",
      "0.080885395\n",
      "0.10199338\n",
      "0.09027064\n",
      "0.21004395\n",
      "0.09680253\n",
      "0.1312995\n",
      "0.085955195\n",
      "0.06801402\n",
      "0.10118153\n",
      "0.0942689\n",
      "0.09197697\n",
      "0.09804136\n",
      "0.11393108\n",
      "0.16987896\n",
      "0.13947506\n",
      "0.08210808\n",
      "0.08864003\n",
      "0.105970025\n",
      "0.09060348\n",
      "0.20814404\n",
      "0.10173471\n",
      "0.13934258\n",
      "0.091808766\n",
      "0.06959411\n",
      "0.103566095\n",
      "0.09211941\n",
      "0.093762934\n",
      "0.09605697\n",
      "0.11550456\n",
      "0.17454891\n",
      "0.14621283\n",
      "0.08394425\n",
      "0.092896216\n",
      "0.10342389\n",
      "0.08630596\n",
      "0.20176548\n",
      "0.099385485\n",
      "0.14011386\n",
      "0.093351014\n",
      "0.07047186\n",
      "0.10653294\n",
      "0.09028688\n",
      "0.09098053\n",
      "0.09333093\n",
      "0.11202479\n",
      "0.17210023\n",
      "0.14544854\n",
      "0.08505076\n",
      "0.09458551\n",
      "0.10384069\n",
      "0.08544623\n",
      "0.19519366\n",
      "0.097412795\n",
      "0.13648689\n",
      "0.09246595\n",
      "0.07056251\n",
      "0.107050344\n",
      "0.09020456\n",
      "0.091757365\n",
      "0.091321915\n",
      "0.109168574\n",
      "0.16879076\n",
      "0.14287154\n",
      "0.08452181\n",
      "0.09478267\n",
      "0.103548884\n",
      "0.08463865\n",
      "0.19152206\n",
      "0.09566039\n",
      "0.13348877\n",
      "0.091143474\n",
      "0.06999533\n",
      "0.107266344\n",
      "0.08961309\n",
      "0.091235675\n",
      "0.09025498\n",
      "0.107101485\n",
      "0.16623867\n",
      "0.14067473\n",
      "0.084013\n",
      "0.09434094\n",
      "0.103555284\n",
      "0.08441538\n",
      "0.1894022\n",
      "0.09433587\n",
      "0.13120225\n",
      "0.090161845\n",
      "0.069597006\n",
      "0.10684681\n",
      "0.08938567\n",
      "0.09126389\n",
      "0.08972338\n",
      "0.105788186\n",
      "0.16425909\n",
      "0.1387557\n",
      "0.083381824\n",
      "0.09375509\n",
      "0.10339834\n",
      "0.0844436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12621865\n",
      "0.087309666\n",
      "0.097511366\n",
      "0.08704233\n",
      "0.06807593\n",
      "0.12206897\n",
      "0.0904886\n",
      "0.10134433\n",
      "0.07801179\n",
      "0.086644895\n",
      "0.11574878\n",
      "0.11207905\n",
      "0.070997424\n",
      "0.089767255\n",
      "0.11319204\n",
      "0.08565818\n",
      "0.16848241\n",
      "0.09208669\n",
      "0.11121122\n",
      "0.086931184\n",
      "0.06484315\n",
      "0.10637803\n",
      "0.093247004\n",
      "0.09484195\n",
      "0.08527405\n",
      "0.10270009\n",
      "0.13343048\n",
      "0.11073334\n",
      "0.06781673\n",
      "0.08327899\n",
      "0.114112556\n",
      "0.09071585\n",
      "0.20328817\n",
      "0.09654171\n",
      "0.12303795\n",
      "0.0888861\n",
      "0.064669706\n",
      "0.102770716\n",
      "0.09797648\n",
      "0.09644425\n",
      "0.09225195\n",
      "0.11427927\n",
      "0.1486542\n",
      "0.118127584\n",
      "0.070049085\n",
      "0.08231246\n",
      "0.11665778\n",
      "0.0929877\n",
      "0.21608604\n",
      "0.10083488\n",
      "0.13258833\n",
      "0.09198558\n",
      "0.06512236\n",
      "0.10228556\n",
      "0.098495245\n",
      "0.097049974\n",
      "0.09559518\n",
      "0.12018339\n",
      "0.15532066\n",
      "0.12239096\n",
      "0.071997926\n",
      "0.08416072\n",
      "0.114274025\n",
      "0.09149378\n",
      "0.21639028\n",
      "0.101952635\n",
      "0.13641289\n",
      "0.094162166\n",
      "0.06577649\n",
      "0.103466794\n",
      "0.09699265\n",
      "0.09648804\n",
      "0.09493636\n",
      "0.12046538\n",
      "0.15672249\n",
      "0.12423808\n",
      "0.074204534\n",
      "0.08658695\n",
      "0.112925105\n",
      "0.089222394\n",
      "0.21134986\n",
      "0.10131358\n",
      "0.13700058\n",
      "0.09499303\n",
      "0.066251814\n",
      "0.104066975\n",
      "0.09541178\n",
      "0.0956161\n",
      "0.093596056\n",
      "0.118912965\n",
      "0.15577607\n",
      "0.12428744\n",
      "0.07507849\n",
      "0.08823484\n",
      "0.111984864\n",
      "0.08780435\n",
      "0.20652404\n",
      "0.09999436\n",
      "0.1357991\n",
      "0.09495469\n",
      "0.066092655\n",
      "0.10229896\n",
      "0.10530935\n",
      "0.09418253\n",
      "0.090007976\n",
      "0.11508852\n",
      "0.14403005\n",
      "0.113980085\n",
      "0.068487324\n",
      "0.08049866\n",
      "0.115470886\n",
      "0.09615433\n",
      "0.2151655\n",
      "0.09816156\n",
      "0.13206083\n",
      "0.090696275\n",
      "0.06687592\n",
      "0.10517295\n",
      "0.106268376\n",
      "0.1017016\n",
      "0.09227783\n",
      "0.1188748\n",
      "0.15003629\n",
      "0.113677\n",
      "0.06977363\n",
      "0.08163232\n",
      "0.11329061\n",
      "0.0962608\n",
      "0.21968275\n",
      "0.10225912\n",
      "0.13424401\n",
      "0.091261625\n",
      "0.06585373\n",
      "0.104340285\n",
      "0.10447082\n",
      "0.09990043\n",
      "0.09265735\n",
      "0.12034097\n",
      "0.15277079\n",
      "0.11672908\n",
      "0.071018785\n",
      "0.08322777\n",
      "0.112211354\n",
      "0.09457052\n",
      "0.21820489\n",
      "0.10092956\n",
      "0.1368199\n",
      "0.09227103\n",
      "0.06621272\n",
      "0.10480869\n",
      "0.10318639\n",
      "0.09922556\n",
      "0.09223476\n",
      "0.12054995\n",
      "0.1540687\n",
      "0.11820254\n",
      "0.07194176\n",
      "0.083951965\n",
      "0.11101533\n",
      "0.09350051\n",
      "0.21639231\n",
      "0.101292945\n",
      "0.13717324\n",
      "0.09279867\n",
      "0.06625428\n",
      "0.105101645\n",
      "0.10182371\n",
      "0.09900982\n",
      "0.091736615\n",
      "0.12010193\n",
      "0.1541714\n",
      "0.11897571\n",
      "0.072594374\n",
      "0.08495192\n",
      "0.110041246\n",
      "0.09239045\n",
      "0.2143855\n",
      "0.10077627\n",
      "0.1373845\n",
      "0.09290625\n",
      "0.06622828\n",
      "0.10531176\n",
      "0.10086192\n",
      "0.09848659\n",
      "0.09132665\n",
      "0.11942925\n",
      "0.15410669\n",
      "0.119318515\n",
      "0.0728901\n",
      "0.085406944\n",
      "0.109468594\n",
      "0.091751434\n",
      "0.21275996\n",
      "0.100467764\n",
      "0.13721314\n",
      "0.092954725\n",
      "0.06636393\n",
      "0.1052786\n",
      "0.10025506\n",
      "0.09824976\n",
      "0.07841012\n",
      "0.08087051\n",
      "0.11311492\n",
      "0.10699803\n",
      "0.072160825\n",
      "0.08990302\n",
      "0.10006147\n",
      "0.082495935\n",
      "0.1661406\n",
      "0.08865942\n",
      "0.10958947\n",
      "0.07887642\n",
      "0.06716871\n",
      "0.1032183\n",
      "0.09462058\n",
      "0.09084535\n",
      "0.08759171\n",
      "0.09878372\n",
      "0.14056213\n",
      "0.11947907\n",
      "0.07340543\n",
      "0.08473612\n",
      "0.10336886\n",
      "0.08691554\n",
      "0.19455443\n",
      "0.094444\n",
      "0.12644655\n",
      "0.085292175\n",
      "0.06567487\n",
      "0.101433806\n",
      "0.09381072\n",
      "0.091743015\n",
      "0.0941983\n",
      "0.109327644\n",
      "0.15555151\n",
      "0.12823452\n",
      "0.075520776\n",
      "0.085217014\n",
      "0.1030107\n",
      "0.08911644\n",
      "0.2077947\n",
      "0.098907545\n",
      "0.13576974\n",
      "0.08843638\n",
      "0.066555046\n",
      "0.10254626\n",
      "0.09473808\n",
      "0.0914547\n",
      "0.09660384\n",
      "0.11394505\n",
      "0.16196376\n",
      "0.13199767\n",
      "0.076379284\n",
      "0.086624116\n",
      "0.10254227\n",
      "0.089601554\n",
      "0.21023215\n",
      "0.10024266\n",
      "0.13902234\n",
      "0.090470955\n",
      "0.06682779\n",
      "0.10367792\n",
      "0.09395457\n",
      "0.09105276\n",
      "0.09644982\n",
      "0.11481246\n",
      "0.16442415\n",
      "0.13342403\n",
      "0.07748915\n",
      "0.08786457\n",
      "0.10234913\n",
      "0.088905364\n",
      "0.20959935\n",
      "0.09999471\n",
      "0.13944381\n",
      "0.09112436\n",
      "0.0669849\n",
      "0.104540326\n",
      "0.09333728\n",
      "0.09079031\n",
      "0.095677\n",
      "0.11426909\n",
      "0.16413674\n",
      "0.13341723\n",
      "0.077904835\n",
      "0.08864123\n",
      "0.10201353\n",
      "0.088317856\n",
      "0.20775253\n",
      "0.099210516\n",
      "0.13857765\n",
      "0.09120628\n",
      "0.06710547\n",
      "0.10495885\n",
      "0.09306978\n",
      "0.09056741\n",
      "0.094826885\n",
      "0.1132422\n",
      "0.16324832\n",
      "0.13273379\n",
      "0.07292694\n",
      "0.07997975\n",
      "0.10084908\n",
      "0.09002479\n",
      "0.21142362\n",
      "0.09800425\n",
      "0.12940346\n",
      "0.086131446\n",
      "0.06779171\n",
      "0.10045143\n",
      "0.09380575\n",
      "0.09077369\n",
      "0.098439924\n",
      "0.11242901\n",
      "0.16731137\n",
      "0.13341188\n",
      "0.08005756\n",
      "0.086035945\n",
      "0.10416151\n",
      "0.090957\n",
      "0.2107925\n",
      "0.102881625\n",
      "0.13679406\n",
      "0.090934396\n",
      "0.06872002\n",
      "0.10248805\n",
      "0.09200863\n",
      "0.092701346\n",
      "0.09725213\n",
      "0.11418041\n",
      "0.17153478\n",
      "0.13917843\n",
      "0.08138074\n",
      "0.08969602\n",
      "0.10165812\n",
      "0.0874649\n",
      "0.20610735\n",
      "0.1010177\n",
      "0.13807014\n",
      "0.092497796\n",
      "0.069388635\n",
      "0.104981184\n",
      "0.09044355\n",
      "0.090254106\n",
      "0.095417306\n",
      "0.1122088\n",
      "0.1704813\n",
      "0.13912316\n",
      "0.08238392\n",
      "0.0910488\n",
      "0.10160966\n",
      "0.086766705\n",
      "0.2021281\n",
      "0.09962985\n",
      "0.13633418\n",
      "0.092225894\n",
      "0.06981065\n",
      "0.10542476\n",
      "0.09037805\n",
      "0.090849906\n",
      "0.093985334\n",
      "0.11032368\n",
      "0.16825853\n",
      "0.13778614\n",
      "0.08218823\n",
      "0.0915547\n",
      "0.10142279\n",
      "0.08624689\n",
      "0.19898738\n",
      "0.09833477\n",
      "0.13388744\n",
      "0.09135492\n",
      "0.06939161\n",
      "0.105755605\n",
      "0.08977346\n",
      "0.09035699\n",
      "0.09298782\n",
      "0.10866195\n",
      "0.16616523\n",
      "0.13619319\n",
      "0.08179258\n",
      "0.0911339\n",
      "0.1012829\n",
      "0.08593152\n",
      "0.19728656\n",
      "0.09714532\n",
      "0.13227402\n",
      "0.09071722\n",
      "0.06930974\n",
      "0.10567708\n",
      "0.08959125\n",
      "0.09034448\n",
      "0.09231606\n",
      "0.107655294\n",
      "0.16449709\n",
      "0.1347057\n",
      "0.08150114\n",
      "0.09064777\n",
      "0.10121954\n",
      "0.0860531\n",
      "0.12914924\n",
      "0.08958682\n",
      "0.098340094\n",
      "0.088759206\n",
      "0.069355644\n",
      "0.12374572\n",
      "0.09299668\n",
      "0.100141674\n",
      "0.0765544\n",
      "0.08576579\n",
      "0.11351341\n",
      "0.10875714\n",
      "0.07214693\n",
      "0.088881195\n",
      "0.108813904\n",
      "0.08648223\n",
      "0.16995399\n",
      "0.09365847\n",
      "0.10915168\n",
      "0.08778569\n",
      "0.06613432\n",
      "0.10676209\n",
      "0.09518273\n",
      "0.09432203\n",
      "0.083055034\n",
      "0.09977761\n",
      "0.129347\n",
      "0.10506768\n",
      "0.06942351\n",
      "0.08119205\n",
      "0.11242125\n",
      "0.09067461\n",
      "0.2044038\n",
      "0.096393034\n",
      "0.12078544\n",
      "0.088718005\n",
      "0.066499956\n",
      "0.10322352\n",
      "0.100429416\n",
      "0.09743886\n",
      "0.089733705\n",
      "0.10972406\n",
      "0.1412719\n",
      "0.11065772\n",
      "0.07020314\n",
      "0.08008018\n",
      "0.11474309\n",
      "0.093893334\n",
      "0.21735883\n",
      "0.10001898\n",
      "0.12946942\n",
      "0.090786085\n",
      "0.06643915\n",
      "0.10195162\n",
      "0.10129484\n",
      "0.09791686\n",
      "0.09319866\n",
      "0.11546577\n",
      "0.14815307\n",
      "0.11430699\n",
      "0.07135291\n",
      "0.08091326\n",
      "0.11314352\n",
      "0.09338197\n",
      "0.2206712\n",
      "0.10139706\n",
      "0.1337855\n",
      "0.0925773\n",
      "0.066669844\n",
      "0.10268417\n",
      "0.10019924\n",
      "0.0977991\n",
      "0.09342803\n",
      "0.11703045\n",
      "0.15107846\n",
      "0.11699153\n",
      "0.07334786\n",
      "0.08285741\n",
      "0.11132905\n",
      "0.09132976\n",
      "0.2175287\n",
      "0.10125771\n",
      "0.13562629\n",
      "0.09393886\n",
      "0.06698283\n",
      "0.103238866\n",
      "0.09872872\n",
      "0.09685695\n",
      "0.09266132\n",
      "0.116754085\n",
      "0.1517227\n",
      "0.11830772\n",
      "0.074593514\n",
      "0.08453864\n",
      "0.109886505\n",
      "0.0898187\n",
      "0.21344532\n",
      "0.10058316\n",
      "0.13566369\n",
      "0.0944404\n",
      "0.06777442\n",
      "0.10150159\n",
      "0.10793292\n",
      "0.095291816\n",
      "0.089314036\n",
      "0.113640904\n",
      "0.14390185\n",
      "0.11238186\n",
      "0.07031172\n",
      "0.07961789\n",
      "0.11202968\n",
      "0.09556629\n",
      "0.21781613\n",
      "0.099784285\n",
      "0.13336757\n",
      "0.091704436\n",
      "0.06855646\n",
      "0.10441634\n",
      "0.10829743\n",
      "0.102549404\n",
      "0.090683974\n",
      "0.11629982\n",
      "0.14826821\n",
      "0.11190957\n",
      "0.07133567\n",
      "0.08082083\n",
      "0.1101438\n",
      "0.095895156\n",
      "0.22091866\n",
      "0.10340636\n",
      "0.13418235\n",
      "0.09184347\n",
      "0.06730572\n",
      "0.1038492\n",
      "0.106404744\n",
      "0.100632176\n",
      "0.09110975\n",
      "0.1173432\n",
      "0.15049033\n",
      "0.11434904\n",
      "0.07224822\n",
      "0.081839174\n",
      "0.10902692\n",
      "0.09441417\n",
      "0.22001511\n",
      "0.10204994\n",
      "0.13689841\n",
      "0.0927891\n",
      "0.06752615\n",
      "0.10443275\n",
      "0.1053613\n",
      "0.1000013\n",
      "0.09086829\n",
      "0.117706954\n",
      "0.15216467\n",
      "0.11543342\n",
      "0.07288541\n",
      "0.08233762\n",
      "0.10794265\n",
      "0.09372995\n",
      "0.21891156\n",
      "0.10233653\n",
      "0.13709778\n",
      "0.0930848\n",
      "0.067299135\n",
      "0.1048172\n",
      "0.10417319\n",
      "0.100043386\n",
      "0.090556145\n",
      "0.117407605\n",
      "0.1523612\n",
      "0.11632652\n",
      "0.073427126\n",
      "0.08314338\n",
      "0.10698201\n",
      "0.09271235\n",
      "0.21729273\n",
      "0.10197304\n",
      "0.13745165\n",
      "0.09324704\n",
      "0.067294456\n",
      "0.104934715\n",
      "0.10330183\n",
      "0.09930488\n",
      "0.090169705\n",
      "0.11706538\n",
      "0.15259668\n",
      "0.116765186\n",
      "0.07384068\n",
      "0.08363311\n",
      "0.10632398\n",
      "0.09211971\n",
      "0.2156287\n",
      "0.10164676\n",
      "0.1373963\n",
      "0.093381114\n",
      "0.06719732\n",
      "0.10531977\n",
      "0.10248401\n",
      "0.09907482\n",
      "0.07782699\n",
      "0.07992053\n",
      "0.11342476\n",
      "0.10588725\n",
      "0.07282759\n",
      "0.08928286\n",
      "0.09724788\n",
      "0.0828163\n",
      "0.16787285\n",
      "0.08903869\n",
      "0.108601496\n",
      "0.07941285\n",
      "0.06787203\n",
      "0.10282261\n",
      "0.09607686\n",
      "0.0907878\n",
      "0.08591151\n",
      "0.096547216\n",
      "0.13886684\n",
      "0.116935305\n",
      "0.07329865\n",
      "0.08361904\n",
      "0.10092223\n",
      "0.086400166\n",
      "0.19491144\n",
      "0.09308613\n",
      "0.1240156\n",
      "0.08499857\n",
      "0.066040054\n",
      "0.10031621\n",
      "0.095371455\n",
      "0.091820955\n",
      "0.09230389\n",
      "0.10660504\n",
      "0.15247484\n",
      "0.12550108\n",
      "0.07592315\n",
      "0.08382498\n",
      "0.10070083\n",
      "0.088326916\n",
      "0.2077971\n",
      "0.09795499\n",
      "0.1330128\n",
      "0.08777229\n",
      "0.06667755\n",
      "0.10109304\n",
      "0.09627656\n",
      "0.091504656\n",
      "0.0944984\n",
      "0.111173466\n",
      "0.15879357\n",
      "0.1289995\n",
      "0.076796606\n",
      "0.08536035\n",
      "0.100257166\n",
      "0.08861119\n",
      "0.20986724\n",
      "0.098733306\n",
      "0.13607728\n",
      "0.08961096\n",
      "0.06668079\n",
      "0.10251808\n",
      "0.09523951\n",
      "0.09103562\n",
      "0.0941556\n",
      "0.11192718\n",
      "0.16090438\n",
      "0.1304989\n",
      "0.07788612\n",
      "0.086426035\n",
      "0.099983715\n",
      "0.08790617\n",
      "0.20919192\n",
      "0.098456755\n",
      "0.13647155\n",
      "0.09022311\n",
      "0.066917524\n",
      "0.10325353\n",
      "0.09486419\n",
      "0.090864725\n",
      "0.093451776\n",
      "0.11138738\n",
      "0.16073526\n",
      "0.13051885\n",
      "0.07826719\n",
      "0.0870462\n",
      "0.09982619\n",
      "0.08750829\n",
      "0.20720209\n",
      "0.097875886\n",
      "0.13565917\n",
      "0.090178445\n",
      "0.06690193\n",
      "0.10369517\n",
      "0.09443047\n",
      "0.09049662\n",
      "0.092695445\n",
      "0.11044429\n",
      "0.15984906\n",
      "0.12992084\n",
      "0.07198869\n",
      "0.07955833\n",
      "0.098623514\n",
      "0.089792244\n",
      "0.21213438\n",
      "0.096653044\n",
      "0.12619129\n",
      "0.08608629\n",
      "0.06774862\n",
      "0.098882675\n",
      "0.0949049\n",
      "0.090595126\n",
      "0.096550085\n",
      "0.10989143\n",
      "0.1630391\n",
      "0.13005498\n",
      "0.078465\n",
      "0.08368491\n",
      "0.10204405\n",
      "0.09178545\n",
      "0.21384384\n",
      "0.10174228\n",
      "0.13361554\n",
      "0.090636574\n",
      "0.06818906\n",
      "0.10046796\n",
      "0.09351061\n",
      "0.092710346\n",
      "0.09623228\n",
      "0.11264235\n",
      "0.16822542\n",
      "0.13564543\n",
      "0.07966692\n",
      "0.08668825\n",
      "0.09939766\n",
      "0.08871627\n",
      "0.21186644\n",
      "0.10067042\n",
      "0.13605636\n",
      "0.09228614\n",
      "0.068786\n",
      "0.102663696\n",
      "0.09213306\n",
      "0.0902019\n",
      "0.09491345\n",
      "0.111436814\n",
      "0.16825816\n",
      "0.13611966\n",
      "0.08097286\n",
      "0.08809334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09948884\n",
      "0.088247895\n",
      "0.20788354\n",
      "0.09975873\n",
      "0.13505352\n",
      "0.092500836\n",
      "0.06908514\n",
      "0.10334581\n",
      "0.09169973\n",
      "0.09076497\n",
      "0.09368207\n",
      "0.10993077\n",
      "0.1667692\n",
      "0.13536437\n",
      "0.08105113\n",
      "0.08880466\n",
      "0.099112354\n",
      "0.08738193\n",
      "0.20444293\n",
      "0.09847839\n",
      "0.13333935\n",
      "0.0919988\n",
      "0.06887478\n",
      "0.10392245\n",
      "0.09107159\n",
      "0.09005407\n",
      "0.09268896\n",
      "0.10834685\n",
      "0.16490895\n",
      "0.13425334\n",
      "0.0809674\n",
      "0.088761285\n",
      "0.099022724\n",
      "0.086990535\n",
      "0.202339\n",
      "0.097387776\n",
      "0.13181886\n",
      "0.09137364\n",
      "0.06872426\n",
      "0.103917\n",
      "0.09085558\n",
      "0.090031765\n",
      "0.09207443\n",
      "0.107137024\n",
      "0.16341996\n",
      "0.1330045\n",
      "0.08053806\n",
      "0.08845189\n",
      "0.09898907\n",
      "0.086896226\n",
      "0.13254888\n",
      "0.09186691\n",
      "0.09914072\n",
      "0.08887309\n",
      "0.07037862\n",
      "0.12345117\n",
      "0.09598647\n",
      "0.10058341\n",
      "0.073036626\n",
      "0.08414242\n",
      "0.111453325\n",
      "0.10686071\n",
      "0.07391462\n",
      "0.08657035\n",
      "0.105906904\n",
      "0.086246185\n",
      "0.17232688\n",
      "0.095073454\n",
      "0.10877114\n",
      "0.08650484\n",
      "0.06804717\n",
      "0.105906084\n",
      "0.09685974\n",
      "0.095706\n",
      "0.08120613\n",
      "0.09608746\n",
      "0.12516117\n",
      "0.10153257\n",
      "0.071343236\n",
      "0.079299755\n",
      "0.11016361\n",
      "0.08921828\n",
      "0.20450309\n",
      "0.09607973\n",
      "0.11791034\n",
      "0.087207966\n",
      "0.06889796\n",
      "0.10194761\n",
      "0.10234764\n",
      "0.09941815\n",
      "0.08728286\n",
      "0.10480146\n",
      "0.13555221\n",
      "0.10658246\n",
      "0.071154\n",
      "0.07813202\n",
      "0.11279953\n",
      "0.09320248\n",
      "0.21785747\n",
      "0.099215694\n",
      "0.12653401\n",
      "0.08893876\n",
      "0.068669036\n",
      "0.10090391\n",
      "0.10385591\n",
      "0.10021787\n",
      "0.09098295\n",
      "0.110524625\n",
      "0.1434866\n",
      "0.10963363\n",
      "0.07148963\n",
      "0.078370556\n",
      "0.11211088\n",
      "0.0938344\n",
      "0.22481957\n",
      "0.100525215\n",
      "0.13123214\n",
      "0.0901252\n",
      "0.0684873\n",
      "0.1010939\n",
      "0.10340725\n",
      "0.1008327\n",
      "0.092312306\n",
      "0.11308229\n",
      "0.14748624\n",
      "0.11204126\n",
      "0.07279516\n",
      "0.07925697\n",
      "0.11106829\n",
      "0.09341159\n",
      "0.2253983\n",
      "0.10134091\n",
      "0.13407665\n",
      "0.09152735\n",
      "0.06839489\n",
      "0.10124535\n",
      "0.10276201\n",
      "0.10032538\n",
      "0.09246429\n",
      "0.11413346\n",
      "0.14971378\n",
      "0.113646135\n",
      "0.07386266\n",
      "0.08044835\n",
      "0.10964204\n",
      "0.09237732\n",
      "0.2234108\n",
      "0.10126583\n",
      "0.13535574\n",
      "0.09256597\n",
      "0.06980587\n",
      "0.10079118\n",
      "0.10985156\n",
      "0.09805307\n",
      "0.0899659\n",
      "0.112892225\n",
      "0.14565405\n",
      "0.11164675\n",
      "0.071770124\n",
      "0.078248665\n",
      "0.11098407\n",
      "0.09570474\n",
      "0.22326028\n",
      "0.10146862\n",
      "0.13387372\n",
      "0.090647615\n",
      "0.070010096\n",
      "0.1034741\n",
      "0.10916877\n",
      "0.10435254\n",
      "0.089989394\n",
      "0.1136961\n",
      "0.1474468\n",
      "0.11093793\n",
      "0.07250576\n",
      "0.079352394\n",
      "0.10895783\n",
      "0.09613413\n",
      "0.2239359\n",
      "0.104189664\n",
      "0.13360038\n",
      "0.09038138\n",
      "0.069058806\n",
      "0.10284691\n",
      "0.10745041\n",
      "0.10215379\n",
      "0.08994126\n",
      "0.11397889\n",
      "0.14879744\n",
      "0.11257477\n",
      "0.07312559\n",
      "0.08039681\n",
      "0.107654884\n",
      "0.0945953\n",
      "0.22244471\n",
      "0.10268018\n",
      "0.13531326\n",
      "0.091060564\n",
      "0.06913286\n",
      "0.10334857\n",
      "0.10638443\n",
      "0.101434484\n",
      "0.089466184\n",
      "0.11405438\n",
      "0.1499631\n",
      "0.1135219\n",
      "0.0737428\n",
      "0.080678865\n",
      "0.10677177\n",
      "0.09409771\n",
      "0.22118056\n",
      "0.10312646\n",
      "0.13541012\n",
      "0.0913769\n",
      "0.06891185\n",
      "0.10358688\n",
      "0.105256066\n",
      "0.10124281\n",
      "0.08918185\n",
      "0.113817975\n",
      "0.15015389\n",
      "0.11421324\n",
      "0.07410905\n",
      "0.08137052\n",
      "0.10571378\n",
      "0.09304025\n",
      "0.21983911\n",
      "0.10248275\n",
      "0.13585463\n",
      "0.09144819\n",
      "0.0688119\n",
      "0.10377107\n",
      "0.10460341\n",
      "0.1004168\n",
      "0.08878203\n",
      "0.1135329\n",
      "0.15046409\n",
      "0.11467618\n",
      "0.074509904\n",
      "0.08182889\n",
      "0.10510801\n",
      "0.092501774\n",
      "0.21833013\n",
      "0.102386\n",
      "0.13593815\n",
      "0.09170623\n",
      "0.06868163\n",
      "0.10402441\n",
      "0.103856236\n",
      "0.100045346\n",
      "0.07611678\n",
      "0.078744166\n",
      "0.11459187\n",
      "0.10459867\n",
      "0.07262914\n",
      "0.08688219\n",
      "0.09471421\n",
      "0.08331981\n",
      "0.17211582\n",
      "0.08924098\n",
      "0.10807397\n",
      "0.079874195\n",
      "0.06925679\n",
      "0.101259224\n",
      "0.09747793\n",
      "0.09199688\n",
      "0.08392676\n",
      "0.09453973\n",
      "0.13932416\n",
      "0.11516809\n",
      "0.07364014\n",
      "0.08233617\n",
      "0.09879002\n",
      "0.086187154\n",
      "0.19700997\n",
      "0.09256248\n",
      "0.122193485\n",
      "0.084977224\n",
      "0.067139104\n",
      "0.09921433\n",
      "0.09671061\n",
      "0.09277977\n",
      "0.08940141\n",
      "0.103637055\n",
      "0.15109375\n",
      "0.122804664\n",
      "0.0754735\n",
      "0.08208136\n",
      "0.09849231\n",
      "0.0884529\n",
      "0.20999771\n",
      "0.09635471\n",
      "0.13002393\n",
      "0.0870746\n",
      "0.0673328\n",
      "0.09953377\n",
      "0.097941875\n",
      "0.092814475\n",
      "0.09178094\n",
      "0.1079694\n",
      "0.15660933\n",
      "0.12622525\n",
      "0.07643314\n",
      "0.08322975\n",
      "0.09825222\n",
      "0.08873834\n",
      "0.21215668\n",
      "0.09762366\n",
      "0.13297138\n",
      "0.08894211\n",
      "0.06719652\n",
      "0.10043095\n",
      "0.09722674\n",
      "0.09236878\n",
      "0.091569185\n",
      "0.108897984\n",
      "0.15870197\n",
      "0.1277935\n",
      "0.0775564\n",
      "0.08426193\n",
      "0.09787047\n",
      "0.08791161\n",
      "0.21139185\n",
      "0.097429946\n",
      "0.13370474\n",
      "0.089613944\n",
      "0.06730026\n",
      "0.10117985\n",
      "0.09664982\n",
      "0.09195562\n",
      "0.090863936\n",
      "0.10850384\n",
      "0.15866259\n",
      "0.12795168\n",
      "0.07797865\n",
      "0.084986426\n",
      "0.097594395\n",
      "0.08743655\n",
      "0.20931154\n",
      "0.09694865\n",
      "0.13303085\n",
      "0.089667976\n",
      "0.06715619\n",
      "0.1017672\n",
      "0.09618554\n",
      "0.0917117\n",
      "0.090119466\n",
      "0.10759803\n",
      "0.15778498\n",
      "0.12756407\n",
      "0.07189186\n",
      "0.07936217\n",
      "0.09670594\n",
      "0.08974146\n",
      "0.21462205\n",
      "0.09498598\n",
      "0.12487474\n",
      "0.087470956\n",
      "0.06832112\n",
      "0.09747307\n",
      "0.096664034\n",
      "0.092262015\n",
      "0.09410757\n",
      "0.10724492\n",
      "0.1607284\n",
      "0.12802398\n",
      "0.078228004\n",
      "0.08256807\n",
      "0.09984221\n",
      "0.091759235\n",
      "0.2161648\n",
      "0.0997489\n",
      "0.13072431\n",
      "0.09105425\n",
      "0.06846881\n",
      "0.09861738\n",
      "0.0954401\n",
      "0.09436823\n",
      "0.09410533\n",
      "0.10969329\n",
      "0.1648444\n",
      "0.13276418\n",
      "0.07896042\n",
      "0.08476054\n",
      "0.09762525\n",
      "0.089278\n",
      "0.21483883\n",
      "0.099154465\n",
      "0.13284679\n",
      "0.092572615\n",
      "0.06850461\n",
      "0.10036468\n",
      "0.09414912\n",
      "0.09185738\n",
      "0.09307963\n",
      "0.10896001\n",
      "0.16511089\n",
      "0.13331535\n",
      "0.07998551\n",
      "0.085786805\n",
      "0.09761973\n",
      "0.089048326\n",
      "0.21192634\n",
      "0.09860237\n",
      "0.1320509\n",
      "0.0927181\n",
      "0.06839386\n",
      "0.100956604\n",
      "0.09362204\n",
      "0.09232356\n",
      "0.09215228\n",
      "0.10794039\n",
      "0.1639392\n",
      "0.13278508\n",
      "0.08001867\n",
      "0.08634078\n",
      "0.097279154\n",
      "0.08829894\n",
      "0.20942765\n",
      "0.097688004\n",
      "0.13081314\n",
      "0.09232901\n",
      "0.06827801\n",
      "0.10125918\n",
      "0.09327054\n",
      "0.09155797\n",
      "0.09142454\n",
      "0.10684232\n",
      "0.1625346\n",
      "0.1318151\n",
      "0.07986145\n",
      "0.08649207\n",
      "0.097228885\n",
      "0.08788918\n",
      "0.20742795\n",
      "0.096800566\n",
      "0.12956771\n",
      "0.09197194\n",
      "0.06813064\n",
      "0.10140088\n",
      "0.092823535\n",
      "0.09143134\n",
      "0.090717115\n",
      "0.10581752\n",
      "0.16130431\n",
      "0.13095385\n",
      "0.07977502\n",
      "0.0863634\n",
      "0.09726532\n",
      "0.087678954\n",
      "0.13627662\n",
      "0.09468311\n",
      "0.09886144\n",
      "0.08800888\n",
      "0.07187249\n",
      "0.12267124\n",
      "0.097432286\n",
      "0.10082446\n",
      "0.072768785\n",
      "0.08385507\n",
      "0.11562141\n",
      "0.10819684\n",
      "0.07557067\n",
      "0.08677943\n",
      "0.10345879\n",
      "0.08652906\n",
      "0.17647696\n",
      "0.09882364\n",
      "0.10948008\n",
      "0.08683832\n",
      "0.06805173\n",
      "0.10723763\n",
      "0.09830485\n",
      "0.09637619\n",
      "0.08042784\n",
      "0.096310854\n",
      "0.12880673\n",
      "0.10396327\n",
      "0.07248467\n",
      "0.08004432\n",
      "0.107518844\n",
      "0.08871308\n",
      "0.20834333\n",
      "0.0983658\n",
      "0.118561156\n",
      "0.08857301\n",
      "0.06925293\n",
      "0.103296645\n",
      "0.103690945\n",
      "0.10077421\n",
      "0.0858084\n",
      "0.104517646\n",
      "0.13754943\n",
      "0.10871061\n",
      "0.07287192\n",
      "0.078722306\n",
      "0.11034859\n",
      "0.092588246\n",
      "0.2203024\n",
      "0.10162532\n",
      "0.12652193\n",
      "0.08997838\n",
      "0.06902299\n",
      "0.101916336\n",
      "0.10466443\n",
      "0.100952424\n",
      "0.089604124\n",
      "0.10940992\n",
      "0.14431874\n",
      "0.11060772\n",
      "0.07318426\n",
      "0.0786812\n",
      "0.10998514\n",
      "0.09306187\n",
      "0.22654814\n",
      "0.102153905\n",
      "0.12983663\n",
      "0.090613514\n",
      "0.06887393\n",
      "0.10190017\n",
      "0.10411659\n",
      "0.10167773\n",
      "0.090720445\n",
      "0.11123496\n",
      "0.14691572\n",
      "0.11219799\n",
      "0.07418447\n",
      "0.07918626\n",
      "0.10949617\n",
      "0.09288466\n",
      "0.2273235\n",
      "0.10246508\n",
      "0.13178022\n",
      "0.09146923\n",
      "0.06881089\n",
      "0.10184195\n",
      "0.10360696\n",
      "0.101006955\n",
      "0.09097853\n",
      "0.1120582\n",
      "0.14855453\n",
      "0.113383204\n",
      "0.07493636\n",
      "0.07980944\n",
      "0.10856708\n",
      "0.09241977\n",
      "0.22635908\n",
      "0.10251529\n",
      "0.13288248\n",
      "0.092195064\n",
      "0.070033\n",
      "0.10208827\n",
      "0.10855429\n",
      "0.097553216\n",
      "0.08966068\n",
      "0.11076926\n",
      "0.14838035\n",
      "0.11345259\n",
      "0.07364468\n",
      "0.07733095\n",
      "0.10858617\n",
      "0.09543567\n",
      "0.22614096\n",
      "0.10221969\n",
      "0.13250647\n",
      "0.0909178\n",
      "0.070447005\n",
      "0.10506178\n",
      "0.10843809\n",
      "0.10468338\n",
      "0.08979494\n",
      "0.11199052\n",
      "0.14944711\n",
      "0.11304174\n",
      "0.074504085\n",
      "0.07885369\n",
      "0.10675804\n",
      "0.096002385\n",
      "0.22662927\n",
      "0.105386525\n",
      "0.13199082\n",
      "0.090661846\n",
      "0.06932097\n",
      "0.104445525\n",
      "0.10671555\n",
      "0.10185806\n",
      "0.08970277\n",
      "0.112496406\n",
      "0.15094349\n",
      "0.11472016\n",
      "0.07527754\n",
      "0.07977209\n",
      "0.10566404\n",
      "0.09409507\n",
      "0.22496444\n",
      "0.10350758\n",
      "0.13371691\n",
      "0.0917362\n",
      "0.06951299\n",
      "0.1051635\n",
      "0.10581872\n",
      "0.101475835\n",
      "0.08901523\n",
      "0.11266799\n",
      "0.15192455\n",
      "0.115593046\n",
      "0.07592817\n",
      "0.08016649\n",
      "0.10488824\n",
      "0.09345929\n",
      "0.22396275\n",
      "0.10372127\n",
      "0.13369776\n",
      "0.09206368\n",
      "0.069396466\n",
      "0.10537204\n",
      "0.10494678\n",
      "0.100982636\n",
      "0.08873431\n",
      "0.11257143\n",
      "0.15207686\n",
      "0.116071604\n",
      "0.07639469\n",
      "0.080876395\n",
      "0.10402733\n",
      "0.092601776\n",
      "0.22281624\n",
      "0.103333786\n",
      "0.1340861\n",
      "0.09227952\n",
      "0.06928686\n",
      "0.10561114\n",
      "0.104275815\n",
      "0.10026242\n",
      "0.08829675\n",
      "0.11241114\n",
      "0.15238757\n",
      "0.11657956\n",
      "0.07688877\n",
      "0.08125468\n",
      "0.103628114\n",
      "0.09203129\n",
      "0.22148746\n",
      "0.10322866\n",
      "0.13388422\n",
      "0.09255177\n",
      "0.06913152\n",
      "0.10584864\n",
      "0.1035719\n",
      "0.09982878\n",
      "0.07629206\n",
      "0.07898361\n",
      "0.11813003\n",
      "0.105283804\n",
      "0.07423268\n",
      "0.0850826\n",
      "0.09302889\n",
      "0.08445553\n",
      "0.17645392\n",
      "0.09012282\n",
      "0.10690147\n",
      "0.080995254\n",
      "0.070411876\n",
      "0.102170125\n",
      "0.098104835\n",
      "0.09381293\n",
      "0.08355495\n",
      "0.094560236\n",
      "0.14235269\n",
      "0.115598194\n",
      "0.074863635\n",
      "0.08043493\n",
      "0.09800936\n",
      "0.0878261\n",
      "0.20291676\n",
      "0.09354242\n",
      "0.12133348\n",
      "0.08642839\n",
      "0.068934135\n",
      "0.09988713\n",
      "0.09829026\n",
      "0.09503785\n",
      "0.08881895\n",
      "0.10361637\n",
      "0.1551233\n",
      "0.122869916\n",
      "0.07644895\n",
      "0.07952867\n",
      "0.098134205\n",
      "0.09013416\n",
      "0.21703869\n",
      "0.09737258\n",
      "0.12992962\n",
      "0.088366635\n",
      "0.068898715\n",
      "0.10020361\n",
      "0.09924604\n",
      "0.095190905\n",
      "0.091547504\n",
      "0.10820354\n",
      "0.1610972\n",
      "0.12643641\n",
      "0.0771311\n",
      "0.08037349\n",
      "0.09809132\n",
      "0.090847716\n",
      "0.22070153\n",
      "0.09874252\n",
      "0.13335551\n",
      "0.09006737\n",
      "0.06868948\n",
      "0.10086094\n",
      "0.09868874\n",
      "0.094861194\n",
      "0.0916255\n",
      "0.10946769\n",
      "0.16372268\n",
      "0.12835857\n",
      "0.07824157\n",
      "0.08078446\n",
      "0.09799151\n",
      "0.09046882\n",
      "0.22088365\n",
      "0.09896069\n",
      "0.1347455\n",
      "0.09089171\n",
      "0.06859981\n",
      "0.10152551\n",
      "0.09800461\n",
      "0.09469993\n",
      "0.0913103\n",
      "0.1095604\n",
      "0.16426207\n",
      "0.12913497\n",
      "0.0787429\n",
      "0.08147737\n",
      "0.09773634\n",
      "0.08982477\n",
      "0.21945646\n",
      "0.09871485\n",
      "0.13472065\n",
      "0.09116542\n",
      "0.068426095\n",
      "0.101944104\n",
      "0.097360656\n",
      "0.094218224\n",
      "0.0905936\n",
      "0.10890521\n",
      "0.16370942\n",
      "0.12934627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07328019\n",
      "0.07770056\n",
      "0.0972685\n",
      "0.091527686\n",
      "0.22513652\n",
      "0.096734226\n",
      "0.12725897\n",
      "0.090293154\n",
      "0.06991466\n",
      "0.098198\n",
      "0.098501116\n",
      "0.0950016\n",
      "0.094474904\n",
      "0.10841608\n",
      "0.1663804\n",
      "0.1313039\n",
      "0.0800854\n",
      "0.08093156\n",
      "0.09982385\n",
      "0.09260668\n",
      "0.22356941\n",
      "0.1014019\n",
      "0.13154253\n",
      "0.09367014\n",
      "0.06992878\n",
      "0.09964843\n",
      "0.0969359\n",
      "0.09633219\n",
      "0.093754545\n",
      "0.10992748\n",
      "0.16976903\n",
      "0.13559647\n",
      "0.081105106\n",
      "0.083144054\n",
      "0.09764752\n",
      "0.08996245\n",
      "0.22038475\n",
      "0.10038994\n",
      "0.1331385\n",
      "0.09504606\n",
      "0.06973946\n",
      "0.10145069\n",
      "0.095271274\n",
      "0.09401615\n",
      "0.09231704\n",
      "0.10872776\n",
      "0.16930372\n",
      "0.13581635\n",
      "0.081967816\n",
      "0.08408398\n",
      "0.09749475\n",
      "0.08924875\n",
      "0.21738856\n",
      "0.09945429\n",
      "0.13226075\n",
      "0.094800845\n",
      "0.069781944\n",
      "0.10193677\n",
      "0.094747745\n",
      "0.094107755\n",
      "0.091307476\n",
      "0.107470125\n",
      "0.16791613\n",
      "0.1353177\n",
      "0.08219411\n",
      "0.08481763\n",
      "0.09728283\n",
      "0.088678986\n",
      "0.21429369\n",
      "0.09880053\n",
      "0.13082856\n",
      "0.094416544\n",
      "0.06943579\n",
      "0.102531\n",
      "0.094087064\n",
      "0.09352616\n",
      "0.090377994\n",
      "0.10625006\n",
      "0.16650425\n",
      "0.13449836\n",
      "0.08213969\n",
      "0.0849841\n",
      "0.09723839\n",
      "0.08819249\n",
      "0.21199028\n",
      "0.09801786\n",
      "0.1294935\n",
      "0.09395731\n",
      "0.06922978\n",
      "0.10268832\n",
      "0.09372886\n",
      "0.093310885\n",
      "0.08968184\n",
      "0.10521902\n",
      "0.1651781\n",
      "0.13363595\n",
      "0.08196142\n",
      "0.08494125\n",
      "0.097217284\n",
      "0.08792754\n",
      "0.14015836\n",
      "0.096431084\n",
      "0.09950057\n",
      "0.08830852\n",
      "0.07228772\n",
      "0.12581038\n",
      "0.09710225\n",
      "0.09920922\n",
      "0.073108755\n",
      "0.084387586\n",
      "0.121613294\n",
      "0.113123946\n",
      "0.078145176\n",
      "0.08780994\n",
      "0.10274404\n",
      "0.087979674\n",
      "0.17927267\n",
      "0.10051791\n",
      "0.11088414\n",
      "0.08831121\n",
      "0.06844542\n",
      "0.11004385\n",
      "0.09744725\n",
      "0.09604058\n",
      "0.08047799\n",
      "0.09651355\n",
      "0.1337443\n",
      "0.10909144\n",
      "0.074725\n",
      "0.08089174\n",
      "0.10651271\n",
      "0.089860335\n",
      "0.21079598\n",
      "0.10079891\n",
      "0.119526684\n",
      "0.09004972\n",
      "0.06918087\n",
      "0.105734006\n",
      "0.10279334\n",
      "0.09931625\n",
      "0.085610695\n",
      "0.105112314\n",
      "0.14256756\n",
      "0.114819124\n",
      "0.07599703\n",
      "0.08005547\n",
      "0.10915305\n",
      "0.093200035\n",
      "0.22153755\n",
      "0.104184024\n",
      "0.12756312\n",
      "0.09216308\n",
      "0.068843774\n",
      "0.104441985\n",
      "0.103392415\n",
      "0.09955309\n",
      "0.08896167\n",
      "0.10978369\n",
      "0.14901477\n",
      "0.1163047\n",
      "0.07618825\n",
      "0.079820625\n",
      "0.10887019\n",
      "0.09298046\n",
      "0.2271099\n",
      "0.10394557\n",
      "0.1300245\n",
      "0.09259422\n",
      "0.0686345\n",
      "0.10432879\n",
      "0.1027168\n",
      "0.1001156\n",
      "0.0895161\n",
      "0.1108268\n",
      "0.15044793\n",
      "0.1169007\n",
      "0.077257015\n",
      "0.080219984\n",
      "0.10871701\n",
      "0.09267978\n",
      "0.22718498\n",
      "0.103943996\n",
      "0.13120295\n",
      "0.09302403\n",
      "0.068690434\n",
      "0.1040765\n",
      "0.10239881\n",
      "0.0998899\n",
      "0.089863054\n",
      "0.11120755\n",
      "0.15147913\n",
      "0.11695876\n",
      "0.07770235\n",
      "0.080641046\n",
      "0.107925095\n",
      "0.09212051\n",
      "0.22620745\n",
      "0.10359503\n",
      "0.13150908\n",
      "0.09327309\n",
      "0.06877276\n",
      "0.10400715\n",
      "0.106305644\n",
      "0.09660911\n",
      "0.08826921\n",
      "0.10911383\n",
      "0.15024808\n",
      "0.11715786\n",
      "0.07601111\n",
      "0.077762395\n",
      "0.10728654\n",
      "0.09669729\n",
      "0.2279057\n",
      "0.10302597\n",
      "0.13209513\n",
      "0.092368364\n",
      "0.06947197\n",
      "0.107000135\n",
      "0.106224075\n",
      "0.10400037\n",
      "0.08919554\n",
      "0.110907435\n",
      "0.15179347\n",
      "0.11667321\n",
      "0.07715243\n",
      "0.07939735\n",
      "0.105911836\n",
      "0.097430795\n",
      "0.22892262\n",
      "0.10671302\n",
      "0.13136292\n",
      "0.092165425\n",
      "0.06834265\n",
      "0.10621146\n",
      "0.10501309\n",
      "0.10166335\n",
      "0.08938061\n",
      "0.11188341\n",
      "0.15353027\n",
      "0.11827418\n",
      "0.07797694\n",
      "0.08045025\n",
      "0.10478895\n",
      "0.09516428\n",
      "0.22763117\n",
      "0.1048052\n",
      "0.13336894\n",
      "0.09332251\n",
      "0.06871272\n",
      "0.106824644\n",
      "0.10426644\n",
      "0.10121215\n",
      "0.08880783\n",
      "0.11217467\n",
      "0.15457202\n",
      "0.11909241\n",
      "0.078766964\n",
      "0.081040286\n",
      "0.104037054\n",
      "0.094751686\n",
      "0.22615197\n",
      "0.1052974\n",
      "0.13329047\n",
      "0.09368145\n",
      "0.0686016\n",
      "0.10729722\n",
      "0.10340505\n",
      "0.10095923\n",
      "0.088496104\n",
      "0.112115264\n",
      "0.15474313\n",
      "0.119521186\n",
      "0.07919061\n",
      "0.081812695\n",
      "0.10322307\n",
      "0.09373531\n",
      "0.22491997\n",
      "0.104916394\n",
      "0.13353683\n",
      "0.093811885\n",
      "0.06857098\n",
      "0.107642904\n",
      "0.10285887\n",
      "0.10019985\n",
      "0.08802669\n",
      "0.111890525\n",
      "0.1549282\n",
      "0.11985139\n",
      "0.0795854\n",
      "0.08231509\n",
      "0.10281405\n",
      "0.09322437\n",
      "0.22336166\n",
      "0.10462661\n",
      "0.13329189\n",
      "0.09410089\n",
      "0.06849753\n",
      "0.10809068\n",
      "0.10241014\n",
      "0.10006832\n",
      "0.07722138\n",
      "0.079364896\n",
      "0.11897261\n",
      "0.10735367\n",
      "0.076595165\n",
      "0.08470996\n",
      "0.093152866\n",
      "0.085100695\n",
      "0.17738256\n",
      "0.091903806\n",
      "0.10718125\n",
      "0.08404175\n",
      "0.07074366\n",
      "0.10545605\n",
      "0.0984529\n",
      "0.09508072\n",
      "0.0836541\n",
      "0.09484771\n",
      "0.14444564\n",
      "0.11766883\n",
      "0.077380344\n",
      "0.0801608\n",
      "0.097627446\n",
      "0.08660202\n",
      "0.20368086\n",
      "0.09490614\n",
      "0.12166814\n",
      "0.08944567\n",
      "0.06952344\n",
      "0.10287042\n",
      "0.09808785\n",
      "0.0959397\n",
      "0.0882126\n",
      "0.10332963\n",
      "0.15637054\n",
      "0.12469985\n",
      "0.079085395\n",
      "0.07908802\n",
      "0.097864226\n",
      "0.08875209\n",
      "0.21695882\n",
      "0.09833908\n",
      "0.12963675\n",
      "0.09105842\n",
      "0.06975645\n",
      "0.103027485\n",
      "0.09940259\n",
      "0.09642257\n",
      "0.090743594\n",
      "0.107661545\n",
      "0.1621026\n",
      "0.128066\n",
      "0.07972558\n",
      "0.079817995\n",
      "0.09813636\n",
      "0.089532554\n",
      "0.22055691\n",
      "0.09987277\n",
      "0.13313189\n",
      "0.09276287\n",
      "0.069335334\n",
      "0.1035579\n",
      "0.09855943\n",
      "0.09589974\n",
      "0.09080139\n",
      "0.10885069\n",
      "0.16476904\n",
      "0.13000467\n",
      "0.080818444\n",
      "0.08014748\n",
      "0.09792127\n",
      "0.089140765\n",
      "0.22101417\n",
      "0.09985423\n",
      "0.134363\n",
      "0.09335615\n",
      "0.06939606\n",
      "0.10414407\n",
      "0.09815135\n",
      "0.09577688\n",
      "0.090578385\n",
      "0.108978264\n",
      "0.16514748\n",
      "0.13073252\n",
      "0.081199124\n",
      "0.0807256\n",
      "0.09772509\n",
      "0.08881281\n",
      "0.21960984\n",
      "0.09978897\n",
      "0.1343433\n",
      "0.09365566\n",
      "0.06921064\n",
      "0.10460273\n",
      "0.09744678\n",
      "0.09534154\n",
      "0.08991332\n",
      "0.10839941\n",
      "0.16485287\n",
      "0.13091391\n"
     ]
    }
   ],
   "source": [
    "# train DEC\n",
    "DEC.compile(optimizer='Adam', loss='kld')\n",
    "\n",
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 10000\n",
    "update_interval = 100\n",
    "index_array = np.arange(D_train.shape[0])\n",
    "batch_size = 256\n",
    "conv_threshold = 0.0001\n",
    "\n",
    "for i in range(int(maxiter)):\n",
    "    if i % update_interval == 0:\n",
    "        q = DEC.predict(D_train, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "        \n",
    "        D_clus = q.argmax(axis=1)\n",
    "        change_ratio = np.sum(D_clus != D_clus_pre).astype(np.float32) / D_clus.shape[0]\n",
    "        D_clus_pre = np.copy(D_clus)\n",
    "        if i > 0 and change_ratio < conv_threshold:\n",
    "            print('Reached convergence threshold. Stopping training.')\n",
    "            break \n",
    "\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, D_train.shape[0])]\n",
    "    loss = DEC.train_on_batch(x=D_train[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= D_train.shape[0] else 0\n",
    "    print(loss)\n",
    "    \n",
    "    \n",
    "DEC.save_weights(model_path + '/DECweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEC.load_weights(model_path + '/DECweights')\n",
    "D_clus = DEC.predict(D_train, verbose=0)\n",
    "D_clus = D_clus.argmax(1)\n",
    "plot_cluster(D_train, D_clus, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train SCCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BranchRModel(rdims, bdims):\n",
    "    inp = Input(shape=(R_train.shape[-1], ), name='recipient_input')\n",
    "    r = inp\n",
    "    r = BatchNormalization()(r)\n",
    "    for i in range(len(rdims)-1):\n",
    "        r = Dense(rdims[i], activation='relu', kernel_initializer='normal', name='r_representation_%d' % i)(r)\n",
    "    r = BatchNormalization()(r)\n",
    "    r = Dense(rdims[-1], activation='relu', kernel_initializer='normal', name='r_representation')(r)\n",
    "    \n",
    "    ys = []\n",
    "    for i in range(n_clusters):\n",
    "        z = Dense(bdims[0], activation='relu', kernel_initializer='normal', name='branch_%d_0' % i)(r)\n",
    "        for j in range(1, len(bdims)):\n",
    "            z = Dense(bdims[j], activation='relu', kernel_initializer='normal', name='branch_%d_%d' % (i, j))(z)\n",
    "        z = Dense(1, activation='relu', kernel_initializer='normal', name='branch_%d' % i)(z)\n",
    "        ys.append(z)\n",
    "        \n",
    "    ys = Concatenate()(ys)\n",
    "    out = Concatenate()([ys, r, DEC.output])\n",
    "    \n",
    "    return Model(inputs=inp, outputs=ys), Model(inputs=[inp, DEC.input], outputs=out)\n",
    "\n",
    "alpha = 10\n",
    "beta = .2\n",
    "\n",
    "def RMloss(y_true, y_pred):\n",
    "    ys = y_pred[:, 0:n_clusters]\n",
    "    r = y_pred[:, n_clusters:-n_clusters]\n",
    "    tpred = y_pred[:, -n_clusters:]\n",
    "    \n",
    "    y = y_true[:, 0]\n",
    "    p = y_true[:, 1:]\n",
    "    \n",
    "    ypred = K.sum(ys*tpred, axis=-1)\n",
    "    yloss = K.square(ypred-y)\n",
    "    tloss = KLD(p, tpred)\n",
    "    \n",
    "    tmean = K.mean(r, axis=0)\n",
    "    tvar = K.var(r, axis=0)\n",
    "    rloss = 0\n",
    "    for i in range(n_clusters):\n",
    "        w = K.reshape(p[:, i], shape=(-1, 1))\n",
    "        weighted = r * w\n",
    "        mean = K.mean(weighted, axis=0)\n",
    "        var = K.var(weighted, axis=0)\n",
    "        rloss += K.sum(K.square(mean-tmean)) + K.sum(K.square(var-tvar))\n",
    "    \n",
    "    return yloss + alpha*tloss + beta*rloss\n",
    "\n",
    "\n",
    "def train_branch_r():\n",
    "    BM, TBM = BranchRModel(rdims, bdims)\n",
    "    TBM.compile(optimizer='Adam', loss=RMloss)\n",
    "    checkpointer = ModelCheckpoint(filepath='./model/BranchRCheckpoint', verbose=1, save_best_only=True)\n",
    "\n",
    "    DEC.load_weights(model_path + '/DECweights')\n",
    "\n",
    "    history = {}\n",
    "    history['loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    for ite in range(10):\n",
    "        q = DEC.predict(D_train, verbose=0)\n",
    "        p = target_distribution(q)\n",
    "    \n",
    "        q_val = DEC.predict(D_test, verbose=0)\n",
    "        p_val = target_distribution(q_val)\n",
    "    \n",
    "        target = np.hstack([y_train.reshape(-1, 1), p])\n",
    "        target_val = np.hstack([y_test.reshape(-1, 1), p_val])\n",
    "    \n",
    "        hist = TBM.fit([R_train, D_train], target, validation_data=([R_test, D_test], target_val), batch_size=256, epochs=5, callbacks=[checkpointer])\n",
    "        history['loss'] += hist.history['loss']\n",
    "        history['val_loss'] += hist.history['val_loss']\n",
    "    return BM, TBM, DEC, history\n",
    "        \n",
    "#TBM.save_weights(model_path + '/RTBMweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 2s 401us/step - loss: 2434871.6345 - val_loss: 2564191.5504\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2564191.55045, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 2432602.8614 - val_loss: 2553574.8722\n",
      "\n",
      "Epoch 00002: val_loss improved from 2564191.55045 to 2553574.87220, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 2401943.2673 - val_loss: 2417963.0673\n",
      "\n",
      "Epoch 00003: val_loss improved from 2553574.87220 to 2417963.06726, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 2164752.8852 - val_loss: 1753132.9372\n",
      "\n",
      "Epoch 00004: val_loss improved from 2417963.06726 to 1753132.93722, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 1434024.3380 - val_loss: 1847031.7539\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1753132.93722\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 1130841.5252 - val_loss: 1238227.5628\n",
      "\n",
      "Epoch 00001: val_loss improved from 1753132.93722 to 1238227.56278, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 1026441.2071 - val_loss: 1121089.1180\n",
      "\n",
      "Epoch 00002: val_loss improved from 1238227.56278 to 1121089.11799, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 978678.0181 - val_loss: 1070334.0645\n",
      "\n",
      "Epoch 00003: val_loss improved from 1121089.11799 to 1070334.06446, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 950816.1546 - val_loss: 1054370.3941\n",
      "\n",
      "Epoch 00004: val_loss improved from 1070334.06446 to 1054370.39406, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 937419.1774 - val_loss: 1055664.5846\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 918630.4724 - val_loss: 1057080.9795\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 897153.0372 - val_loss: 1063172.6202\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 886703.4012 - val_loss: 1062762.6129\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 863471.4819 - val_loss: 1071539.5981\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 843112.8521 - val_loss: 1086717.6216\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 825547.3733 - val_loss: 1088336.8949\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 796994.9206 - val_loss: 1094382.0207\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 781952.3990 - val_loss: 1094184.1693\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 51us/step - loss: 768201.8011 - val_loss: 1096090.6839\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 745442.0251 - val_loss: 1129120.2040\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 733387.6627 - val_loss: 1138554.9417\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 718784.4625 - val_loss: 1134900.0376\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 710593.7770 - val_loss: 1149769.7024\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 688979.5431 - val_loss: 1150505.1872\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 679935.2096 - val_loss: 1199600.4041\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 693367.3156 - val_loss: 1229154.0107\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 662624.6856 - val_loss: 1166697.5577\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 651896.6409 - val_loss: 1199319.1385\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 637018.5334 - val_loss: 1211884.3526\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 606657.6016 - val_loss: 1259154.7197\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 607545.4489 - val_loss: 1236040.2029\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 603033.1925 - val_loss: 1275893.6889\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 593779.7660 - val_loss: 1343509.8251\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 578067.0082 - val_loss: 1258025.2410\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 569938.9856 - val_loss: 1261457.8537\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 51us/step - loss: 574653.1137 - val_loss: 1358456.6620\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 553601.2188 - val_loss: 1339744.0006\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 534523.7578 - val_loss: 1344073.1536\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 529331.0884 - val_loss: 1402505.5645\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 520929.1349 - val_loss: 1352306.9512\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 523772.8888 - val_loss: 1383379.5880\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 506826.1969 - val_loss: 1461917.4109\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 488021.8040 - val_loss: 1455464.0830\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 54us/step - loss: 490246.3140 - val_loss: 1484951.3346\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 477751.8760 - val_loss: 1412856.5387\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 463046.0093 - val_loss: 1393450.9182\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1054370.39406\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 470074.2216 - val_loss: 1502862.4467\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1054370.39406\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 462089.7642 - val_loss: 1489814.4378\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1054370.39406\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 437972.7296 - val_loss: 1518638.9950\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1054370.39406\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 51us/step - loss: 444110.9299 - val_loss: 1509491.2455\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1054370.39406\n",
      "13.867942\n",
      "13.869766\n"
     ]
    }
   ],
   "source": [
    "BM, TBM, DEC, hist = train_branch_r()\n",
    "TBM.load_weights('./model/BranchRCheckpoint')\n",
    "    \n",
    "test_Y_pred = BM.predict(R_test)\n",
    "test_T_pred = DEC.predict(D_test)\n",
    "\n",
    "# expected loss\n",
    "e_test_y_pred = np.sum(np.multiply(test_Y_pred, test_T_pred), axis=-1)\n",
    "e_test_loss = np.mean(np.square(e_test_y_pred - y_test))\n",
    "\n",
    "#absolute loss\n",
    "a_test_clus = np.argmax(test_T_pred, axis=-1)\n",
    "a_test_y_pred = test_Y_pred[np.arange(a_test_clus.size), a_test_clus]\n",
    "a_test_loss = np.mean(np.square(a_test_y_pred - y_test))\n",
    "\n",
    "print(np.log(e_test_loss))\n",
    "print(np.log(a_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916 2398 700\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2dfZAkZ33fP8/2GnTs6HqFkR0iEUv45LOxlB3drjHOLZo5i/BmEilVuEpOKqhsHMqJy+ZccSUQU4Vsk6o4RcyZSmyjoDjY5TIQ7LKI4pjiZWZ1R9ny7WnnkECcdQIKyRAQpdu5neOwb+d++eN5nplnemdmZ+9Wd3vX389U13Q//fTz1jPf5/e89NPOzBBCCFEOpi53AoQQQlw6JPpCCFEiJPpCCFEiJPpCCFEiJPpCCFEipi93Asbxkpe8xG666aaLDucEJwDo0AGgQoWznGUXuwbcAPay96Lj2w6GpRl2TvpSWrTo0h15Pk37CU5w9oQv+7179/byGfN14kQ43pvkM7ixdzDvxWu3m62GPzTtY2jRAqBK9QJSd/HxX9T1J07w7bNneXrXrvH+472LFO/huDhH3PdN/Uxy3VXOsWPHvmVm1w87t6NF/6abbmJ5efmiw6lTB2CJJQDmmadFiyrVATeAJs2Ljm87GJZm2DnpS5lllg6dkcIfy/tk/aSvxDq+MmtVQmXRhJOtkwB0bg+VXKXCoVaLarUKHe9GxVce9aY/LFaI21U2sewnDb9eD/47/bQDNJvD/c8yC9Arr5P4vK+yemHp3WL823J9vU6r1eJgtbqpvwEKfmPcQ8OI104SfupnkuuuYkKZXjPq/I4W/e0i/lnjnzn98w5z2wmMS/NOI4pVFP9FFmnSHEh7nXrPshVXMEHsq+02VeC+pSVas7MbxT8K79LShuuB8MuApXB+QPyL144T9tRPqwXV6vjrRDlEX1x+mjSh6SuxVt23sprNpGKoNgGo18LxGMstnnm+KsStVrgxrWOt1oS0kkyPL5Stxr/d14udQbx/oSKtjPJXKtEf9ufdyRY07Pz0pRTF60pKu5iQZtOPQASL/75g4TeH+CP4G+Ye/Q+taIrXDquExvmRhT+WUom+uPxEi3/gOD0/7g8+LKznka2Gv1UL+WIt/IuNf7uvF5eXtMW2tLTUGeXP7eS1dxYWFmw7BnKFEKIsRNE3s2uHndc8fSGEuIoIFv+JUecl+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSImEn3n3C855z7vnHvcOfdHzrlrnHM3O+cecc496Zz7iHPuBcHvC8PxyXD+piScdwb3E8651z0/WRJCCDGKTUXfOXcD8IvAgpndCmTAPcBvAO8zs1uAU8BbwyVvBU6Z2R7gfcEfzrlXhOt+GHg98NvOuWx7syOEEGIck3bvTAO7nHPTwIuArwM/DnwsnP8QcHfYvyscE87f6Zxzwf3DZva3ZvZl4CTwyovPghBCiEnZVPTN7G+A9wJfxYt9GzgGrJrZevD2DHBD2L8BeDpcux78f3fqPuQaIYQQl4BJuneuw1vpNwN/H5gB3jDEq8VLRpwb5V6M723OuWXn3PKzzz67WfKEEEJsgUm6d14DfNnMnjWzc8CfAP8ImA3dPQA3Al8L+88ALwMI53PgudR9yDU9zOx+M1sws4Xrr7/+ArIkhBBiFJOI/leBVznnXhT65u8EvgA0gDcHP/cCD4b9j4djwvnPmJkF93vC7J6bgVuAv9qebAghhJiE6c08mNkjzrmPAY8C68AKcD/wf4APO+feE9weCJc8APyBc+4k3sK/J4TzeefcR/EVxjrw82bW3eb8CCGEGIPzRvjOZGFhwZaXly93MoQQ4orCOXfMzBaGndMTuUIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSIk+kIIUSImEn3n3Kxz7mPOuS86555wzv2Yc+7FzrlPOueeDN/XBb/OOfd+59xJ59znnHP7knDuDf6fdM7d+3xlSgghxHAmtfR/C/hzM/tBYA54AngH8GkzuwX4dDgGeANwS9jeBvwOgHPuxcC7gR8FXgm8O1YUQgghLg2bir5zbjdwB/AAgJn9nZmtAncBHwrePgTcHfbvAn7fPH8JzDrnXgq8DvikmT1nZqeATwKv39bcCCGEGMsklv7LgWeB33POrTjnPuicmwG+18y+DhC+vyf4vwF4Orn+meA2yn0A59zbnHPLzrnlZ599dssZEkIIMZpJRH8a2Af8jpndDpyh35UzDDfEzca4DzqY3W9mC2a2cP3110+QPCGEEJMyieg/AzxjZo+E44/hK4FvhG4bwvc3E/8vS66/EfjaGHchhBCXiE1F38z+H/C0c25vcLoT+ALwcSDOwLkXeDDsfxx4S5jF8yqgHbp/PgG81jl3XRjAfW1wE0IIcYmYntDfLwB/6Jx7AfAl4KfxFcZHnXNvBb4K/GTw+2fAG4GTwLeDX8zsOefcrwNHg79fM7PntiUXQgghJsKZbehW3zEsLCzY8vLy5U6GEEJcUTjnjpnZwrBzeiJXCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKhERfCCFKxMSi75zLnHMrzrmHwvHNzrlHnHNPOuc+4px7QXB/YTg+Gc7flITxzuB+wjn3uu3OjBBCiPFsxdJ/O/BEcvwbwPvM7BbgFPDW4P5W4JSZ7QHeF/zhnHsFcA/ww8Drgd92zmUXl3whhBBbYSLRd87dCPwE8MFw7IAfBz4WvHwIuDvs3xWOCefvDP7vAj5sZn9rZl8GTgKv3I5MCCGEmIxJLf1DwL8Dzofj7wZWzWw9HD8D3BD2bwCeBgjn28F/z33INT2cc29zzi0755afffbZLWRFCCHEZmwq+s65NwHfNLNjqfMQr7bJuXHX9B3M7jezBTNbuP766zdLnhBCiC0wPYGf/cA/dc69EbgG2I23/Gedc9PBmr8R+Frw/wzwMuAZ59w0kAPPJe6R9BohhBCXgE0tfTN7p5ndaGY34QdiP2Nm/wJoAG8O3u4FHgz7Hw/HhPOfMTML7veE2T03A7cAf7VtORFCCLEpk1j6o/j3wIedc+8BVoAHgvsDwB84507iLfx7AMzs8865jwJfANaBnzez7kXEL4QQYos4b4TvTBYWFmx5eflyJ0MIIa4onHPHzGxh2Dk9kSuEECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECVCoi+EECViU9F3zr3MOddwzj3hnPu8c+7twf3FzrlPOueeDN/XBXfnnHu/c+6kc+5zzrl9SVj3Bv9POufuff6yJYQQYhiTWPrrwL81sx8CXgX8vHPuFcA7gE+b2S3Ap8MxwBuAW8L2NuB3wFcSwLuBHwVeCbw7VhRCCCEuDZuKvpl93cweDftrwBPADcBdwIeCtw8Bd4f9u4DfN89fArPOuZcCrwM+aWbPmdkp4JPA67c1N0IIIcaypT5959xNwO3AI8D3mtnXwVcMwPcEbzcATyeXPRPcRrkX43ibc27ZObf87LPPbiV5QgghNmFi0XfOVYA/Bg6a2elxXoe42Rj3QQez+81swcwWrr/++kmTJ4QQYgImEn3n3HfhBf8PzexPgvM3QrcN4fubwf0Z4GXJ5TcCXxvjLoQQ4hIxyewdBzwAPGFmv5mc+jgQZ+DcCzyYuL8lzOJ5FdAO3T+fAF7rnLsuDOC+NrgJIYS4RExP4Gc/8C+Bx5xzreD2H4D/BHzUOfdW4KvAT4Zzfwa8ETgJfBv4aQAze8459+vA0eDv18zsuW3JhRBCiIlwZhu61XcMCwsLtry8fLmTIYQQVxTOuWNmtjDsnJ7IFUKIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRF0KIEiHRvwDq4XM547wcaRBCXPlI9IUQokRM8rpEEYiW9RJLA8dNmpcszulwy7p0L1kahBBXD7L0hRCiRMjS3wLRmt6qdX0x1ni8ZpZZANq0AcjJLzhMIUR5uSot/Z0w0Hop6NDRYK7Y2dTrfhM7Bln6F8BWLfztGANYZXVDGDtZ8DXWIMTO5KoR/Tp1WrSAfhfIdgnPuHAux+BuixazzG5LPifJ28SVXLDoms2tp+NC4ttOVEltM9G6X1oaPL7A34bYPq4a0b9QtvJnn8RvrHhSf8+HoHTobFtYW6WYn7TF0aJFnfrzUgleykpc4i+uVq540S9a2pGMrLc/yyxVqhv+yLF1UKU69M8+avA0ZZjwPZ8c4Uhvumaapq2K1LgWymatl7RiA2jVQwtrqT1wPGmSLra1dDFCPVsP97g5+h6LCyBa9BNa+BfbShSTc8WLPmwUIfDz2Ie5R6Lgt2mzxNJAJTGKcd0p47qWopDWqY8Ups2EK55PBT/GNy6fw8JPK7th/sadi3HCxkoxUqXaC2M7LfyLbT0MC6dDhwqVnp/t7hrcMuoGEc8zV6zoDxOxSPzjpmK0xFLP4gd6gh9JH3aKghXPxwqhKLhpOmK4xRbHdtGiNbZLZ1yFMowoyMO6auK5+CBYcdpopJeeEG1eD62OZv+eTJKui50Ku1ll0ArWPM2kIqv7Mu0udWnTJquHe9zceI9Hxl8Pv5Vmvyxb9VDZbYNoXxVdTRNa+Euh718W//PPFSv6sLH/uNg0z8iGCjV40Rsm0MXuE2DAEuzQGWoJjxKuUcJUJJ5Pu6KKYY3KC/RbGpvFnw4AzzLbs3RjvOmTv126Ay2gYsW2yOLI9Ew8u2gCy3azsp2Y4L3WrHGkcwRO9k/FexzzuF1CO7GIxXJIBz4PtaC6scV1QeHvFIbc70OtFqvAgU38DZ6uh9PDz4vR7HjRH2bRd+j0BHBc10YqkhlZb9pjJApgsXLIyHrhZmQ9IYgVQhT+tMIptiJGWbjDKow0DzHs6B7n4scuk2JXCgx2SRTHKIp0Oh2SOsyLXR3anTatSqtntcfy6NLtWfix/IoWPzBg4W5W0fVEu17nUKtFtdoX2no9dINN8GdOxT+W2UCXW7Dwq2GsoZMfCXlepPLBCtWD/ftVtNZnW7O9dLXw5dK7v4UxjKX6Eq7loAoshVZlfXbDfR5G7/fd6v8G6g1gtsVStQ1JC/X5tPgvV6ui2WpBp0Mrz6lVkxZSFH2x7ex40Ye+iEZSge7Q6Vn4xf7ZlC5dppmmQqX3Z4xiWaHS249hp8fFFkEX3yVwhCMD7rFSSK3jojBFUU8Hh4vdTB06PUsbJu8yivEvsbTpgGRGNliBVKD90MYKBaDT6lCp9st1WCtnnNUd8x3vzSyzVFvQOdLhYLdLc2mJViKSrVbLW3LNfvjp9yiW2ku4xxy1xVphQN7TboffzfQSM124jyXuznM6e/rdZsO6CzdjrgUnO3AmcWsf8b+Pbjd0G/YsV5+/6kEvcIfioHdi0bemB39XqSEQGdktEs83+/kZSb0OrdCaaCZucMFjChtaYzFdR0KeQnn04u50oNul2m7TOXKE+vQ0zcXFfotnNty9VW9wtMLxUrvdD7/VolmtahxkUsxsx26V+YoRPpllvf3ip2Y1y5LPKH9Z4TNJ2BfzyS23mtUst3zr8Z3fenxDwz1fCOtU4lbDmMcgbPsxHi1cv47VajVLycMH82HktbwXRl7LLa/5fI9M6aNYZRWbm8JqYAbWqGKH57N+OHk/nB61mt96hzXL1jLL1pJ8r4Zyr9V66a5kWCVL8gk2AzbXwPIVn49GDWvOY1mShoEt93lthK1mNctqmWW1zFZquVmtZiu13Ko5Nl/B5kMe4vXZSmb5Sm4rtdwOz/s052tZP7yG39bAVjLvt1fGyW8pzXu+khsNH36tVrOVPDfLc6sd9mEXy25lSHnWDmdWW0niWfPHQymWv9W83+g2JI7efQj3ubdl2Qa3Glgty3x40T3kKbKaYavJvaxlmf8NFX6jZQdYthG6uqMt/XTgclx/9rB++GGM8jPJtRfCsK6YieNzW49vZLjnw/e3gRa9vu1GB+gmfamfBTrAGrCrf/nSfUtMd6bp7upSy2pAkrdD0N7ThmXgAL2ut7GW8s9AJ4PjIV3V3XAcYFcX5kKYtOFu33VSb/oEN0fleybJdw7tdpulX10iuyujXq/Tiaczn1/wVvlXDkKXNjGpdg10f6sLC6OTfvCQ/54Fbmv5wKpt3w1zcw5fakM3C1Z/u93vSqvCzcfa3HoEOhX41E945/rD/vvQwZAXYLXbpb3a9geh0RhnafW6DUNhHF7IqOSVXturdVObI7dCt+JbiAtHpmkBi60Kh6Kn2dmehc06sJr8Tte7/nh2NrQAmoNWeaWSWN9J66Q+y55Wh5vocl99qdcF+JWlwn8gy/rWfp5Du91rIy4BdLvUW7GrLpQhwPQ0dLu99msjh2oHvnJr19+TA0ua+TQpo2qDnbBl8+Mt9yvhs+X0Fy3z7fqcZ0PY0cpkEeMoxsObXLuCZbVso7thrPnzIz81jGV6linRWmtgM81wnGPcEfw06FnXh6M1Hay/lTy3ajP4SdMSP80knqPJfmHLwR4FOzrXt7Qb+7AGGDMYU9hU4n9+JtnyzO5oYpVT/rrUWs3TeGYwKv2yjv5O5di5rH98PrF4G2HLV3JjvV9+seWTtgAaNezUbmxlPuvlIf1kp3z+8xh2zceVxldrYPmp/jXVoz48q9W8lV2wzFeq2Mpc3//KHHa6Mpi3w3lmhys+nnNFKz+1zMFWQrnFMqvRbwHGbX1qMM3nQh7WZnz8vZZCPqKVUjIYY+lfdmEftzE/saRdHZ/zDBXnia4pflaHuK1jnMPmVgYFqFHB5h5mvGinYWz1XKxQ5vBdSPEPXg1ua/iKZ23wumwlM9Z8N8xMI/ypG1i16TfOJb7X8F1Tp7Es73fRTJ0L7jGu2N0Cth/sFFhjf1/0T+Ve+DmMkQXRb/htZh6by/2WzWDZUWxqFdvfxNZyL4rzRdGf81uj5q+LYnYu89vqbuzovkFhXAU7HLo55ldCJXs65G0lM2o+nafy/jVreWYrc4OiHwU/ViI1sP1JRRO3w/ux7Bw2c9hvaSU2bjs677e1mSHn89wsy+w82HrRPdlPK6Ao9qm4F8V+WCU50D0Uu6DS7SKohc+Vxo4SfeD1wAn8hLl3jPVbNtG/hJ+5lf6fu3FHsPhX2SC8Gz7n8FZ5lrjVwpZWGithW8Nb3g/TF8EoiIvhOsOPNZzGtwbSMAwvwA/7fvjdc9jMKWyqgW8JnAppamJUwv4pv81FP3kSZxBhprA8iORMYuXOrWDc4QU+tdanqlh1ZtAi7eXbvMW/e26wZTCw7fNjGCtgecNvUazuaGD7G9ip0F+9kueWr+QDfe29yv20L5ei6J+a8pVII1rga/RaOFH0j4I9PESgV+awud0+f/GzMoc9Ojdc7NdmvBjH1svKEH9pRTAg0KEPvmjJn8t8q+sOsHNTg9cOE/hhW8xnfjiz/HC2YTzgQpDoX7zgZ8BTwMuBF+C7cl8x0r9Ef+uf1PottgLWBs/NrWCN0IWC0RfuUa2H6JbhRTYe5/gB4TT8KPrpJ4prLYkzDXslhHWavngfTfw0wvEaPWEvhj91FJva76+fWsRXBKno76fX2pjCh7cviWNlzgvjYiIisZupuhtbnErCivlY8eFUpvpWfgOsmYr+PDYXw9uPZYlQzh/1wn86Eco7GtjikO62Rs23THpin/vr9jd8i6OxO/g87FsPaQviHINdSqmwngY7GoT8VOYrkVTMRwnv+SDUR6f6Fv0K2PLUcP+pRV8MZ6T1vkka0vw1wOYbfutVannuB7m3QBT7WO5XmviPE/1LPZD7SuCkmX0JwDn3YeAu4AuXOB07H6M/mGvhe5LB3Q79AcvZcBwHE09Cb8TP9QcPexxK9uMgYhpnG19Nd0Oabg/HAMeAxXDNA+EcQC18N/vpmAnj8+kURxx+EPdb9CcSV4A9iZ9qyEPMj7GBT/+c93PgNXD+MejNBYgjgHE2ZDPE+XPQOgEc9uF3Tnr3b6eBhrFEO++vq/wCdGZDGGv4duuP+JdTxGFLhz+eyYBbYe9/gWN1f27uP/nv6VvhtpPQCgPH/+ShkLQD8Ok74eyLYPfaYP7e9JDPd2d3KKK2L/LpWB5daIRyKi4scuBheP8vwm2PwXQYS10P57rAnvPw5SX4Mn7sf/54/9r0p1j8GR5vwHcOgj3mL9wTyqroN2S/NwE5HjdjGqZg+jwj2eznf6ABjwHtEHC94b8fes3lW5xwJ3KpRf8G4Onk+BngR1MPzrm3AW8D4B9csnTtHOrAQzDicYPNiZPSoyDeGMJbx0/TWQnuB8OsHT/xxM9YWccL/3XBz2ohrOPADNAI6fxKOI7q/Thw24h0reNVyGDPB+Hkz+IF89rETzJbBej/OuNsmvcOCTeK4rGQv91w8EHglyCdPDXXBir9OgrCpKYWnK9A42f9+cWQl/uB+eCvAVSW4GczyLoFMe3CVMdPduoGv9Cv6/78Glj/Chyo9UUo3lsL3426/77vvnBtwwtc80BSQQa/Z2Z8wps1qHTg4Pu8gC+FMH75IZ+WxgEvgoRwINQJIQ/xlsYijj+bKuNtjaLwngeefhO8+iFoHehnbw34EQbq+gEaIc6D+HKbArKC4K8A9+Fva8her4JI6/uYpgxvNzxciGu926UF3LeFp3gvdGmQK4FLLfrDKusBe83M7sf/55hemLbnazrljuXQELdJLfz039XGW8Vt+uYcwC/QE9+BcHcF9zq+khgWf1Sy5fAdWw3RdJtJ3FMLv8DxKmSfoi/4seXwnUIeOnizs4K30F9DX1mngZ8Iaf0xaLzLX1I/7cVk7hAcenuo2B6HQzPAP4Q7V+F8i8Fn/tvwpmPeQo0TTdMGBjns6cBvzUAtXJefgu4qnLnOJy/WdRueVa7S+5ctB0U9E1od+aoX7tlQuf7pmyA/k1QOwFIN1q6FA0369yzzYl85Da0q/PkbYH06+Amkf+yVKjiDz77aTxc9MwPXnB1uVY8T+Bj9QCN0CrrTcLwOrbmQ5eP+1n6AgZm//ee/QjlVg9sKDH12eQ8M/PtjX9n5qX4FkYpH8wCsZ76yO3ttxkMHuuzCN+IO0P9Jlp5R/T7Pxwb8GPCJ5PidwDtH+i9Tn37sA28w2C8/6aeG7+s+R3+6YzoYGQc0Dd9/HmbyDO1fb4StxvAZPYfD+XkG+8vB96HXRoSbfkIeGzVs7qg/nhrSRz8w3XIffhA2pi9O86xhcxVsrukHZ2O8c0f9AOXcTL9/eC73feJUC+kGYyr044c4Ho798rvp9dPvb4QB4vCZawymMQvbqbBlR8MsGvNjKNXk4bfDhcHSR6t+i59aw/f3zx/19ywrjtec9mk9N4Utz/X7sWsNvx8/ldM+jHTG1vqYwdKtDJ4aYcZTzedlOclPg/CwWcH/o+m1YTsd/K4N8Vvs/0/TXhxEXpsJU1Cbg9etxIe+SgI7qE//KHCLc+5m4G+Ae4B/fonTsHPZg7eiN7srwzpWm8n+Av2umQX6VnwkWIsU+8Rjd0i0goeZRukzV+tAsbs0xtOAkY20dfp962twqA4HzsBtGZz8FJyphngOFK7bBZxN3Of6p47PAxWYScrl+AI0bg/xfTYJ5wxMvQ/OF8M/77sbDuG7G2J02W3Q/SwT0QC6MzAbuok+9XNw8Hd9Wo5XYfcqZOtQOQKV44MWbjsHl7TKWsnJrAv7j8DD9X4eGm+E6le8xT5/HD6QjNGkt7bzFOw6C3nS3VXsSkl/TvHU5ouNB3/rML3qLfzj+K6dLv42PYq3zDnvb+ltDPYAZvhyjj/5FnBrcvyvwvm0IXtkEfY9Ck99/8a0zJzxFn99wrSXklG1wfO1AW8E/ho/i+dXxvotk6UfP5PMlS9+ijNt0jDuYLTFvYa3+IvX18I2LNyiZZ8n26h4CmmcWcP2NbHGfN8SO5r5h4ww/CycZhLHfgZn6uzDT/eM0y+jv93J/mJSDlVv7cdzUzMjplbG8OKn4aeGNsAaycya0/jZNXHK6+FgzeZgqzPYw4uJtVvzDy7NH+37n2+EFkKwYh/FW+ucwirJjJ38lJ9ZdEfD7w88eLWGPbx/cNrmuSw8dxC2uYafdz+X91sAp4dM2Uyt6/Uxs26KW3zwKj7jUHwIq2i1rwc30jQWWgLn8WmMLYTT9PN0NPNbfso/WxBbJOn1NQoPemWZf2BrG+bsX0kwxtKfumS1S8DM/szMfsDMvt/M/uOljn/HULSyIwfZaD1H/3ErUrT654K/DvAZhq9fYHjTKSu4peE7NhYsp9AAAA0WSURBVMaXmqZ5ON58MckB9nzRz9RwTxZOOGAd5r4Pdt1Jf8bNZ4E30TffYrq/ErbAzOn+/lQHprr4MmgxMFXo/Jm+NdvzD97EHzKmUrR4HaOHWR7b4/PWCtFm67Dr23DNd/p+/vsBOBRaGdGw/4vbfDzzSeuoPQuPLnjrvh0GC2bWYO4YUIE7jviF1Vpz/WzGhtiBKT+TZe818D9v8kV36AB0k3KIUaWToc6mnfAM/7lFN4dvCX3qANx3N3x2N6yOaB4YPn8VehOlAN8yOBjSXW/A8Tk/RhBXvT4+12/0dSp+a98Nr572fffH5/rXG77sB+iWbExwEkbVBjthq8xXNiw6hTGwYNoVu0zDVpdaiH3wxf7+YeGMe2p2s3Ss4efJD/MX/dboW/bF69eT+NMlGmoYR7GZ06EPP3weDf3avWsfDVuMI1ZFGb3+/Jl5BscNatjMPt8HX8M/+DSXtnaO4lsPM/4BL6LVP+XdKmG/18/foNdPnzWwyhx2tNq3OqM1Ga3VHKyyz/edF5dbaN7hv6tHsVMzG58+PYefF7+v0u+D3r3qH/YqfvY3QxkVHqA7Ou8tYGr98spCa6bSCJZ4YgE3GN5/fj6x9M8zum9/mNu5rJ/3tRlvia/M9ZebSB/WWgsW/Fz4DTVqvmzmlrFqIxzn9J4wvgP/3ES08OOzJdl+fEs25OlUhh2eGSxfy7LBp4BLYvGzg/r0t8Re9lKhMvDqveLa7nFKVfpmqTjjJyMbWEq5+PapYTOD4tr6GRmLLA4sg1yjNvHibmn8Kb0llV0+ckG2DUTzLbYCjibnhpmcZ9kwT27cC2V6dPFzzs9CVs1YrCT5Pw/Z2cSt2ob3AXfgTbfIY/3pj3HKoDczgb3e4D6eWL2da31/M0Ce5bRvD2XSDB5eDfxFSFuwws9EazT6qcOZa+G23dA57b0+flOSpr3AU3D+O2HK4x789NIw/fH7d8P0Z4A1OHGgP0f/WqB6AFZn4OwZv4gZBp0zgxbwHNDqQtf1Z9A06t7v3Q/C6nV+lk7xsQcDjkzBm6qwZLD3BHAGbl8JQzJpS2sa3nMXHFhl6BzIs68C/jdwAKrH4P1n4BeBqQP9YlrP4DVdP3Omgm8h3PaYj+Lx23yffHZ+MG/H5mDheD+94Gf0fioUY5ytdCbM3DrQ9LOFKh2YO+7dz14D1yYtjAqwVgnjFx1gHVpn4NCPQ3ba33Lw6Wkc6M8groaZUqsAS2Az/TAPALtvhduPFxq2lQud+3wVM6o22AlbZb5iZlt7FDptDQy7ppZ8Yosh7mP9J+/iUrbxXG750HDSFkdxyea4BHFMRzHOor9iqyW69Z4ODFZtbvmgRT3mkyefYefMrLduTVZLljZeyf1ywIUlfc3MLwQWH/Nv0B8bWOtbXY3cr4NSs5pfbnkl933sNayy5i3+qbRFMKTFklvuZwhVEos/Z3DM4jyWrWVmZrZSy2056y91MHXKW9+1RtIq2Be2EF60NuMywbnXEssyv8jbSs0v0zyfZZaFBeMa+Fk9U/iF1+JMqd79X8ea+/szgQ7Pef/N1DIOrQaCRVqc4RL75Oeafhvaolvx1u7KfGZZXGup5lsu+5IOwSZ+LZ9arWb7w3LW6Xo4Rcv93BR2uOrX8zmaDVr+0c8Kfo2gtLXwaBVb3uet6ziLZm4F2394Y8sgjgPEtZHyQpqGpS0to/gbo+ZnbGX76S/zHJdfSC36klj4Ea5USz+ylQcjim/H2iysWWb9u1ILVvAqq71WRHxpyrAHNSpU6NDptQqgb80XX7036jV/o96kVWwJZM2sn7Y767QOtaDafxF5mzY5+YZ4i2+yysnp0On5y8jonh/Mf/VgdeCNUgPUIduT0X1v13fOpg8ZzUH9ONCGXz2yhC3AgffS78R9EDozbGih5Gcy2pXuhvQ1l33cs/VZ2q02eTUfeKNVx3VYrPjXNlab/t4fWZjmoV+Gdy179+Z9UI+vHqzDkU6/tXa8MB6xWqv17k5vueJDLb8ScXha60AD8tmcaxf8i2Fqq/0XwQMcyY5w15EKzMJDx9u8K8s4kkG326URwj1OMsMky/zLZIDD0WI+AxyALPetRVtdHfjdtGjROdihMl2hurzKYr3OkUNHqDR9vE9Bb1niLPO/m/hWs0P1FlkyC8tVq37p5HYbsozHKxXe9HvQPAgd2qy24Kk9cOdyzmq9CktLHATew+BwjjM/ywigEmfRNP3zEikGNO+rAf6+tH4JqjlkcbnnNMxQPlQqVJPz0zOwHif613MqQLO66kt1k1dMlp0dLfrxVYFw6Z6I28obk9Kun/i2qlTAR6V5s6f9hr2OEGA9DPvVqUOzXykMewXjVuJbr6xTn6+HP1D/9YBt2iyxtOG6arVK61CL9nrbD6TGqCsMDkIPa1kX+ziAzGVUKxvft1tMZ17NWW32xW9UBV8JTfr+9fV+mOFpzPRNVj2/QUQGY4VqsdKc9RVPcz2ERf9l8in1ap13hbXhF6vV3luuHs8y3r242H9+rdnsrVH/6iBYA+cKxNc1xpe6x7wsVkMltzqYx8UkjHQt/oH155O3aFWbTV+yTXqVxDxVVsO19WkvG68OAhwf8q7Gx51ri/03ctXgYAuasy2/Hj/gKpVenM2D1X78MS1h7XzyvPfGrF768L2d7+p06L6nS47/TfQzuLG8xrqXkB0t+peC4su+h4n2pa54iuMVm8V7MS/zHtUKGEcUotnZWTp7OlSWvcje3QTq8Kd5G6pwX9NLV61Ov1VS6Y/NdLptFh/Lg4U2Pr/pH3vT8mgWKoPmxd259HeQvupws/QUBbter3N3eCdws9nsVzyFtAbPY9MBG1+jueVcpukbI4oHi629xVBBx1caBku8Z2EnIt67rlpPXs3YHB9/pdJ/eUrxfL3u6+bQapFNv3Wc7/7ZmbhZ93e0+dwliez28PtZGWLq/yB7AfgiJ4ae28UuznJ26Pmt8RL8kmObxzvJ+UmIYcwEu/xMUPwvcmKC8Peyl128CNL8/0DFL1vz1x2OjfR3O1WmyDhDZ4L0D5bL5eLiy3tv+L7Y30nkVuDcNoa3JXaFNt7ZMNXgB0P+vniZ0pOwM34vl5fvM7Prh53Y0aJfNpxzy2Y25mV95UTlMhyVy3BULuO55A9nCSGEuHxI9IUQokRI9HcW91/uBOxQVC7DUbkMR+UyBvXpCyFEiZClL4QQJUKiL4QQJUKiv804537SOfd559x559xC4dw7nXMnnXMnnHOvS9xfH9xOOufekbjf7Jx7xDn3pHPuI865FwT3F4bjk+H8TZvFcSUyqlyudJxz/8M5903n3OOJ24udc58M9/qTzrnrgrtzzr0/lMHnnHP7kmvuDf6fdM7dm7jPO+ceC9e83znnxsWxU3DOvcw513DOPRH+Q28P7qUvm21l1KI82i5sA34I/5BKE1hI3F+BX3LlhcDN+OVRsrA9BbwceEHw84pwzUeBe8L+7wL/Ouz/G+B3w/49wEfGxXG5y+QCy3FkuVzpG35t0n3A44nbfwbeEfbfAfxG2H8j8H/xi1e8CngkuL8Y+FL4vi7sXxfO/RX+1aQuXPuGcXHslA14KbAv7F+Lf9nSK1Q221zOlzsBV+s2RPQH3gcMfCL8+Ia+Nzj8KL8FTAf3nr94bdifDv7cqDgud1lcYPlt6X3KV9oG3FQQ/RPAS8P+S4ETYf8DwE8V/QE/BXwgcf9AcHsp8MXEvedvVBw7dQMeBP6xymZ7N3XvXDpuAJ5Ojp8JbqPcvxtYNbP1gvtAWOF8O/gfFdaVyNWUl0n4XjP7OkD4/p7gvtXfzQ1hv+g+Lo4dR+iyvB14BJXNtlL6BdcuBOfcp4C/N+TUr5jZg6MuG+JmDB9XsTH+x4U17porjaspLxfDVu/1FV9uzrkK8MfAQTM7Hbrdh3od4nZVl812ING/AMzsNRdw2TPAy5LjG4Gvhf1h7t8CZp1z08GaT/3HsJ5xzk3j3yb73CZxXGlcTXmZhG84515qZl93zr0U+GZwH1UOz5CuGe3dm8H9xiH+x8WxY3DOfRde8P/QzP4kOKtsthF171w6Pg7cE2be3Azcgh9UOgrcEmbqvAA/MPtx852LDeDN4fp78X2cMaw4I+HNwGeC/1FxXIkMLZfLnKbnk/SeFu/1W8JMlVcB7dD98Angtc6568JMk9fix0C+Dqw5514VZqa8heG/mzSOHUFI7wPAE2b2m8mp0pfNtnK5BxWutg34Z3iL4m+BbzA4GPkr+BkpJwizBoL7G/EzFZ7CdxFF95fjRfsk8L+AFwb3a8LxyXD+5ZvFcSVuo8rlSt+APwK+jl8W+RngrfgxmU8DT4bvFwe/DvhvoQweY3BywM+E38BJ4KcT9wX8W4CfAv4r/Sfvh8axUzZgEd/d8jn8cs2t8Bsofdls56ZlGIQQokSoe0cIIUqERF8IIUqERF8IIUqERF8IIUqERF8IIUqERF8IIUqERF8IIUrE/wf6mGTGRBJ/zQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "D_clus = DEC.predict(D_train, verbose=0)\n",
    "D_clus = D_clus.argmax(1)\n",
    "plot_cluster(D_train, D_clus, n_clusters)\n",
    "\n",
    "a0 = np.sum(np.where(D_clus==0, 1, 0))\n",
    "a1 = np.sum(np.where(D_clus==1, 1, 0))\n",
    "a2 = np.sum(np.where(D_clus==2, 1, 0))\n",
    "print(a0, a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24170403587443945\n",
      "500.37467443555295\n",
      "2457.784784870404\n"
     ]
    }
   ],
   "source": [
    "REG = REGModel()\n",
    "REG.load_weights('./model/REGweights')\n",
    "\n",
    "X = np.vstack([X_train, X_test])\n",
    "np.random.shuffle(X)\n",
    "y = REG.predict(X).reshape(-1)\n",
    "\n",
    "total_survival = 0\n",
    "total_benefit = 0\n",
    "death_count = 0\n",
    "\n",
    "S = np.copy(SS)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    \n",
    "    if len(y) < 1:\n",
    "        break\n",
    "    \n",
    "    total_benefit += max(0, y[0] - S[0])\n",
    "    total_survival += y[0]\n",
    "    \n",
    "    y = y[1:]\n",
    "    S = S[1:]\n",
    "    \n",
    "    total_survival += len(S)\n",
    "    \n",
    "    S -= 1\n",
    "    death_count += np.sum(np.where(S==0, 1, 0))\n",
    "    non_zero = np.nonzero(S)\n",
    "    y = y[non_zero]\n",
    "    S = S[non_zero]\n",
    "    \n",
    "\n",
    "# a = SS - np.arange(len(SS))\n",
    "# death_count = np.sum(np.where(a<=0, 1, 0))\n",
    "death_rate = death_count / len(X)\n",
    "\n",
    "# b = np.where(a>0, a, y)\n",
    "# b = y - b\n",
    "# benefits = np.sum(np.where(b>0, b, 0))\n",
    "mean_bene = total_benefit / (len(X) - death_count)\n",
    "\n",
    "mean_survival = total_survival / len(X)\n",
    "\n",
    "print(death_rate)\n",
    "print(mean_bene)\n",
    "print(mean_survival)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078\n",
      "death rate:  0.24170403587443945\n",
      "mean benefit:  460.3988763977267\n",
      "mean survival:  2431.4856940059917\n"
     ]
    }
   ],
   "source": [
    "total_benefit = 0\n",
    "total_survival = 0\n",
    "death_count = 0\n",
    "\n",
    "REG = REGModel()\n",
    "REG.load_weights('./model/REGweights')\n",
    "\n",
    "R = np.copy(RR)\n",
    "D = np.copy(DD)\n",
    "S = np.copy(SS)\n",
    "\n",
    "for i in range(len(D)):\n",
    "    d = D[i]\n",
    "    \n",
    "    if len(S) < 1:\n",
    "        break\n",
    "        \n",
    "    r = R[0]\n",
    "    x = np.hstack([d, r]).reshape((1, -1))\n",
    "    y = REG.predict(x)[0][0]\n",
    "    total_survival += y\n",
    "    benefit = y - S[0]\n",
    "    benefit = max(0, benefit)\n",
    "    total_benefit += benefit\n",
    "    \n",
    "    R = R[1:]\n",
    "    S = S[1:]\n",
    "    \n",
    "    total_survival += len(S)\n",
    "    \n",
    "    S -= 1\n",
    "    death_count += np.sum(np.where(S==0, 1, 0))\n",
    "    non_zero = np.nonzero(S)\n",
    "    R = R[non_zero]\n",
    "    S = S[non_zero]\n",
    "    \n",
    "mean_benefit = total_benefit / (len(D) - death_count)\n",
    "death_rate = death_count / len(D)\n",
    "mean_survival = total_survival / len(D)\n",
    "\n",
    "print(death_count)\n",
    "print('death rate: ', death_rate)\n",
    "print('mean benefit: ', mean_benefit)\n",
    "print('mean survival: ', mean_survival)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urgent First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "mean benefit:  638.0567815960006\n",
      "death rate:  0.15358744394618834\n",
      "mean survival:  2780.731799676493\n"
     ]
    }
   ],
   "source": [
    "total_benefit = 0\n",
    "death_count = 0\n",
    "total_survival = 0\n",
    "\n",
    "REG = REGModel()\n",
    "REG.load_weights('./model/REGweights')\n",
    "\n",
    "R = np.copy(RR)\n",
    "D = np.copy(DD)\n",
    "S = np.copy(SS)\n",
    "\n",
    "for i in range(len(D)):\n",
    "    d = D[i]\n",
    "    \n",
    "    if len(S) < 1:\n",
    "        break\n",
    "    \n",
    "    idx = np.argmin(S[:500])\n",
    "    r = R[idx]\n",
    "    \n",
    "    x = np.hstack([d, r]).reshape((1, -1))\n",
    "    y = REG.predict(x)[0][0]\n",
    "    total_survival += y\n",
    "    benefit = y - S[idx]\n",
    "    benefit = max(0, benefit)\n",
    "    total_benefit += benefit\n",
    "    \n",
    "    R = np.delete(R, idx, axis=0)\n",
    "    S = np.delete(S, idx)\n",
    "    \n",
    "    total_survival += len(S)\n",
    "    \n",
    "    S -= 1\n",
    "    death_count += np.sum(np.where(S==0, 1, 0))\n",
    "    non_zero = np.nonzero(S)\n",
    "    R = R[non_zero]\n",
    "    S = S[non_zero]\n",
    "    \n",
    "mean_benefit = total_benefit / (len(D) - death_count)\n",
    "death_rate = death_count / len(D)\n",
    "mean_survival = total_survival / len(D)\n",
    "\n",
    "print(death_count)\n",
    "print('mean benefit: ', mean_benefit)\n",
    "print('death rate: ', death_rate)\n",
    "print('mean survival: ', mean_survival)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Predicted Outcome First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1358\n",
      "mean benefit:  388.7056117841769\n",
      "death rate:  0.30448430493273543\n",
      "mean survival:  2255.1112850394484\n"
     ]
    }
   ],
   "source": [
    "total_benefit = 0\n",
    "death_count = 0\n",
    "total_survival = 0\n",
    "\n",
    "REG = REGModel()\n",
    "REG.load_weights('./model/REGweights')\n",
    "BM, TBM = BranchRModel(rdims, bdims)\n",
    "TBM.load_weights('./model/BranchRCheckpoint')\n",
    "# BM, RBM = BMCross(rdims, bdims)\n",
    "# CLS = CLUS(cdims)\n",
    "# RBM.load_weights('./model/AlterCheckpoint')\n",
    "# CLS.load_weights('./model/CLSCheckpoint')\n",
    "\n",
    "R = np.copy(RR)\n",
    "D = np.copy(DD)\n",
    "S = np.copy(SS)\n",
    "\n",
    "for i in range(len(D)):\n",
    "    d = D[i]\n",
    "    \n",
    "    if len(S) < 1:\n",
    "        break\n",
    "    \n",
    "    rs = R[:500]\n",
    "    ds = np.array([d])[np.zeros(len(rs)).astype('int')]\n",
    "        \n",
    "    ys = BM.predict(rs)\n",
    "    ts = DEC.predict(ds)\n",
    "    y = np.sum(ys * ts, axis=-1)\n",
    "    y = y.reshape(-1)\n",
    "    idx = np.argmax(y)\n",
    "    \n",
    "    r = R[idx]\n",
    "    s = S[idx]\n",
    "    x = np.hstack([d, r]).reshape(1, -1)\n",
    "    y = REG.predict(x)[0][0]\n",
    "    total_survival += y\n",
    "    benefit = max(0, y - s)\n",
    "    total_benefit += benefit\n",
    "    \n",
    "    R = np.delete(R, idx, axis=0)\n",
    "    S = np.delete(S, idx)\n",
    "    \n",
    "    total_survival += len(S)\n",
    "    \n",
    "    S -= 1\n",
    "    death_count += np.sum(np.where(S==0, 1, 0))\n",
    "    non_zero = np.nonzero(S)\n",
    "    R = R[non_zero]\n",
    "    S = S[non_zero]\n",
    "    \n",
    "mean_benefit = total_benefit / (len(D) - death_count)\n",
    "death_rate = death_count / len(D)\n",
    "mean_survival = total_survival / len(D)\n",
    "\n",
    "print(death_count)\n",
    "print('mean benefit: ', mean_benefit)\n",
    "print('death rate: ', death_rate)\n",
    "print('mean survival: ', mean_survival)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefit First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695\n",
      "mean benefit:  633.6750256157175\n",
      "death rate:  0.15582959641255606\n",
      "mean survival:  2770.571443284039\n"
     ]
    }
   ],
   "source": [
    "total_benefit = 0\n",
    "death_count = 0\n",
    "total_survival = 0\n",
    "\n",
    "REG = REGModel()\n",
    "REG.load_weights('./model/REGweights')\n",
    "BM, TBM = BranchRModel(rdims, bdims)\n",
    "TBM.load_weights('./model/BranchRCheckpoint')\n",
    "# BM, RBM = BMCross(rdims, bdims)\n",
    "# CLS = CLUS(cdims)\n",
    "# RBM.load_weights('./model/AlterCheckpoint')\n",
    "# CLS.load_weights('./model/CLSCheckpoint')\n",
    "\n",
    "R = np.copy(RR)\n",
    "D = np.copy(DD)\n",
    "S = np.copy(SS)\n",
    "\n",
    "for i in range(len(D)):\n",
    "    d = D[i]\n",
    "    \n",
    "    if len(S) < 1:\n",
    "        break\n",
    "    \n",
    "    rs = R[:500]\n",
    "    ds = np.array([d])[np.zeros(len(rs)).astype('int')]\n",
    "        \n",
    "    ys = BM.predict(rs)\n",
    "    ts = DEC.predict(ds)\n",
    "    y = np.sum(ys * ts, axis=-1)\n",
    "    y = y.reshape(-1)\n",
    "    potential_benefits = y - S[:500]\n",
    "    idx = np.argmax(potential_benefits)\n",
    "    \n",
    "    r = R[idx]\n",
    "    s = S[idx]\n",
    "    x = np.hstack([d, r]).reshape(1, -1)\n",
    "    y = REG.predict(x)[0][0]\n",
    "    total_survival += y\n",
    "    benefit = max(0, y - s)\n",
    "    total_benefit += benefit\n",
    "    \n",
    "    R = np.delete(R, idx, axis=0)\n",
    "    S = np.delete(S, idx)\n",
    "    \n",
    "    total_survival += len(S)\n",
    "    \n",
    "    S -= 1\n",
    "    death_count += np.sum(np.where(S==0, 1, 0))\n",
    "    non_zero = np.nonzero(S)\n",
    "    R = R[non_zero]\n",
    "    S = S[non_zero]\n",
    "    \n",
    "mean_benefit = total_benefit / (len(D) - death_count)\n",
    "death_rate = death_count / len(D)\n",
    "mean_survival = total_survival / len(D)\n",
    "\n",
    "print(death_count)\n",
    "print('mean benefit: ', mean_benefit)\n",
    "print('death rate: ', death_rate)\n",
    "print('mean survival: ', mean_survival)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Donor Type First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\key-s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 2s 429us/step - loss: 2456825.9179 - val_loss: 2366309.7769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366309.77691, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 2454019.9045 - val_loss: 2354827.0818\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366309.77691 to 2354827.08184, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 2415947.4062 - val_loss: 2195493.9036\n",
      "\n",
      "Epoch 00003: val_loss improved from 2354827.08184 to 2195493.90359, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 2113013.3596 - val_loss: 1493673.4697\n",
      "\n",
      "Epoch 00004: val_loss improved from 2195493.90359 to 1493673.46973, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 1371722.1981 - val_loss: 1617342.0151\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1493673.46973\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 1147726.8446 - val_loss: 1138891.6883\n",
      "\n",
      "Epoch 00001: val_loss improved from 1493673.46973 to 1138891.68834, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 1054384.6262 - val_loss: 1078571.1127\n",
      "\n",
      "Epoch 00002: val_loss improved from 1138891.68834 to 1078571.11267, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 997972.2816 - val_loss: 1028773.8201\n",
      "\n",
      "Epoch 00003: val_loss improved from 1078571.11267 to 1028773.82007, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 974722.8266 - val_loss: 1015663.1469\n",
      "\n",
      "Epoch 00004: val_loss improved from 1028773.82007 to 1015663.14686, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 950083.0858 - val_loss: 1008824.8935\n",
      "\n",
      "Epoch 00005: val_loss improved from 1015663.14686 to 1008824.89350, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 935628.2917 - val_loss: 1021731.3430\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 922750.2425 - val_loss: 1026117.0045\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 907512.1726 - val_loss: 1023739.9630\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 887308.2797 - val_loss: 1029257.8907\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 871502.9190 - val_loss: 1030328.5493\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 860796.7963 - val_loss: 1030091.5488\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 836716.2007 - val_loss: 1034645.1861\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 833496.5021 - val_loss: 1038988.4271\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 818040.0076 - val_loss: 1036605.2276\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 789604.8946 - val_loss: 1042874.2601\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 784124.0261 - val_loss: 1033435.2141\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 773541.3164 - val_loss: 1049378.6054\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 755980.0899 - val_loss: 1056818.7887\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 748752.9316 - val_loss: 1064596.4445\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 725323.2724 - val_loss: 1070140.7360\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 708230.3588 - val_loss: 1108442.5011\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 697536.9535 - val_loss: 1095106.1104\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 685153.3633 - val_loss: 1091714.7040\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 675501.2229 - val_loss: 1114746.3117\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 655915.0303 - val_loss: 1127888.5729\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 647042.3745 - val_loss: 1148208.9126\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 631024.3297 - val_loss: 1200650.3705\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 607443.0928 - val_loss: 1216099.0174\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 603266.6114 - val_loss: 1197091.1037\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 588286.0671 - val_loss: 1323311.2298\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 589970.7493 - val_loss: 1280682.2147\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 54us/step - loss: 566211.7152 - val_loss: 1354339.3851\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 550170.2237 - val_loss: 1356803.4955\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 544787.1951 - val_loss: 1357168.9798\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 539049.0140 - val_loss: 1342865.7450\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 535516.9931 - val_loss: 1440441.0555\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 500345.0072 - val_loss: 1377767.3318\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 513894.8229 - val_loss: 1418390.7904\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 495984.4579 - val_loss: 1381708.3156\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 52us/step - loss: 485283.0116 - val_loss: 1387347.5135\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 475277.6308 - val_loss: 1432053.0902\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008824.89350\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 463076.5230 - val_loss: 1449909.2186\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008824.89350\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 452793.9756 - val_loss: 1545937.6474\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008824.89350\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 436136.0695 - val_loss: 1570517.8133\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008824.89350\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 456271.3351 - val_loss: 1630143.9860\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008824.89350\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 2s 502us/step - loss: 2456903.7896 - val_loss: 2366583.4955\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366583.49552, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 2455449.1925 - val_loss: 2360106.5751\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366583.49552 to 2360106.57511, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 2430736.0386 - val_loss: 2243612.9596\n",
      "\n",
      "Epoch 00003: val_loss improved from 2360106.57511 to 2243612.95964, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 2188008.8498 - val_loss: 1602355.7248\n",
      "\n",
      "Epoch 00004: val_loss improved from 2243612.95964 to 1602355.72478, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 1435922.3094 - val_loss: 1613271.7960\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1602355.72478\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 1155015.1754 - val_loss: 1143246.9159\n",
      "\n",
      "Epoch 00001: val_loss improved from 1602355.72478 to 1143246.91592, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 1069339.9906 - val_loss: 1093001.5067\n",
      "\n",
      "Epoch 00002: val_loss improved from 1143246.91592 to 1093001.50673, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 1005876.0220 - val_loss: 1039460.4938\n",
      "\n",
      "Epoch 00003: val_loss improved from 1093001.50673 to 1039460.49383, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 973404.8019 - val_loss: 1013629.4753\n",
      "\n",
      "Epoch 00004: val_loss improved from 1039460.49383 to 1013629.47534, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 952650.3563 - val_loss: 1017902.6244\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 938446.3165 - val_loss: 1024438.8447\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 917703.6125 - val_loss: 1028667.9692\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 906585.4019 - val_loss: 1028572.3380\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 893373.8050 - val_loss: 1034541.3318\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 877648.7882 - val_loss: 1018304.5185\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 853524.7117 - val_loss: 1024918.6923\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 840746.1720 - val_loss: 1040818.7971\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 832779.6903 - val_loss: 1032689.1698\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 808060.4887 - val_loss: 1036766.2640\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 792148.4292 - val_loss: 1047967.2898\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 777308.6125 - val_loss: 1067479.1939\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 766173.1867 - val_loss: 1062740.7242\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 746325.7393 - val_loss: 1105138.3806\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 741296.5672 - val_loss: 1106430.7293\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 734515.3607 - val_loss: 1105523.0252\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 711219.3395 - val_loss: 1118142.8576\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 689371.7270 - val_loss: 1136498.7298\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 677745.3839 - val_loss: 1158151.0925\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 671779.4831 - val_loss: 1166974.1878\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 55us/step - loss: 666868.5412 - val_loss: 1235242.1244\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 646965.6288 - val_loss: 1271284.7623\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 624320.6634 - val_loss: 1259087.2371\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 625478.0756 - val_loss: 1255568.9809\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 597965.5456 - val_loss: 1212544.5555\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 596335.2330 - val_loss: 1245193.0639\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 584170.3076 - val_loss: 1323758.2186\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 53us/step - loss: 576667.9646 - val_loss: 1337466.1015\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 577503.3518 - val_loss: 1379236.4490\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 563608.7226 - val_loss: 1308074.4815\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 546866.5147 - val_loss: 1258580.9978\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 543785.6099 - val_loss: 1322885.2612\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 540449.9525 - val_loss: 1304623.1872\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 541141.7498 - val_loss: 1367628.5432\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 509995.4393 - val_loss: 1356406.5919\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 55us/step - loss: 499645.1701 - val_loss: 1412281.6721\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 54us/step - loss: 476085.0035 - val_loss: 1498251.8610\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1013629.47534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 479409.5844 - val_loss: 1533905.0510\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1013629.47534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 472301.3355 - val_loss: 1515293.9815\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1013629.47534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 458575.2682 - val_loss: 1609346.9496\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1013629.47534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 443758.6793 - val_loss: 1638141.1099\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1013629.47534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 2s 564us/step - loss: 2456843.2688 - val_loss: 2366362.4036\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366362.40359, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 2454363.4497 - val_loss: 2356418.6760\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366362.40359 to 2356418.67601, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 2419331.5813 - val_loss: 2229316.5897\n",
      "\n",
      "Epoch 00003: val_loss improved from 2356418.67601 to 2229316.58969, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 2145097.2342 - val_loss: 1567508.0706\n",
      "\n",
      "Epoch 00004: val_loss improved from 2229316.58969 to 1567508.07063, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 1403079.3997 - val_loss: 1770108.9176\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1567508.07063\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 1159183.1127 - val_loss: 1391694.9142\n",
      "\n",
      "Epoch 00001: val_loss improved from 1567508.07063 to 1391694.91424, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 1091897.8585 - val_loss: 1236061.4961\n",
      "\n",
      "Epoch 00002: val_loss improved from 1391694.91424 to 1236061.49608, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 1041764.8393 - val_loss: 1185311.1648\n",
      "\n",
      "Epoch 00003: val_loss improved from 1236061.49608 to 1185311.16480, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 994775.9888 - val_loss: 1101950.7276\n",
      "\n",
      "Epoch 00004: val_loss improved from 1185311.16480 to 1101950.72758, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 970770.3125 - val_loss: 1082994.2124\n",
      "\n",
      "Epoch 00005: val_loss improved from 1101950.72758 to 1082994.21244, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 962631.4325 - val_loss: 1062689.2601\n",
      "\n",
      "Epoch 00001: val_loss improved from 1082994.21244 to 1062689.26009, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 951665.2406 - val_loss: 1051034.8335\n",
      "\n",
      "Epoch 00002: val_loss improved from 1062689.26009 to 1051034.83352, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 943270.9551 - val_loss: 1039079.6698\n",
      "\n",
      "Epoch 00003: val_loss improved from 1051034.83352 to 1039079.66984, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 927765.3664 - val_loss: 1036254.5112\n",
      "\n",
      "Epoch 00004: val_loss improved from 1039079.66984 to 1036254.51121, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 908468.5802 - val_loss: 1023529.0555\n",
      "\n",
      "Epoch 00005: val_loss improved from 1036254.51121 to 1023529.05549, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 894806.6885 - val_loss: 1032302.2360\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1023529.05549\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 888030.6935 - val_loss: 1024746.3901\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1023529.05549\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 863976.0267 - val_loss: 1041876.5387\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1023529.05549\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 861040.9118 - val_loss: 1022516.6272\n",
      "\n",
      "Epoch 00004: val_loss improved from 1023529.05549 to 1022516.62724, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 849711.1620 - val_loss: 1045163.2752\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1022516.62724\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 57us/step - loss: 815419.7240 - val_loss: 1044825.9529\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1022516.62724\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 812894.7613 - val_loss: 1043723.6827\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1022516.62724\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 776241.8445 - val_loss: 1055210.2578\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1022516.62724\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 760908.5912 - val_loss: 1080996.9860\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1022516.62724\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 748435.5250 - val_loss: 1075150.5880\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1022516.62724\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 744715.0520 - val_loss: 1087205.4339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1022516.62724\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 721798.4444 - val_loss: 1110153.4692\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1022516.62724\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 696734.4707 - val_loss: 1140593.4546\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1022516.62724\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 678225.6874 - val_loss: 1102467.1771\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1022516.62724\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 668820.6347 - val_loss: 1136149.6491\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1022516.62724\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 655587.1836 - val_loss: 1142028.6418\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1022516.62724\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 632596.5947 - val_loss: 1173290.8716\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1022516.62724\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 626370.4863 - val_loss: 1195177.3419\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1022516.62724\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 614520.1120 - val_loss: 1143631.9776\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1022516.62724\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 608235.6884 - val_loss: 1175792.5757\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1022516.62724\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 579704.4082 - val_loss: 1293858.8666\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1022516.62724\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 569714.4667 - val_loss: 1234565.8896\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1022516.62724\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 536797.3681 - val_loss: 1273863.0073\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1022516.62724\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 538954.4986 - val_loss: 1346151.1934\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1022516.62724\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 520687.9587 - val_loss: 1318773.3100\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1022516.62724\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 520706.9818 - val_loss: 1240447.0561\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1022516.62724\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 537832.4629 - val_loss: 1444891.4226\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1022516.62724\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 495282.9159 - val_loss: 1343220.7550\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1022516.62724\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 482305.5690 - val_loss: 1393392.6519\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1022516.62724\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 481279.8039 - val_loss: 1442761.6861\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1022516.62724\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 493906.6570 - val_loss: 1630357.3627\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1022516.62724\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 463146.8059 - val_loss: 1486683.0628\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1022516.62724\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 441567.4483 - val_loss: 1573848.0874\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1022516.62724\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 443297.2204 - val_loss: 1450987.3161\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1022516.62724\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 448373.5018 - val_loss: 1433982.5174\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1022516.62724\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 3s 636us/step - loss: 2456781.8814 - val_loss: 2366062.3857\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366062.38565, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 2453241.1568 - val_loss: 2349945.4809\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366062.38565 to 2349945.48094, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 2406943.3591 - val_loss: 2154561.9036\n",
      "\n",
      "Epoch 00003: val_loss improved from 2349945.48094 to 2154561.90359, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 2047498.6117 - val_loss: 1357337.7780\n",
      "\n",
      "Epoch 00004: val_loss improved from 2154561.90359 to 1357337.77803, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 1322191.5209 - val_loss: 1366534.9098\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1357337.77803\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 1133179.7164 - val_loss: 1135856.5510\n",
      "\n",
      "Epoch 00001: val_loss improved from 1357337.77803 to 1135856.55101, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 1054336.8112 - val_loss: 1116317.1541\n",
      "\n",
      "Epoch 00002: val_loss improved from 1135856.55101 to 1116317.15415, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 999234.6274 - val_loss: 1065632.7382\n",
      "\n",
      "Epoch 00003: val_loss improved from 1116317.15415 to 1065632.73823, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 973492.5295 - val_loss: 1047089.7365\n",
      "\n",
      "Epoch 00004: val_loss improved from 1065632.73823 to 1047089.73655, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 965746.6226 - val_loss: 1023011.6827\n",
      "\n",
      "Epoch 00005: val_loss improved from 1047089.73655 to 1023011.68274, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 943417.2557 - val_loss: 1019466.5286\n",
      "\n",
      "Epoch 00001: val_loss improved from 1023011.68274 to 1019466.52859, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 932649.3306 - val_loss: 1020885.8061\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019466.52859\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 919629.8124 - val_loss: 1020927.2640\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019466.52859\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 59us/step - loss: 910956.3481 - val_loss: 1025360.3582\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019466.52859\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 894666.6688 - val_loss: 1012309.1099\n",
      "\n",
      "Epoch 00005: val_loss improved from 1019466.52859 to 1012309.10987, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 887450.8961 - val_loss: 1022626.6026\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012309.10987\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 866425.3451 - val_loss: 1050927.3167\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012309.10987\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 851730.4024 - val_loss: 1043479.7349\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012309.10987\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 838866.5438 - val_loss: 1041016.1642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012309.10987\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 814317.1706 - val_loss: 1024278.5790\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012309.10987\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 800592.3143 - val_loss: 1060334.7096\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012309.10987\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 772303.2635 - val_loss: 1054072.4669\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012309.10987\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 755033.9415 - val_loss: 1072986.4182\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012309.10987\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 744425.7229 - val_loss: 1072607.3526\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012309.10987\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 719495.1702 - val_loss: 1070283.3184\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012309.10987\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 713537.2194 - val_loss: 1101209.8778\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012309.10987\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 698520.0516 - val_loss: 1126553.2769\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012309.10987\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 683277.3241 - val_loss: 1078595.4400\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012309.10987\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 677528.9315 - val_loss: 1109686.0163\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012309.10987\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 654095.7865 - val_loss: 1093228.1508\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012309.10987\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 634862.3453 - val_loss: 1119573.3397\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012309.10987\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 627938.6250 - val_loss: 1149943.8952\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012309.10987\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 620099.0760 - val_loss: 1225037.9809\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012309.10987\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 599386.6808 - val_loss: 1202241.4815\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012309.10987\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 603206.4468 - val_loss: 1318438.3145\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012309.10987\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 567591.2868 - val_loss: 1202309.8828\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012309.10987\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 564579.8254 - val_loss: 1326473.0280\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012309.10987\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 545295.7322 - val_loss: 1296413.6093\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012309.10987\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 534400.9340 - val_loss: 1300546.0297\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012309.10987\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 533336.9631 - val_loss: 1310015.4339\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012309.10987\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 518773.3916 - val_loss: 1320935.2999\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012309.10987\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 520459.3102 - val_loss: 1337098.9098\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012309.10987\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 499810.3585 - val_loss: 1390504.9641\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012309.10987\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 492689.2503 - val_loss: 1390246.6177\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012309.10987\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 476403.1270 - val_loss: 1430863.2993\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012309.10987\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 461867.2223 - val_loss: 1428534.4563\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012309.10987\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 452544.1213 - val_loss: 1413026.7214\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012309.10987\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 456648.5881 - val_loss: 1475549.0824\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012309.10987\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 433044.8686 - val_loss: 1424773.0353\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012309.10987\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 429157.0543 - val_loss: 1470286.3610\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012309.10987\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 3s 708us/step - loss: 2456882.0040 - val_loss: 2366516.3857\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366516.38565, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 2455303.2140 - val_loss: 2359641.8150\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366516.38565 to 2359641.81502, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 2433717.8860 - val_loss: 2264877.5359\n",
      "\n",
      "Epoch 00003: val_loss improved from 2359641.81502 to 2264877.53587, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 2251043.7091 - val_loss: 1711278.3459\n",
      "\n",
      "Epoch 00004: val_loss improved from 2264877.53587 to 1711278.34585, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 1538440.8813 - val_loss: 1430982.6530\n",
      "\n",
      "Epoch 00005: val_loss improved from 1711278.34585 to 1430982.65303, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 1108798.8883 - val_loss: 1096490.8604\n",
      "\n",
      "Epoch 00001: val_loss improved from 1430982.65303 to 1096490.86043, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 57us/step - loss: 999892.6722 - val_loss: 1035675.4002\n",
      "\n",
      "Epoch 00002: val_loss improved from 1096490.86043 to 1035675.40022, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 964443.6961 - val_loss: 1015778.6570\n",
      "\n",
      "Epoch 00003: val_loss improved from 1035675.40022 to 1015778.65695, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 946887.8074 - val_loss: 1024392.2365\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 929694.9183 - val_loss: 1048529.1923\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 916142.2180 - val_loss: 1058192.9490\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 897130.5882 - val_loss: 1071067.1833\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 879925.7786 - val_loss: 1109143.6188\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 865202.5497 - val_loss: 1122375.5555\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 846909.4958 - val_loss: 1118649.1553\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 820981.9081 - val_loss: 1176662.8363\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 803519.1014 - val_loss: 1197289.4821\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 784529.1046 - val_loss: 1173732.5095\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 766801.6868 - val_loss: 1161802.3077\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 751456.8320 - val_loss: 1172594.7186\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 735446.0581 - val_loss: 1183628.2567\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 715798.7537 - val_loss: 1164622.8330\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 695364.0526 - val_loss: 1222691.3913\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 685984.5672 - val_loss: 1217309.4327\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 663463.0977 - val_loss: 1256226.1379\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 664857.7933 - val_loss: 1238689.3571\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 645085.5418 - val_loss: 1248378.8380\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 639864.5669 - val_loss: 1226464.8459\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 635400.5982 - val_loss: 1254409.0622\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 623916.5869 - val_loss: 1236365.4361\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 603235.7771 - val_loss: 1290465.9535\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 584007.5245 - val_loss: 1279150.9378\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 591576.7817 - val_loss: 1280151.0678\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 563413.4890 - val_loss: 1335664.7915\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 558382.3020 - val_loss: 1324035.1351\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 545347.7164 - val_loss: 1325925.9047\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 555244.3303 - val_loss: 1425467.1205\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 536366.8223 - val_loss: 1347706.4692\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 532495.5642 - val_loss: 1384940.2124\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 514398.3182 - val_loss: 1418565.1474\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 521827.8644 - val_loss: 1478646.8985\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 482850.4859 - val_loss: 1505334.3834\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 486466.7386 - val_loss: 1527297.5830\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 486026.0882 - val_loss: 1497299.2399\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 478981.5990 - val_loss: 1473271.2679\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 457416.6581 - val_loss: 1439417.5942\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1015778.65695\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 465527.3589 - val_loss: 1479357.1979\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1015778.65695\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 454886.3740 - val_loss: 1418628.5678\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1015778.65695\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 446491.1117 - val_loss: 1520829.5443\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1015778.65695\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 426076.8833 - val_loss: 1703674.7848\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1015778.65695\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 3s 778us/step - loss: 2456862.6909 - val_loss: 2366425.1278\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366425.12780, saving model to ./model/BranchRCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 2454763.9143 - val_loss: 2357458.3386\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366425.12780 to 2357458.33857, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 2426427.9600 - val_loss: 2240744.0516\n",
      "\n",
      "Epoch 00003: val_loss improved from 2357458.33857 to 2240744.05157, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 2211033.2473 - val_loss: 1646923.5482\n",
      "\n",
      "Epoch 00004: val_loss improved from 2240744.05157 to 1646923.54821, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 1472168.0945 - val_loss: 1599426.8800\n",
      "\n",
      "Epoch 00005: val_loss improved from 1646923.54821 to 1599426.88004, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 1064457.8134 - val_loss: 1205620.6138\n",
      "\n",
      "Epoch 00001: val_loss improved from 1599426.88004 to 1205620.61379, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 1003829.2145 - val_loss: 1152501.0011\n",
      "\n",
      "Epoch 00002: val_loss improved from 1205620.61379 to 1152501.00112, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 981943.6061 - val_loss: 1050583.6138\n",
      "\n",
      "Epoch 00003: val_loss improved from 1152501.00112 to 1050583.61379, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 965403.1659 - val_loss: 1039585.4765\n",
      "\n",
      "Epoch 00004: val_loss improved from 1050583.61379 to 1039585.47646, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 947923.1837 - val_loss: 1039458.0964\n",
      "\n",
      "Epoch 00005: val_loss improved from 1039585.47646 to 1039458.09641, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 943853.4074 - val_loss: 1029948.3386\n",
      "\n",
      "Epoch 00001: val_loss improved from 1039458.09641 to 1029948.33857, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 937468.4600 - val_loss: 1021940.1754\n",
      "\n",
      "Epoch 00002: val_loss improved from 1029948.33857 to 1021940.17545, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 56us/step - loss: 918205.3764 - val_loss: 1030499.4540\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 908902.6857 - val_loss: 1033876.8240\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 891738.9843 - val_loss: 1030457.9002\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 875797.7909 - val_loss: 1038919.8632\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1021940.17545\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 866616.2435 - val_loss: 1038560.3038\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1021940.17545\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 845980.3411 - val_loss: 1042299.9933\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 828890.0316 - val_loss: 1059258.0589\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 816766.7852 - val_loss: 1061665.9568\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 792998.2547 - val_loss: 1068502.1435\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1021940.17545\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 765001.0457 - val_loss: 1069076.2197\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1021940.17545\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 753536.5371 - val_loss: 1074292.1480\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 731642.4164 - val_loss: 1073658.6160\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 722520.2946 - val_loss: 1086283.0645\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 700926.7232 - val_loss: 1085166.7556\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1021940.17545\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 691441.0395 - val_loss: 1072133.3812\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1021940.17545\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 663230.2733 - val_loss: 1103395.1530\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 643943.8096 - val_loss: 1123223.8599\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 633801.0127 - val_loss: 1164955.0107\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 611770.0121 - val_loss: 1153070.2382\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1021940.17545\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 606045.8773 - val_loss: 1154846.7287\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1021940.17545\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 596053.7432 - val_loss: 1144527.4120\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 568787.8085 - val_loss: 1178147.5191\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 557965.7521 - val_loss: 1218916.7932\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 555036.2086 - val_loss: 1247491.3907\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1021940.17545\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 529326.1234 - val_loss: 1218549.7948\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1021940.17545\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 518649.4328 - val_loss: 1277037.3929\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 503037.9459 - val_loss: 1258324.7012\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 485765.2839 - val_loss: 1277955.9664\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 478183.5277 - val_loss: 1291131.9877\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1021940.17545\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 478269.3253 - val_loss: 1308164.8744\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1021940.17545\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 464330.2359 - val_loss: 1298450.1362\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 57us/step - loss: 448636.2229 - val_loss: 1330473.0516\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 439229.9698 - val_loss: 1375039.7842\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 437185.7956 - val_loss: 1416288.0174\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1021940.17545\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 411786.9391 - val_loss: 1380531.1132\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1021940.17545\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 406816.7172 - val_loss: 1382349.7808\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1021940.17545\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 389519.6528 - val_loss: 1433572.8229\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1021940.17545\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 400083.3011 - val_loss: 1391799.7472\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1021940.17545\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 3s 868us/step - loss: 2456794.5181 - val_loss: 2366088.4619\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366088.46188, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 2453190.3858 - val_loss: 2349117.0897\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366088.46188 to 2349117.08969, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 2404621.4007 - val_loss: 2159117.7522\n",
      "\n",
      "Epoch 00003: val_loss improved from 2349117.08969 to 2159117.75224, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 2059726.8355 - val_loss: 1410592.7848\n",
      "\n",
      "Epoch 00004: val_loss improved from 2159117.75224 to 1410592.78475, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 1292487.1350 - val_loss: 1421450.1003\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1410592.78475\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 1107428.6614 - val_loss: 1233934.1110\n",
      "\n",
      "Epoch 00001: val_loss improved from 1410592.78475 to 1233934.11099, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 1034205.1738 - val_loss: 1170057.6693\n",
      "\n",
      "Epoch 00002: val_loss improved from 1233934.11099 to 1170057.66928, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 1003729.7268 - val_loss: 1096802.7584\n",
      "\n",
      "Epoch 00003: val_loss improved from 1170057.66928 to 1096802.75841, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 974475.6830 - val_loss: 1074949.6816\n",
      "\n",
      "Epoch 00004: val_loss improved from 1096802.75841 to 1074949.68161, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 946644.6769 - val_loss: 1050272.0841\n",
      "\n",
      "Epoch 00005: val_loss improved from 1074949.68161 to 1050272.08408, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 938906.1649 - val_loss: 1052914.6037\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1050272.08408\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 923805.2421 - val_loss: 1041670.4787\n",
      "\n",
      "Epoch 00002: val_loss improved from 1050272.08408 to 1041670.47870, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 906894.2812 - val_loss: 1050811.9103\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 898109.7488 - val_loss: 1059396.8436\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 888937.6109 - val_loss: 1064290.7074\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 878171.4691 - val_loss: 1060758.3750\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1041670.47870\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 850719.2590 - val_loss: 1056458.9944\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1041670.47870\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 848296.2234 - val_loss: 1060582.7068\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 837149.1744 - val_loss: 1053544.5908\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 825380.2093 - val_loss: 1069274.9226\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 809074.4399 - val_loss: 1059587.2909\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1041670.47870\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 783897.3562 - val_loss: 1094950.0577\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1041670.47870\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 768044.7193 - val_loss: 1131845.7399\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 741043.1061 - val_loss: 1132094.5370\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 736670.4008 - val_loss: 1110983.3358\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 717487.9150 - val_loss: 1115436.0050\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1041670.47870\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 706519.3636 - val_loss: 1153679.4821\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1041670.47870\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 687426.0779 - val_loss: 1164455.3744\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 689931.4675 - val_loss: 1180021.8840\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 675442.5345 - val_loss: 1123243.6822\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 669797.0299 - val_loss: 1148607.4126\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1041670.47870\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 639852.9368 - val_loss: 1153689.5017\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1041670.47870\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 643778.3329 - val_loss: 1268816.7108\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 635657.9880 - val_loss: 1247637.8632\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 613014.8412 - val_loss: 1218751.5050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 57us/step - loss: 598949.2399 - val_loss: 1221315.1665\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1041670.47870\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 586036.5637 - val_loss: 1256200.9075\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1041670.47870\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 572589.4597 - val_loss: 1293324.8167\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 570207.6722 - val_loss: 1299125.6418\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 558419.9713 - val_loss: 1301685.6435\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 543785.4271 - val_loss: 1356239.6334\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1041670.47870\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 528950.9849 - val_loss: 1334612.2253\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1041670.47870\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 517022.1634 - val_loss: 1316317.3352\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 507804.9914 - val_loss: 1338623.3733\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 504892.0240 - val_loss: 1570773.2932\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 501079.6082 - val_loss: 1414189.7999\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1041670.47870\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 484425.6357 - val_loss: 1488850.1990\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1041670.47870\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 471730.9475 - val_loss: 1549840.8767\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1041670.47870\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 469746.8647 - val_loss: 1512602.7741\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1041670.47870\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 462692.6535 - val_loss: 1540638.1990\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1041670.47870\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 4s 954us/step - loss: 2456805.9273 - val_loss: 2366233.3744\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366233.37444, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 2453201.6177 - val_loss: 2353371.0258\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366233.37444 to 2353371.02578, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 2406049.1268 - val_loss: 2199229.2220\n",
      "\n",
      "Epoch 00003: val_loss improved from 2353371.02578 to 2199229.22197, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 2072534.0007 - val_loss: 1526224.9781\n",
      "\n",
      "Epoch 00004: val_loss improved from 2199229.22197 to 1526224.97814, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 1305119.0533 - val_loss: 1430934.6194\n",
      "\n",
      "Epoch 00005: val_loss improved from 1526224.97814 to 1430934.61939, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 1094868.3989 - val_loss: 1071575.0678\n",
      "\n",
      "Epoch 00001: val_loss improved from 1430934.61939 to 1071575.06783, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 1002696.6032 - val_loss: 1040156.6687\n",
      "\n",
      "Epoch 00002: val_loss improved from 1071575.06783 to 1040156.66872, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 961206.4576 - val_loss: 1012249.7752\n",
      "\n",
      "Epoch 00003: val_loss improved from 1040156.66872 to 1012249.77522, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 948344.0825 - val_loss: 1008642.7253\n",
      "\n",
      "Epoch 00004: val_loss improved from 1012249.77522 to 1008642.72534, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 936391.0201 - val_loss: 1016567.5583\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 916680.5747 - val_loss: 1019738.2786\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 908954.3426 - val_loss: 1025174.1816\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 896086.7492 - val_loss: 1026478.4484\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 873036.4120 - val_loss: 1023841.0017\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 860197.7965 - val_loss: 1024041.4120\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 849389.8225 - val_loss: 1040022.4877\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 823500.5566 - val_loss: 1038239.4737\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 802675.6262 - val_loss: 1041411.6833\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 797663.5338 - val_loss: 1049101.6850\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 761086.0932 - val_loss: 1051062.2707\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 752371.3773 - val_loss: 1071894.2450\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 731860.8121 - val_loss: 1077037.5723\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 722957.3819 - val_loss: 1099201.2024\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 713394.8219 - val_loss: 1106720.0516\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 682454.4023 - val_loss: 1132691.1048\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 686041.0583 - val_loss: 1129342.6485\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 676271.6540 - val_loss: 1127138.5235\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 660223.8594 - val_loss: 1130561.9137\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 58us/step - loss: 629849.2164 - val_loss: 1187272.7399\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 648195.8456 - val_loss: 1188325.5830\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 622249.2686 - val_loss: 1180831.7158\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 612781.4476 - val_loss: 1295211.7091\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 597649.2781 - val_loss: 1270746.9041\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 598506.9737 - val_loss: 1283689.9501\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 572354.8501 - val_loss: 1315943.0555\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 563902.8186 - val_loss: 1337800.4467\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 560055.6342 - val_loss: 1272324.6883\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 552662.1263 - val_loss: 1298409.4165\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 546169.7370 - val_loss: 1308667.6020\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 524813.5539 - val_loss: 1397025.5101\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 525931.7433 - val_loss: 1504408.3128\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 497578.4717 - val_loss: 1361401.0852\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 494374.0303 - val_loss: 1419756.3313\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 485406.2495 - val_loss: 1423559.9832\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 479612.4721 - val_loss: 1459587.4933\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 465806.9747 - val_loss: 1478767.6883\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1008642.72534\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 465648.8299 - val_loss: 1532379.4271\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1008642.72534\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 447895.5820 - val_loss: 1486381.9378\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1008642.72534\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 444391.7010 - val_loss: 1624060.9647\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1008642.72534\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 57us/step - loss: 434116.3529 - val_loss: 1547455.1244\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1008642.72534\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 4s 1ms/step - loss: 2456872.7755 - val_loss: 2366480.7489\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366480.74888, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 2455045.0298 - val_loss: 2358901.7848\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366480.74888 to 2358901.78475, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 2427612.3529 - val_loss: 2239880.8812\n",
      "\n",
      "Epoch 00003: val_loss improved from 2358901.78475 to 2239880.88117, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 2203156.1150 - val_loss: 1657052.8217\n",
      "\n",
      "Epoch 00004: val_loss improved from 2239880.88117 to 1657052.82175, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 1520071.4471 - val_loss: 1860909.4305\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1657052.82175\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 1215087.3814 - val_loss: 1219162.4877\n",
      "\n",
      "Epoch 00001: val_loss improved from 1657052.82175 to 1219162.48767, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 1086235.5117 - val_loss: 1115601.4882\n",
      "\n",
      "Epoch 00002: val_loss improved from 1219162.48767 to 1115601.48823, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 1019795.0907 - val_loss: 1051806.9865\n",
      "\n",
      "Epoch 00003: val_loss improved from 1115601.48823 to 1051806.98655, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 971855.6266 - val_loss: 1045817.6732\n",
      "\n",
      "Epoch 00004: val_loss improved from 1051806.98655 to 1045817.67321, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 955703.7661 - val_loss: 1023091.1424\n",
      "\n",
      "Epoch 00005: val_loss improved from 1045817.67321 to 1023091.14238, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 938501.1869 - val_loss: 1019120.8352\n",
      "\n",
      "Epoch 00001: val_loss improved from 1023091.14238 to 1019120.83520, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 922007.0440 - val_loss: 1013706.7091\n",
      "\n",
      "Epoch 00002: val_loss improved from 1019120.83520 to 1013706.70908, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 910272.3837 - val_loss: 1011651.1833\n",
      "\n",
      "Epoch 00003: val_loss improved from 1013706.70908 to 1011651.18330, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 886175.2558 - val_loss: 1002166.3991\n",
      "\n",
      "Epoch 00004: val_loss improved from 1011651.18330 to 1002166.39910, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 867675.1174 - val_loss: 1004326.6115\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002166.39910\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 853136.5848 - val_loss: 1003963.2130\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002166.39910\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 831847.3972 - val_loss: 1008680.6110\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002166.39910\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 828371.8792 - val_loss: 1001259.5219\n",
      "\n",
      "Epoch 00003: val_loss improved from 1002166.39910 to 1001259.52186, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 792782.0713 - val_loss: 1006983.2203\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1001259.52186\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 60us/step - loss: 778893.2539 - val_loss: 1015457.6598\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1001259.52186\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 759766.2453 - val_loss: 1031506.4221\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1001259.52186\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 741186.4996 - val_loss: 1040964.2814\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1001259.52186\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 721523.8197 - val_loss: 1041919.9378\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1001259.52186\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 726678.5533 - val_loss: 1045545.2416\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1001259.52186\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 705005.7145 - val_loss: 1074052.0538\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1001259.52186\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 682317.4216 - val_loss: 1082210.5286\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1001259.52186\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 662757.8892 - val_loss: 1094883.2724\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1001259.52186\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 650867.7180 - val_loss: 1131596.2522\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1001259.52186\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 649410.5651 - val_loss: 1137500.8223\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1001259.52186\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 631324.8394 - val_loss: 1165815.0555\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1001259.52186\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 623700.0826 - val_loss: 1199401.0992\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1001259.52186\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 607681.7328 - val_loss: 1203155.6390\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1001259.52186\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 592744.1232 - val_loss: 1267928.0622\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1001259.52186\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 594302.8995 - val_loss: 1252319.4720\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1001259.52186\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 563838.1691 - val_loss: 1257583.9787\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1001259.52186\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 558311.4444 - val_loss: 1264915.8408\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1001259.52186\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 560907.0697 - val_loss: 1332834.8907\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1001259.52186\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 548248.8724 - val_loss: 1309821.0488\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1001259.52186\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 527635.8847 - val_loss: 1302867.5835\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1001259.52186\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 525450.6917 - val_loss: 1295783.9008\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1001259.52186\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 505219.0692 - val_loss: 1438155.3991\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1001259.52186\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 495004.6485 - val_loss: 1516974.2562\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1001259.52186\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 487971.4818 - val_loss: 1375713.8178\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1001259.52186\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 469788.8676 - val_loss: 1407162.2416\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1001259.52186\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 478653.6198 - val_loss: 1420571.0555\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1001259.52186\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 470341.2993 - val_loss: 1438896.5846\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1001259.52186\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 462488.1847 - val_loss: 1457385.5454\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1001259.52186\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 453324.4835 - val_loss: 1480040.9013\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1001259.52186\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 462858.2711 - val_loss: 1484885.9355\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1001259.52186\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 439215.7296 - val_loss: 1501565.1424\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1001259.52186\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 4s 1ms/step - loss: 2456813.8358 - val_loss: 2366180.0280\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366180.02803, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 2453655.4357 - val_loss: 2351222.4137\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366180.02803 to 2351222.41368, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 2407886.8762 - val_loss: 2167513.4854\n",
      "\n",
      "Epoch 00003: val_loss improved from 2351222.41368 to 2167513.48543, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 2062113.5239 - val_loss: 1432539.2175\n",
      "\n",
      "Epoch 00004: val_loss improved from 2167513.48543 to 1432539.21749, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 1331622.2349 - val_loss: 1455358.8475\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1432539.21749\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 1127891.6124 - val_loss: 1108098.2797\n",
      "\n",
      "Epoch 00001: val_loss improved from 1432539.21749 to 1108098.27971, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 1037886.4841 - val_loss: 1028263.5863\n",
      "\n",
      "Epoch 00002: val_loss improved from 1108098.27971 to 1028263.58632, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 990208.8146 - val_loss: 1007738.0832\n",
      "\n",
      "Epoch 00003: val_loss improved from 1028263.58632 to 1007738.08324, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 972542.3882 - val_loss: 994964.9263\n",
      "\n",
      "Epoch 00004: val_loss improved from 1007738.08324 to 994964.92629, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 956843.9265 - val_loss: 984929.0339\n",
      "\n",
      "Epoch 00005: val_loss improved from 994964.92629 to 984929.03391, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 944655.0442 - val_loss: 985738.4731\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 933897.4235 - val_loss: 988432.9955\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 61us/step - loss: 927822.4083 - val_loss: 1005548.5975\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 911656.9912 - val_loss: 1004336.1432\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 894681.7348 - val_loss: 1004714.1771\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 873683.3496 - val_loss: 1003662.0992\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 868287.6569 - val_loss: 1020041.5886\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 848977.2192 - val_loss: 1025604.4686\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 835366.7808 - val_loss: 1021204.4232\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 818779.0807 - val_loss: 1021872.9574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 805107.7546 - val_loss: 1038670.1900\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 784481.1841 - val_loss: 1023476.4226\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 769557.7753 - val_loss: 1034162.4496\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 760697.1850 - val_loss: 1057712.9479\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 741244.8905 - val_loss: 1082280.9187\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 724758.1817 - val_loss: 1081019.4546\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 713103.3954 - val_loss: 1090083.5746\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 695927.8324 - val_loss: 1104206.4809\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 687602.5397 - val_loss: 1112419.1811\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 674268.3218 - val_loss: 1135772.9378\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 669872.6688 - val_loss: 1165563.5975\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 654710.9708 - val_loss: 1144708.3240\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 641804.9371 - val_loss: 1191585.4540\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 630977.1520 - val_loss: 1227946.7724\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 610341.6545 - val_loss: 1254607.9462\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 596328.0219 - val_loss: 1249744.7337\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 588565.9536 - val_loss: 1301599.5807\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 580536.6749 - val_loss: 1298743.7444\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 553459.6443 - val_loss: 1431254.2943\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 547571.9373 - val_loss: 1409047.6783\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 532267.4235 - val_loss: 1490526.9843\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 531506.7843 - val_loss: 1445287.0516\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 531840.3760 - val_loss: 1492589.3330\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 512705.7518 - val_loss: 1374894.3430\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 491982.7589 - val_loss: 1337997.3705\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 478180.4909 - val_loss: 1446365.2820\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 984929.03391\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 480024.3926 - val_loss: 1436536.1317\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 984929.03391\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 473205.4886 - val_loss: 1516917.7405\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 984929.03391\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 461296.3688 - val_loss: 1494027.5342\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 984929.03391\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 452894.6062 - val_loss: 1671666.9961\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 984929.03391\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 5s 1ms/step - loss: 2456838.4300 - val_loss: 2366336.1637\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366336.16368, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 2454355.3350 - val_loss: 2355975.2052\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366336.16368 to 2355975.20516, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 2421363.0609 - val_loss: 2221314.5594\n",
      "\n",
      "Epoch 00003: val_loss improved from 2355975.20516 to 2221314.55942, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 2163018.4140 - val_loss: 1567809.2786\n",
      "\n",
      "Epoch 00004: val_loss improved from 2221314.55942 to 1567809.27859, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - ETA: 0s - loss: 1455122.500 - 0s 66us/step - loss: 1425870.7260 - val_loss: 1808993.7410\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1567809.27859\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 1161560.1671 - val_loss: 1202937.5504\n",
      "\n",
      "Epoch 00001: val_loss improved from 1567809.27859 to 1202937.55045, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 59us/step - loss: 1071009.2446 - val_loss: 1131446.5465\n",
      "\n",
      "Epoch 00002: val_loss improved from 1202937.55045 to 1131446.54652, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 1012837.4580 - val_loss: 1028385.0022\n",
      "\n",
      "Epoch 00003: val_loss improved from 1131446.54652 to 1028385.00224, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 972393.3077 - val_loss: 1002042.3985\n",
      "\n",
      "Epoch 00004: val_loss improved from 1028385.00224 to 1002042.39854, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 959395.0421 - val_loss: 1006647.1463\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 942325.0133 - val_loss: 1019413.8632\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 933283.2296 - val_loss: 1016384.2360\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 916975.6570 - val_loss: 1017341.5084\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 913836.2041 - val_loss: 1023335.0191\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 892416.3757 - val_loss: 1016346.8498\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 881747.4561 - val_loss: 1036014.4798\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 878779.1331 - val_loss: 1030075.0247\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 862172.9314 - val_loss: 1040047.2427\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 848167.4301 - val_loss: 1039124.1043\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 836094.3647 - val_loss: 1037552.5695\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 821810.1418 - val_loss: 1042236.2024\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 797806.0598 - val_loss: 1043065.6749\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 800581.5712 - val_loss: 1037816.5146\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 790047.1974 - val_loss: 1033467.6048\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 770687.3282 - val_loss: 1040659.2685\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 751515.4805 - val_loss: 1035399.0561\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 732302.8968 - val_loss: 1051960.4311\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 724182.5567 - val_loss: 1049402.0336\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 706250.1382 - val_loss: 1057256.4193\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 689069.2196 - val_loss: 1074428.3873\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 674612.9535 - val_loss: 1087252.5140\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 669973.8004 - val_loss: 1106337.8997\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 645615.9763 - val_loss: 1099724.8946\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 635360.2893 - val_loss: 1152754.8094\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 628984.3196 - val_loss: 1200423.4025\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 611255.2324 - val_loss: 1187341.4473\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 612782.6768 - val_loss: 1177155.7091\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 587617.2628 - val_loss: 1135461.0645\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 576393.9623 - val_loss: 1153845.4473\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 574141.9449 - val_loss: 1216647.2186\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 550822.4918 - val_loss: 1217483.4950\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 58us/step - loss: 525317.0669 - val_loss: 1208551.6407\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 529560.1928 - val_loss: 1218633.7237\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 510867.1111 - val_loss: 1327100.5701\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 496862.3097 - val_loss: 1275123.9882\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 493173.8227 - val_loss: 1367320.9226\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1002042.39854\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 59us/step - loss: 486967.9597 - val_loss: 1334458.1256\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1002042.39854\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 470036.3504 - val_loss: 1333633.5258\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1002042.39854\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 470088.0026 - val_loss: 1317091.4395\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1002042.39854\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 60us/step - loss: 470216.8495 - val_loss: 1337106.1054\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1002042.39854\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 5s 1ms/step - loss: 2456834.9207 - val_loss: 2366268.0404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366268.04036, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 2453894.0897 - val_loss: 2353589.7567\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366268.04036 to 2353589.75673, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 2413538.3827 - val_loss: 2197456.8812\n",
      "\n",
      "Epoch 00003: val_loss improved from 2353589.75673 to 2197456.88117, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 2113421.9507 - val_loss: 1518914.5695\n",
      "\n",
      "Epoch 00004: val_loss improved from 2197456.88117 to 1518914.56951, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 1340896.1019 - val_loss: 1515362.3100\n",
      "\n",
      "Epoch 00005: val_loss improved from 1518914.56951 to 1515362.30998, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 1121629.3901 - val_loss: 1123229.7825\n",
      "\n",
      "Epoch 00001: val_loss improved from 1515362.30998 to 1123229.78251, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 1038324.4118 - val_loss: 1086058.4467\n",
      "\n",
      "Epoch 00002: val_loss improved from 1123229.78251 to 1086058.44675, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 991481.6099 - val_loss: 1023224.1143\n",
      "\n",
      "Epoch 00003: val_loss improved from 1086058.44675 to 1023224.11435, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 971032.2833 - val_loss: 1007592.6715\n",
      "\n",
      "Epoch 00004: val_loss improved from 1023224.11435 to 1007592.67152, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 958406.6065 - val_loss: 1001430.0998\n",
      "\n",
      "Epoch 00005: val_loss improved from 1007592.67152 to 1001430.09978, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 943731.7601 - val_loss: 998193.8812\n",
      "\n",
      "Epoch 00001: val_loss improved from 1001430.09978 to 998193.88117, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 930484.9542 - val_loss: 998197.2578\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 998193.88117\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 926466.8621 - val_loss: 992458.7012\n",
      "\n",
      "Epoch 00003: val_loss improved from 998193.88117 to 992458.70123, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 925444.5789 - val_loss: 997833.9832\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 992458.70123\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 899589.2230 - val_loss: 990265.0275\n",
      "\n",
      "Epoch 00005: val_loss improved from 992458.70123 to 990265.02747, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 891772.7660 - val_loss: 991434.8010\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990265.02747\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 878465.3916 - val_loss: 1008542.9787\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990265.02747\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 865931.3921 - val_loss: 1015431.3021\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990265.02747\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 848720.3588 - val_loss: 1001033.9652\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990265.02747\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 831604.2221 - val_loss: 1022537.8862\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990265.02747\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 824081.1647 - val_loss: 1021610.5286\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990265.02747\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 815036.2088 - val_loss: 1007708.7691\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990265.02747\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 786852.2372 - val_loss: 1036892.2836\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990265.02747\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 781373.1613 - val_loss: 1027494.2763\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990265.02747\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 782606.6787 - val_loss: 1085931.2169\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990265.02747\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 748057.6634 - val_loss: 1058945.3010\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990265.02747\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 738153.4391 - val_loss: 1094380.4025\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990265.02747\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 723011.5593 - val_loss: 1097997.0583\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990265.02747\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 710667.8009 - val_loss: 1088012.1396\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990265.02747\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 61us/step - loss: 684707.0604 - val_loss: 1123850.8397\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990265.02747\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 670731.8814 - val_loss: 1130234.7007\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990265.02747\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 663915.2621 - val_loss: 1197845.8313\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990265.02747\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 643473.1505 - val_loss: 1221674.7096\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990265.02747\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 631076.9208 - val_loss: 1238083.4204\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990265.02747\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 617835.2199 - val_loss: 1265596.2578\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990265.02747\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 615238.4306 - val_loss: 1212783.3610\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990265.02747\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 594594.4680 - val_loss: 1240431.3795\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990265.02747\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 580354.3001 - val_loss: 1281201.1726\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990265.02747\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 585270.8521 - val_loss: 1317516.6020\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990265.02747\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 553064.5557 - val_loss: 1324876.5364\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990265.02747\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 547763.5704 - val_loss: 1347152.2136\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990265.02747\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 541102.0653 - val_loss: 1273826.4423\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990265.02747\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 523558.4900 - val_loss: 1383504.1642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 990265.02747\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 502037.4154 - val_loss: 1348719.1216\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990265.02747\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 509539.8516 - val_loss: 1484140.9815\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990265.02747\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 518023.5211 - val_loss: 1585424.3638\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990265.02747\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 501637.2590 - val_loss: 1475357.2640\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990265.02747\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 476378.8006 - val_loss: 1389828.4232\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990265.02747\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 455181.2018 - val_loss: 1353532.9019\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990265.02747\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 454687.6674 - val_loss: 1361147.4271\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990265.02747\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 6s 1ms/step - loss: 2456849.4782 - val_loss: 2366386.4865\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366386.48655, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 2454720.5588 - val_loss: 2356830.6547\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366386.48655 to 2356830.65471, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 2423549.5630 - val_loss: 2221497.7276\n",
      "\n",
      "Epoch 00003: val_loss improved from 2356830.65471 to 2221497.72758, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 2160191.1456 - val_loss: 1526168.1586\n",
      "\n",
      "Epoch 00004: val_loss improved from 2221497.72758 to 1526168.15863, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 1399631.0123 - val_loss: 1702521.8302\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1526168.15863\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 1163295.5706 - val_loss: 1149325.5561\n",
      "\n",
      "Epoch 00001: val_loss improved from 1526168.15863 to 1149325.55605, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 1081393.4346 - val_loss: 1089177.6642\n",
      "\n",
      "Epoch 00002: val_loss improved from 1149325.55605 to 1089177.66424, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 1022282.5360 - val_loss: 1028443.7180\n",
      "\n",
      "Epoch 00003: val_loss improved from 1089177.66424 to 1028443.71805, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 970007.7361 - val_loss: 1002668.9316\n",
      "\n",
      "Epoch 00004: val_loss improved from 1028443.71805 to 1002668.93161, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 945629.4497 - val_loss: 999515.1788\n",
      "\n",
      "Epoch 00005: val_loss improved from 1002668.93161 to 999515.17881, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 933846.1530 - val_loss: 1005599.3436\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 915939.5966 - val_loss: 1010330.5028\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 898675.4800 - val_loss: 1022949.5258\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 887470.1618 - val_loss: 1023303.2001\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 868796.0667 - val_loss: 1031526.3694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 848426.4993 - val_loss: 1043277.2668\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 832075.6551 - val_loss: 1074497.7735\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 813409.8328 - val_loss: 1081785.6687\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 793462.4856 - val_loss: 1078195.9075\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 767329.7273 - val_loss: 1076892.3873\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 762785.1989 - val_loss: 1088247.5291\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 755386.2486 - val_loss: 1098272.0235\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 726601.4275 - val_loss: 1116825.7595\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 716133.5530 - val_loss: 1133477.3845\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 700839.3047 - val_loss: 1167896.9933\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 688901.4921 - val_loss: 1138274.6441\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 684569.0032 - val_loss: 1180549.2550\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 673618.7427 - val_loss: 1180442.0291\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 654786.8551 - val_loss: 1149816.6474\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 646862.2529 - val_loss: 1199835.9989\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 627495.4344 - val_loss: 1234685.8610\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 606968.6269 - val_loss: 1298282.4922\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 578257.9190 - val_loss: 1250766.0039\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 590834.3930 - val_loss: 1279830.5998\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 580324.0417 - val_loss: 1322052.3615\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 563568.8962 - val_loss: 1311718.7550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 556302.8189 - val_loss: 1342480.8806\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 542824.9667 - val_loss: 1320028.2433\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 530664.5906 - val_loss: 1376045.4165\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 528655.4436 - val_loss: 1513862.1788\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 528705.6500 - val_loss: 1472873.0488\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 512674.5548 - val_loss: 1432795.5751\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 507102.3265 - val_loss: 1442096.2321\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 485979.5845 - val_loss: 1502609.6967\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 482326.0200 - val_loss: 1485540.6715\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 486020.6454 - val_loss: 1462339.7113\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999515.17881\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 480875.3719 - val_loss: 1449766.1917\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999515.17881\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 62us/step - loss: 471275.5445 - val_loss: 1425783.0123\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999515.17881\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 452112.7355 - val_loss: 1527507.8929\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999515.17881\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 458770.2269 - val_loss: 1611236.6586\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999515.17881\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 6s 1ms/step - loss: 2456879.0750 - val_loss: 2366506.7646\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366506.76457, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 2455348.6494 - val_loss: 2359985.3733\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366506.76457 to 2359985.37332, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 2433619.3718 - val_loss: 2269694.1099\n",
      "\n",
      "Epoch 00003: val_loss improved from 2359985.37332 to 2269694.10987, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 2243475.9219 - val_loss: 1720552.3711\n",
      "\n",
      "Epoch 00004: val_loss improved from 2269694.10987 to 1720552.37108, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 1525336.4524 - val_loss: 1712154.2035\n",
      "\n",
      "Epoch 00005: val_loss improved from 1720552.37108 to 1712154.20348, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 1174281.1455 - val_loss: 1266296.7657\n",
      "\n",
      "Epoch 00001: val_loss improved from 1712154.20348 to 1266296.76570, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 1062458.4761 - val_loss: 1194574.1491\n",
      "\n",
      "Epoch 00002: val_loss improved from 1266296.76570 to 1194574.14910, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 1009776.3364 - val_loss: 1097245.4047\n",
      "\n",
      "Epoch 00003: val_loss improved from 1194574.14910 to 1097245.40471, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 967030.9145 - val_loss: 1050591.8380\n",
      "\n",
      "Epoch 00004: val_loss improved from 1097245.40471 to 1050591.83800, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 943300.1410 - val_loss: 1030846.3111\n",
      "\n",
      "Epoch 00005: val_loss improved from 1050591.83800 to 1030846.31110, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 934373.7036 - val_loss: 1020551.1827\n",
      "\n",
      "Epoch 00001: val_loss improved from 1030846.31110 to 1020551.18274, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 915470.0955 - val_loss: 1034344.2147\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 903926.1391 - val_loss: 1035191.7651\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 886578.0136 - val_loss: 1053271.5706\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 874004.1813 - val_loss: 1030246.7377\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 857151.0996 - val_loss: 1030183.2747\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020551.18274\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 835710.6612 - val_loss: 1045457.0516\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 812386.0617 - val_loss: 1029234.9210\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 795687.5445 - val_loss: 1041703.4120\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 793152.0626 - val_loss: 1045893.1104\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 759846.0099 - val_loss: 1051880.9423\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020551.18274\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 749996.2775 - val_loss: 1045846.4120\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 63us/step - loss: 720611.7561 - val_loss: 1050270.4737\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 699629.2689 - val_loss: 1059005.4798\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 695714.3066 - val_loss: 1076844.4226\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 688924.2899 - val_loss: 1087135.8346\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020551.18274\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 685444.3958 - val_loss: 1084879.9238\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 663521.2666 - val_loss: 1096006.3033\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 66us/step - loss: 636141.6515 - val_loss: 1076424.5275\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 627903.8293 - val_loss: 1106565.0437\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 624886.4496 - val_loss: 1123440.6413\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020551.18274\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 608158.3130 - val_loss: 1113834.5942\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 604630.8709 - val_loss: 1160302.5471\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 588019.2824 - val_loss: 1185952.8111\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 590703.2258 - val_loss: 1220782.8066\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 570432.0506 - val_loss: 1202867.3217\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020551.18274\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 555257.4694 - val_loss: 1227153.2999\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 545594.0345 - val_loss: 1155570.7444\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 530356.8438 - val_loss: 1212766.4064\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 516949.9068 - val_loss: 1196187.5320\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 510744.1554 - val_loss: 1244733.1525\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020551.18274\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 505414.5255 - val_loss: 1253779.2393\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 481046.3967 - val_loss: 1303709.8447\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 495930.7861 - val_loss: 1342595.9955\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 475294.7605 - val_loss: 1372350.9467\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 463262.5705 - val_loss: 1321508.6984\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020551.18274\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 462188.2515 - val_loss: 1276374.1939\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020551.18274\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 443165.7115 - val_loss: 1319455.0561\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020551.18274\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 436456.5402 - val_loss: 1295749.5897\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020551.18274\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 427721.6426 - val_loss: 1314102.6895\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020551.18274\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 6s 2ms/step - loss: 2456808.6023 - val_loss: 2366214.4843\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366214.48430, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 2453687.4978 - val_loss: 2352427.4910\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366214.48430 to 2352427.49103, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 2412077.3286 - val_loss: 2175650.9327\n",
      "\n",
      "Epoch 00003: val_loss improved from 2352427.49103 to 2175650.93274, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 2095776.6614 - val_loss: 1474631.1648\n",
      "\n",
      "Epoch 00004: val_loss improved from 2175650.93274 to 1474631.16480, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 1374395.2734 - val_loss: 1741206.4350\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1474631.16480\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 1156279.7942 - val_loss: 1186886.5908\n",
      "\n",
      "Epoch 00001: val_loss improved from 1474631.16480 to 1186886.59081, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 1068254.9362 - val_loss: 1085672.6631\n",
      "\n",
      "Epoch 00002: val_loss improved from 1186886.59081 to 1085672.66312, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 1004315.9654 - val_loss: 1016113.3498\n",
      "\n",
      "Epoch 00003: val_loss improved from 1085672.66312 to 1016113.34978, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 963207.2379 - val_loss: 1010507.3217\n",
      "\n",
      "Epoch 00004: val_loss improved from 1016113.34978 to 1010507.32175, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 958113.5900 - val_loss: 1004325.3571\n",
      "\n",
      "Epoch 00005: val_loss improved from 1010507.32175 to 1004325.35706, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 930805.1815 - val_loss: 1006308.7281\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1004325.35706\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 919542.7793 - val_loss: 990833.2136\n",
      "\n",
      "Epoch 00002: val_loss improved from 1004325.35706 to 990833.21357, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 909073.8666 - val_loss: 1001313.8711\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 893148.6992 - val_loss: 1011081.4148\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 881094.4261 - val_loss: 1005039.2618\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 851346.0740 - val_loss: 1001549.2671\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990833.21357\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 843078.0867 - val_loss: 1007535.7197\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990833.21357\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 817969.3340 - val_loss: 1016419.9176\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 807325.8662 - val_loss: 1022337.7525\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 793649.0492 - val_loss: 1033475.8929\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 66us/step - loss: 779506.3516 - val_loss: 1040462.2242\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990833.21357\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 762400.0865 - val_loss: 1063549.7887\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990833.21357\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 738518.0548 - val_loss: 1073914.5303\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 734866.2513 - val_loss: 1094758.6956\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 731790.4013 - val_loss: 1074474.9254\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 705106.6526 - val_loss: 1082068.2068\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990833.21357\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 686273.0610 - val_loss: 1102158.9238\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990833.21357\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 678129.4827 - val_loss: 1147423.5695\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 670568.6610 - val_loss: 1168770.1710\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 97us/step - loss: 654432.7308 - val_loss: 1144902.2253\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 633845.8166 - val_loss: 1139342.6362\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990833.21357\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 612882.8385 - val_loss: 1135919.9395\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990833.21357\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 616798.9981 - val_loss: 1201189.8627\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 601136.2760 - val_loss: 1191023.8750\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 595597.9192 - val_loss: 1245202.3492\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 588441.8937 - val_loss: 1278012.5544\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990833.21357\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 566572.9125 - val_loss: 1211052.6530\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990833.21357\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 553430.7798 - val_loss: 1239184.7511\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 549089.5713 - val_loss: 1200369.5689\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 515736.2467 - val_loss: 1316569.3868\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 518233.7296 - val_loss: 1303633.5493\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990833.21357\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 503106.1109 - val_loss: 1301137.2965\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990833.21357\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 494611.4882 - val_loss: 1378020.5095\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 479463.5459 - val_loss: 1410661.4311\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 471190.2848 - val_loss: 1356059.0230\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 473327.0424 - val_loss: 1374667.2898\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 990833.21357\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 459594.3515 - val_loss: 1378114.7096\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 990833.21357\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 455325.8071 - val_loss: 1483667.9210\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 990833.21357\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 444823.6955 - val_loss: 1360601.9905\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 990833.21357\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 447934.4416 - val_loss: 1526328.2511\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 990833.21357\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 7s 2ms/step - loss: 2456808.3667 - val_loss: 2366110.7298\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366110.72982, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 2453062.5529 - val_loss: 2346934.2343\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366110.72982 to 2346934.23430, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 2402607.2080 - val_loss: 2140897.8464\n",
      "\n",
      "Epoch 00003: val_loss improved from 2346934.23430 to 2140897.84641, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 2036540.9082 - val_loss: 1415153.0998\n",
      "\n",
      "Epoch 00004: val_loss improved from 2140897.84641 to 1415153.09978, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 1300788.4978 - val_loss: 1428371.9484\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1415153.09978\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 1117053.3120 - val_loss: 1159986.4574\n",
      "\n",
      "Epoch 00001: val_loss improved from 1415153.09978 to 1159986.45740, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 1026144.0686 - val_loss: 1081129.9395\n",
      "\n",
      "Epoch 00002: val_loss improved from 1159986.45740 to 1081129.93946, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 980260.6923 - val_loss: 1046020.7175\n",
      "\n",
      "Epoch 00003: val_loss improved from 1081129.93946 to 1046020.71749, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 948872.9355 - val_loss: 1024199.9787\n",
      "\n",
      "Epoch 00004: val_loss improved from 1046020.71749 to 1024199.97870, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 935460.8220 - val_loss: 1020705.0790\n",
      "\n",
      "Epoch 00005: val_loss improved from 1024199.97870 to 1020705.07904, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 924389.2205 - val_loss: 1023903.4400\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 915539.3648 - val_loss: 1055418.4922\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 904554.8614 - val_loss: 1029798.0830\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 66us/step - loss: 887055.0764 - val_loss: 1045796.0420\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 868245.7984 - val_loss: 1062332.1844\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 850986.0318 - val_loss: 1058009.3436\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 832498.7592 - val_loss: 1080619.5263\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 823858.8295 - val_loss: 1083832.9938\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 800707.2538 - val_loss: 1095916.4714\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 791661.5406 - val_loss: 1115707.5348\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 776613.5291 - val_loss: 1106909.1895\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 757956.0615 - val_loss: 1123586.6732\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 742703.3594 - val_loss: 1137625.6883\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 734401.9052 - val_loss: 1193066.5880\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 709887.1940 - val_loss: 1155062.0381\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 695059.0342 - val_loss: 1188921.4507\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 687783.7943 - val_loss: 1243608.0107\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 673015.6207 - val_loss: 1264997.1104\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 651245.1463 - val_loss: 1271715.6973\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 641422.5139 - val_loss: 1280080.9742\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 639461.9284 - val_loss: 1251750.0992\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 625549.1974 - val_loss: 1330914.0056\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 608445.3599 - val_loss: 1305450.0544\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 592388.9838 - val_loss: 1374663.3313\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 599531.9424 - val_loss: 1394058.4008\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 585056.8382 - val_loss: 1508196.7612\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 557293.5820 - val_loss: 1342252.9070\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 538632.3019 - val_loss: 1424670.2063\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 542611.4285 - val_loss: 1472580.3559\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 530135.9700 - val_loss: 1609915.0533\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 65us/step - loss: 507820.5631 - val_loss: 1526191.2629\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 494793.6204 - val_loss: 1529201.3537\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 64us/step - loss: 483134.9732 - val_loss: 1548972.8985\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 491139.7647 - val_loss: 1491800.7797\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 471273.4762 - val_loss: 1676825.1945\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 451265.2037 - val_loss: 1704418.8957\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1020705.07904\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 452259.6997 - val_loss: 1550777.9546\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1020705.07904\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 449452.1150 - val_loss: 1636874.3952\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1020705.07904\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 432466.0351 - val_loss: 1685230.6340\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1020705.07904\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 421472.0551 - val_loss: 1760040.6923\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1020705.07904\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 7s 2ms/step - loss: 2456839.3034 - val_loss: 2366376.3666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366376.36659, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 2454420.4794 - val_loss: 2357512.6592\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366376.36659 to 2357512.65919, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 2423121.4889 - val_loss: 2248692.2971\n",
      "\n",
      "Epoch 00003: val_loss improved from 2357512.65919 to 2248692.29709, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 2184389.3021 - val_loss: 1653925.4652\n",
      "\n",
      "Epoch 00004: val_loss improved from 2248692.29709 to 1653925.46525, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 1431002.4134 - val_loss: 1767828.7517\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1653925.46525\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 1098038.9137 - val_loss: 1119177.6956\n",
      "\n",
      "Epoch 00001: val_loss improved from 1653925.46525 to 1119177.69563, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 1002104.7896 - val_loss: 1072144.4798\n",
      "\n",
      "Epoch 00002: val_loss improved from 1119177.69563 to 1072144.47982, saving model to ./model/BranchRCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 974956.1962 - val_loss: 1044811.3548\n",
      "\n",
      "Epoch 00003: val_loss improved from 1072144.47982 to 1044811.35482, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 971123.7345 - val_loss: 1046980.6037\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 952502.6405 - val_loss: 1051852.6463\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 941814.4864 - val_loss: 1052849.0874\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 933072.3083 - val_loss: 1053046.4075\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 926238.6570 - val_loss: 1053964.7831\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 911399.9908 - val_loss: 1048531.8206\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 902109.4220 - val_loss: 1059398.0942\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 891625.0037 - val_loss: 1059254.8189\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 871328.5270 - val_loss: 1060630.7276\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 858085.7571 - val_loss: 1059420.5863\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 839797.7401 - val_loss: 1065378.1334\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 817834.3707 - val_loss: 1071422.3985\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 802939.7511 - val_loss: 1078411.6833\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 781125.8110 - val_loss: 1078982.8587\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 767513.8060 - val_loss: 1082229.2130\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 751943.0148 - val_loss: 1098471.4983\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 726081.0603 - val_loss: 1107665.8767\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 720016.7063 - val_loss: 1132867.4339\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 705115.5518 - val_loss: 1121419.1620\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 684598.3619 - val_loss: 1133839.8772\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 667937.8691 - val_loss: 1144963.6603\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 661797.7573 - val_loss: 1191214.1110\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 645624.0180 - val_loss: 1185367.5947\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 621652.1621 - val_loss: 1156346.3498\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 620172.1539 - val_loss: 1231430.3643\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 609789.9517 - val_loss: 1223849.8767\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 592672.4560 - val_loss: 1304935.3722\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 594929.1890 - val_loss: 1291702.1306\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 570918.9611 - val_loss: 1270957.7304\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 566197.5687 - val_loss: 1267006.7567\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 562618.5792 - val_loss: 1303735.7954\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 530829.1228 - val_loss: 1432937.2735\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 534851.0471 - val_loss: 1379537.9215\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 521469.7799 - val_loss: 1361786.6693\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 503319.3971 - val_loss: 1415127.4008\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 500503.8073 - val_loss: 1444105.4944\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 505733.3975 - val_loss: 1469253.6530\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 477486.4170 - val_loss: 1434416.2180\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1044811.35482\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 463590.5336 - val_loss: 1450584.8223\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1044811.35482\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 465925.4908 - val_loss: 1507234.2864\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1044811.35482\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 458037.4415 - val_loss: 1494617.6973\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1044811.35482\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 468602.7431 - val_loss: 1544610.1003\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1044811.35482\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 2456836.0777 - val_loss: 2366336.3139\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366336.31390, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 2454512.5628 - val_loss: 2356486.7478\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366336.31390 to 2356486.74776, saving model to ./model/BranchRCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 2423868.8847 - val_loss: 2231942.4641\n",
      "\n",
      "Epoch 00003: val_loss improved from 2356486.74776 to 2231942.46413, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 2173054.9136 - val_loss: 1624800.1900\n",
      "\n",
      "Epoch 00004: val_loss improved from 2231942.46413 to 1624800.19002, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 1412148.5370 - val_loss: 2056935.8688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1624800.19002\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 1172453.7682 - val_loss: 1238864.6278\n",
      "\n",
      "Epoch 00001: val_loss improved from 1624800.19002 to 1238864.62780, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 1059149.6868 - val_loss: 1147604.9630\n",
      "\n",
      "Epoch 00002: val_loss improved from 1238864.62780 to 1147604.96300, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 1001168.8379 - val_loss: 1038424.9910\n",
      "\n",
      "Epoch 00003: val_loss improved from 1147604.96300 to 1038424.99103, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 965694.5051 - val_loss: 1010239.3352\n",
      "\n",
      "Epoch 00004: val_loss improved from 1038424.99103 to 1010239.33520, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 947905.9607 - val_loss: 994586.6357\n",
      "\n",
      "Epoch 00005: val_loss improved from 1010239.33520 to 994586.63565, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 930900.0100 - val_loss: 1000983.0544\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 917311.4294 - val_loss: 1010946.6491\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 908635.9736 - val_loss: 1023030.9955\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 887720.0589 - val_loss: 1014190.8492\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 876073.4640 - val_loss: 1007162.1900\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 856872.8650 - val_loss: 1021966.6009\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 838661.4852 - val_loss: 1013553.5006\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 820127.0438 - val_loss: 1020526.6704\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 805755.9749 - val_loss: 1022102.8038\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 792282.1450 - val_loss: 1048127.9793\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 772785.5467 - val_loss: 1039907.7494\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 754659.7100 - val_loss: 1055351.9765\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 736022.7887 - val_loss: 1057227.1138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 724931.2061 - val_loss: 1081058.4254\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 723784.1199 - val_loss: 1070424.9854\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 704224.1800 - val_loss: 1124787.3279\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 663898.4945 - val_loss: 1138081.1228\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 664749.0435 - val_loss: 1120058.8604\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 641063.8174 - val_loss: 1130387.2909\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 649746.2305 - val_loss: 1159012.3621\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 66us/step - loss: 628658.6263 - val_loss: 1157639.1973\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 626894.3052 - val_loss: 1167284.5303\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 598396.1431 - val_loss: 1156793.0678\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 592335.7509 - val_loss: 1175803.8974\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 564426.6860 - val_loss: 1162163.5729\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 558512.6186 - val_loss: 1243018.7747\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 550675.0645 - val_loss: 1288200.4047\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 541940.0274 - val_loss: 1242014.9922\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 526782.3738 - val_loss: 1300136.0252\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 518560.3216 - val_loss: 1308593.3061\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 512343.0978 - val_loss: 1259705.6183\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 497665.2321 - val_loss: 1209802.9210\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 480286.6713 - val_loss: 1361593.9961\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 468737.1172 - val_loss: 1337250.8571\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 484558.2119 - val_loss: 1281609.6407\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 68us/step - loss: 473166.2908 - val_loss: 1369235.1553\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 994586.63565\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 457320.8750 - val_loss: 1423444.7012\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 994586.63565\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 456517.7161 - val_loss: 1381746.8234\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 994586.63565\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 430186.1950 - val_loss: 1342510.7590\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 994586.63565\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 438053.1894 - val_loss: 1420610.4944\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 994586.63565\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 2456836.8510 - val_loss: 2366293.1883\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366293.18834, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 2454175.5110 - val_loss: 2355517.8173\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366293.18834 to 2355517.81726, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 2421372.2343 - val_loss: 2232136.8016\n",
      "\n",
      "Epoch 00003: val_loss improved from 2355517.81726 to 2232136.80157, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 2187620.3917 - val_loss: 1623600.2730\n",
      "\n",
      "Epoch 00004: val_loss improved from 2232136.80157 to 1623600.27298, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 1444108.9178 - val_loss: 1489010.9882\n",
      "\n",
      "Epoch 00005: val_loss improved from 1623600.27298 to 1489010.98823, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 1082559.9174 - val_loss: 1152683.2836\n",
      "\n",
      "Epoch 00001: val_loss improved from 1489010.98823 to 1152683.28363, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 1009424.1039 - val_loss: 1113255.5510\n",
      "\n",
      "Epoch 00002: val_loss improved from 1152683.28363 to 1113255.55101, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 991726.2352 - val_loss: 1061606.6345\n",
      "\n",
      "Epoch 00003: val_loss improved from 1113255.55101 to 1061606.63453, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 976505.9287 - val_loss: 1045871.8599\n",
      "\n",
      "Epoch 00004: val_loss improved from 1061606.63453 to 1045871.85987, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 963143.5369 - val_loss: 1031528.9731\n",
      "\n",
      "Epoch 00005: val_loss improved from 1045871.85987 to 1031528.97309, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 949551.6964 - val_loss: 1030607.2337\n",
      "\n",
      "Epoch 00001: val_loss improved from 1031528.97309 to 1030607.23374, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 932495.8684 - val_loss: 1032964.8711\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030607.23374\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 923178.1963 - val_loss: 1029629.7422\n",
      "\n",
      "Epoch 00003: val_loss improved from 1030607.23374 to 1029629.74215, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 906671.0788 - val_loss: 1037834.5073\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 887418.8572 - val_loss: 1044829.2450\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 866001.8786 - val_loss: 1047690.2769\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1029629.74215\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 843061.4220 - val_loss: 1053824.0852\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1029629.74215\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 825190.2780 - val_loss: 1055160.1934\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1029629.74215\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 806158.5162 - val_loss: 1078158.6110\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 780939.9963 - val_loss: 1074663.4025\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 760367.6587 - val_loss: 1101057.6401\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1029629.74215\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 748752.3388 - val_loss: 1089890.7063\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1029629.74215\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 722578.2699 - val_loss: 1106066.9479\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1029629.74215\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 704279.2911 - val_loss: 1125130.2556\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 685073.6315 - val_loss: 1118233.7214\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 659662.9996 - val_loss: 1124368.5017\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1029629.74215\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 638601.9383 - val_loss: 1138711.2926\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1029629.74215\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 641804.8236 - val_loss: 1180717.3582\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1029629.74215\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 620851.4322 - val_loss: 1178461.4339\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 596770.5915 - val_loss: 1211965.4860\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 573278.5567 - val_loss: 1205864.2965\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1029629.74215\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 572061.1584 - val_loss: 1252861.6687\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1029629.74215\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 550071.1712 - val_loss: 1247880.3049\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1029629.74215\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 67us/step - loss: 531512.3718 - val_loss: 1260946.1312\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 516216.3037 - val_loss: 1310036.5078\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 491728.0976 - val_loss: 1378273.2270\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1029629.74215\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 497818.7215 - val_loss: 1363523.9703\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1029629.74215\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 70us/step - loss: 482499.0631 - val_loss: 1390201.1979\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1029629.74215\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 470374.4159 - val_loss: 1350929.9546\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 455915.6812 - val_loss: 1458747.1110\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 437718.3076 - val_loss: 1449734.0364\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1029629.74215\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 424032.7549 - val_loss: 1400909.4826\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1029629.74215\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 423018.4281 - val_loss: 1524152.6233\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1029629.74215\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 422336.0574 - val_loss: 1508619.4815\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 408833.7937 - val_loss: 1414884.2976\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 391217.8392 - val_loss: 1555322.1872\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1029629.74215\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 385314.7726 - val_loss: 1520477.5706\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1029629.74215\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 372990.0291 - val_loss: 1525325.6984\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1029629.74215\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 359409.7054 - val_loss: 1529654.1805\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1029629.74215\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 349147.3696 - val_loss: 1541127.5936\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1029629.74215\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 9s 2ms/step - loss: 2456840.4799 - val_loss: 2366329.3744\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366329.37444, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 2454507.0967 - val_loss: 2355972.4843\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366329.37444 to 2355972.48430, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 2424149.5275 - val_loss: 2229229.8027\n",
      "\n",
      "Epoch 00003: val_loss improved from 2355972.48430 to 2229229.80269, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 2186425.0797 - val_loss: 1596854.5930\n",
      "\n",
      "Epoch 00004: val_loss improved from 2229229.80269 to 1596854.59305, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 1446733.7740 - val_loss: 1868555.3834\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1596854.59305\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 1143897.7544 - val_loss: 1243015.6844\n",
      "\n",
      "Epoch 00001: val_loss improved from 1596854.59305 to 1243015.68442, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 1037270.4212 - val_loss: 1076878.0779\n",
      "\n",
      "Epoch 00002: val_loss improved from 1243015.68442 to 1076878.07791, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 982387.4594 - val_loss: 1032734.4064\n",
      "\n",
      "Epoch 00003: val_loss improved from 1076878.07791 to 1032734.40639, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 964226.3489 - val_loss: 999364.3503\n",
      "\n",
      "Epoch 00004: val_loss improved from 1032734.40639 to 999364.35034, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 945157.5113 - val_loss: 997292.1996\n",
      "\n",
      "Epoch 00005: val_loss improved from 999364.35034 to 997292.19955, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 934214.4652 - val_loss: 1005745.3985\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 913175.0822 - val_loss: 1000323.1446\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 902986.2237 - val_loss: 1006358.5942\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 881308.2440 - val_loss: 1007749.5129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 868921.4778 - val_loss: 1012583.5818\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 848052.0726 - val_loss: 1019852.9849\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 832827.2043 - val_loss: 1024758.6250\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 814156.8197 - val_loss: 1032235.9557\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 798571.4479 - val_loss: 1039704.2735\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 775082.4171 - val_loss: 1051190.3739\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 749093.4074 - val_loss: 1067758.8688\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 728396.8721 - val_loss: 1058199.3419\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 718003.0144 - val_loss: 1084310.4798\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 696638.0075 - val_loss: 1084830.2500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 682286.5450 - val_loss: 1108708.5095\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 666270.5889 - val_loss: 1144375.9395\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 653784.5849 - val_loss: 1140854.0291\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 638726.9706 - val_loss: 1179839.4070\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 651732.9220 - val_loss: 1207593.3946\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 612673.8027 - val_loss: 1179061.3901\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 73us/step - loss: 601129.6634 - val_loss: 1153929.3503\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 598969.4160 - val_loss: 1152952.4882\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 587792.6615 - val_loss: 1171595.5275\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 567872.5066 - val_loss: 1220322.2461\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 546847.7472 - val_loss: 1224918.5348\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 543529.7747 - val_loss: 1286889.7298\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 529407.5826 - val_loss: 1236604.9182\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 515214.2476 - val_loss: 1289817.4311\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 68us/step - loss: 505408.0166 - val_loss: 1328764.7640\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 496114.6629 - val_loss: 1365400.1626\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 479854.6586 - val_loss: 1328550.8049\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 468893.9234 - val_loss: 1377985.0090\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 471194.1935 - val_loss: 1313175.9109\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 462215.5650 - val_loss: 1453100.1172\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 460286.6703 - val_loss: 1501639.3122\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 458771.6036 - val_loss: 1410728.3307\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 997292.19955\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 427631.1809 - val_loss: 1452866.8481\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 997292.19955\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 425422.9347 - val_loss: 1372423.3901\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 997292.19955\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 434423.4935 - val_loss: 1483797.4395\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 997292.19955\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 415055.2252 - val_loss: 1417635.9933\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 997292.19955\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 9s 2ms/step - loss: 2456866.7684 - val_loss: 2366440.8756\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366440.87556, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 2454978.9936 - val_loss: 2358747.8016\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366440.87556 to 2358747.80157, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 2428870.9758 - val_loss: 2253666.1670\n",
      "\n",
      "Epoch 00003: val_loss improved from 2358747.80157 to 2253666.16704, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 2213538.4178 - val_loss: 1661024.8823\n",
      "\n",
      "Epoch 00004: val_loss improved from 2253666.16704 to 1661024.88229, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 1476980.4384 - val_loss: 1571542.9456\n",
      "\n",
      "Epoch 00005: val_loss improved from 1661024.88229 to 1571542.94563, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 1148735.5374 - val_loss: 1199013.4064\n",
      "\n",
      "Epoch 00001: val_loss improved from 1571542.94563 to 1199013.40639, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 1059809.7870 - val_loss: 1132724.3543\n",
      "\n",
      "Epoch 00002: val_loss improved from 1199013.40639 to 1132724.35426, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 1004875.5866 - val_loss: 1071147.0504\n",
      "\n",
      "Epoch 00003: val_loss improved from 1132724.35426 to 1071147.05045, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 974020.2291 - val_loss: 1029942.3565\n",
      "\n",
      "Epoch 00004: val_loss improved from 1071147.05045 to 1029942.35650, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 954595.5529 - val_loss: 1020057.1620\n",
      "\n",
      "Epoch 00005: val_loss improved from 1029942.35650 to 1020057.16200, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 938990.9246 - val_loss: 1014941.1054\n",
      "\n",
      "Epoch 00001: val_loss improved from 1020057.16200 to 1014941.10538, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 929906.8618 - val_loss: 1016912.2685\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 913409.9601 - val_loss: 1021621.0353\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 888840.6809 - val_loss: 1030979.7920\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 883778.8312 - val_loss: 1037170.2836\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 865004.7795 - val_loss: 1040425.6267\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014941.10538\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 837558.7557 - val_loss: 1051042.2304\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 831474.9264 - val_loss: 1037733.2203\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 804698.0972 - val_loss: 1067724.2534\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 784585.9728 - val_loss: 1071366.2965\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 749132.0206 - val_loss: 1061750.7242\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014941.10538\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 730733.8248 - val_loss: 1072862.1143\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 724614.2097 - val_loss: 1063075.0757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 712467.8970 - val_loss: 1097975.7478\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 703493.5496 - val_loss: 1110699.2668\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 688285.5726 - val_loss: 1095969.6474\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014941.10538\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 667571.6006 - val_loss: 1109828.8845\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 656962.6153 - val_loss: 1123704.0953\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 641612.3926 - val_loss: 1165642.7438\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 631343.0916 - val_loss: 1152947.8666\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 623117.1467 - val_loss: 1152595.0645\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014941.10538\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 614907.7977 - val_loss: 1212307.7763\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 612273.0433 - val_loss: 1247567.2203\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 588152.0001 - val_loss: 1242769.2685\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 565919.9040 - val_loss: 1241162.2349\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 561062.2696 - val_loss: 1355703.5813\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014941.10538\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 549361.9239 - val_loss: 1319704.7646\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 550205.2527 - val_loss: 1356953.2438\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 548548.4180 - val_loss: 1330902.2618\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 536027.9189 - val_loss: 1366229.3666\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 517435.6964 - val_loss: 1375642.9877\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014941.10538\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 500893.7148 - val_loss: 1442737.9210\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 503411.1812 - val_loss: 1429629.1317\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 69us/step - loss: 489702.5709 - val_loss: 1384777.5555\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 483827.3065 - val_loss: 1379148.9596\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 487280.2592 - val_loss: 1390516.3941\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014941.10538\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 472720.0948 - val_loss: 1383066.0291\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014941.10538\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 457756.7237 - val_loss: 1402938.5650\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014941.10538\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 471450.4093 - val_loss: 1444643.0123\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014941.10538\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 455544.0854 - val_loss: 1504394.0572\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014941.10538\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 10s 2ms/step - loss: 2456846.6598 - val_loss: 2366347.3374\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366347.33744, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 84us/step - loss: 2454568.8757 - val_loss: 2356733.5527\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366347.33744 to 2356733.55269, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 2422804.3505 - val_loss: 2234724.6951\n",
      "\n",
      "Epoch 00003: val_loss improved from 2356733.55269 to 2234724.69507, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 2178012.1730 - val_loss: 1620643.0258\n",
      "\n",
      "Epoch 00004: val_loss improved from 2234724.69507 to 1620643.02578, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 1457272.1341 - val_loss: 1928120.9647\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1620643.02578\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 1174040.6086 - val_loss: 1314407.2343\n",
      "\n",
      "Epoch 00001: val_loss improved from 1620643.02578 to 1314407.23430, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 1081051.4893 - val_loss: 1202188.0762\n",
      "\n",
      "Epoch 00002: val_loss improved from 1314407.23430 to 1202188.07623, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 1016190.2067 - val_loss: 1076089.0947\n",
      "\n",
      "Epoch 00003: val_loss improved from 1202188.07623 to 1076089.09473, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 978404.8261 - val_loss: 1039892.8515\n",
      "\n",
      "Epoch 00004: val_loss improved from 1076089.09473 to 1039892.85146, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 963534.6191 - val_loss: 1030289.5291\n",
      "\n",
      "Epoch 00005: val_loss improved from 1039892.85146 to 1030289.52915, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 950830.9477 - val_loss: 1039464.2040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030289.52915\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 937708.6363 - val_loss: 1023956.5426\n",
      "\n",
      "Epoch 00002: val_loss improved from 1030289.52915 to 1023956.54260, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 917109.8047 - val_loss: 1018602.8828\n",
      "\n",
      "Epoch 00003: val_loss improved from 1023956.54260 to 1018602.88285, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 905502.5459 - val_loss: 1017075.9451\n",
      "\n",
      "Epoch 00004: val_loss improved from 1018602.88285 to 1017075.94507, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 888867.8574 - val_loss: 1032165.2304\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1017075.94507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 867617.4112 - val_loss: 1030103.5897\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1017075.94507\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 853101.8909 - val_loss: 1006939.2029\n",
      "\n",
      "Epoch 00002: val_loss improved from 1017075.94507 to 1006939.20291, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 830896.3950 - val_loss: 1018789.7377\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1006939.20291\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 819370.7389 - val_loss: 1034320.0471\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1006939.20291\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 797887.6932 - val_loss: 1026740.3335\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1006939.20291\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 783323.8888 - val_loss: 1048920.2528\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1006939.20291\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 761326.1651 - val_loss: 1066903.1284\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1006939.20291\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 743938.2387 - val_loss: 1059671.1648\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1006939.20291\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 717393.4426 - val_loss: 1108341.8599\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1006939.20291\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 705942.5203 - val_loss: 1094054.4221\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1006939.20291\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 691804.1503 - val_loss: 1138315.6491\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1006939.20291\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 679182.6818 - val_loss: 1119685.4624\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1006939.20291\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 674383.5087 - val_loss: 1137200.7057\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1006939.20291\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 654539.7111 - val_loss: 1147616.2657\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1006939.20291\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 643466.3871 - val_loss: 1242118.3492\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1006939.20291\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 629706.4824 - val_loss: 1315563.1306\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1006939.20291\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 629265.5195 - val_loss: 1267167.7730\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1006939.20291\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 608222.8052 - val_loss: 1313749.9574\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1006939.20291\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 585115.6640 - val_loss: 1317423.2365\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1006939.20291\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 563485.8622 - val_loss: 1383584.2209\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1006939.20291\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 557079.7615 - val_loss: 1404306.3027\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1006939.20291\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 547979.8395 - val_loss: 1407093.4563\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1006939.20291\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 554203.0132 - val_loss: 1365179.3386\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1006939.20291\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 533814.4360 - val_loss: 1381999.8492\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1006939.20291\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 521428.3076 - val_loss: 1525925.0947\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1006939.20291\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 509077.0558 - val_loss: 1472180.2164\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1006939.20291\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 484064.7666 - val_loss: 1436463.0185\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1006939.20291\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 468551.1822 - val_loss: 1587942.5936\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1006939.20291\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 462930.6920 - val_loss: 1521585.6967\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1006939.20291\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 463313.0904 - val_loss: 1404624.7926\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1006939.20291\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 450912.7990 - val_loss: 1675180.7601\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1006939.20291\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 437822.1869 - val_loss: 1648555.4243\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1006939.20291\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 424683.3202 - val_loss: 1522613.6855\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1006939.20291\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 425236.9370 - val_loss: 1594145.0516\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1006939.20291\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 418516.1193 - val_loss: 1702993.9899\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1006939.20291\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 10s 3ms/step - loss: 2456812.8000 - val_loss: 2366190.2130\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366190.21300, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 86us/step - loss: 2453889.5644 - val_loss: 2352997.1883\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366190.21300 to 2352997.18834, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 2417572.2763 - val_loss: 2204358.0605\n",
      "\n",
      "Epoch 00003: val_loss improved from 2352997.18834 to 2204358.06054, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 2159889.1713 - val_loss: 1542062.9008\n",
      "\n",
      "Epoch 00004: val_loss improved from 2204358.06054 to 1542062.90078, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 1396150.9377 - val_loss: 1709370.8576\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1542062.90078\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 1137000.3281 - val_loss: 1174198.2904\n",
      "\n",
      "Epoch 00001: val_loss improved from 1542062.90078 to 1174198.29036, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 1042540.9898 - val_loss: 1094934.5891\n",
      "\n",
      "Epoch 00002: val_loss improved from 1174198.29036 to 1094934.58913, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 981861.7637 - val_loss: 1052659.4798\n",
      "\n",
      "Epoch 00003: val_loss improved from 1094934.58913 to 1052659.47982, saving model to ./model/BranchRCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 962997.8829 - val_loss: 1042373.9417\n",
      "\n",
      "Epoch 00004: val_loss improved from 1052659.47982 to 1042373.94170, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 943661.7593 - val_loss: 1043513.1003\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 927110.0823 - val_loss: 1055323.7130\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 912675.0216 - val_loss: 1056449.9910\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 899188.9666 - val_loss: 1049067.3997\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 887681.5345 - val_loss: 1052236.7820\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 873980.1826 - val_loss: 1060704.0925\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 857752.5234 - val_loss: 1057881.4871\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 830253.4759 - val_loss: 1051750.1564\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 816575.2415 - val_loss: 1052458.4568\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 789123.0618 - val_loss: 1091398.6553\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 785362.0093 - val_loss: 1095934.4367\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 760886.2020 - val_loss: 1087956.2158\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 764641.1287 - val_loss: 1075438.7068\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 741259.2784 - val_loss: 1078548.9899\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 711392.6851 - val_loss: 1071248.3464\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 686871.3769 - val_loss: 1114208.0062\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 662882.1695 - val_loss: 1117994.0622\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 664772.1645 - val_loss: 1182538.2904\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 656724.4253 - val_loss: 1130675.1104\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 622420.1603 - val_loss: 1167688.8638\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 611760.5708 - val_loss: 1188178.6306\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 587791.6001 - val_loss: 1211733.8038\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 574718.1577 - val_loss: 1216228.3234\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 575717.0310 - val_loss: 1198795.6654\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 546375.7187 - val_loss: 1282094.2657\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 548652.6270 - val_loss: 1383977.9552\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 535469.8326 - val_loss: 1306731.7791\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 527704.3393 - val_loss: 1393992.6059\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 509680.3191 - val_loss: 1347845.7214\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 507120.2991 - val_loss: 1438654.2517\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 490895.5680 - val_loss: 1513535.0880\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 485372.8208 - val_loss: 1457273.5919\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 460825.6235 - val_loss: 1435600.7663\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 476044.7394 - val_loss: 1498008.4720\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 456436.9610 - val_loss: 1614334.5006\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 445243.0040 - val_loss: 1587001.0275\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 434908.6192 - val_loss: 1528564.0841\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1042373.94170\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 420374.5484 - val_loss: 1557994.3851\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042373.94170\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 426243.2744 - val_loss: 1478051.6357\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042373.94170\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 70us/step - loss: 425401.0465 - val_loss: 1579918.8346\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042373.94170\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 71us/step - loss: 396335.5386 - val_loss: 1657155.2836\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1042373.94170\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 11s 3ms/step - loss: 2456822.1589 - val_loss: 2366211.9877\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366211.98767, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 84us/step - loss: 2453970.9296 - val_loss: 2353048.6312\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366211.98767 to 2353048.63117, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 2415987.1601 - val_loss: 2194171.6592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 2353048.63117 to 2194171.65919, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 2130835.0845 - val_loss: 1480343.1160\n",
      "\n",
      "Epoch 00004: val_loss improved from 2194171.65919 to 1480343.11603, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 1388404.9144 - val_loss: 1784124.8358\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1480343.11603\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 1154821.3557 - val_loss: 1212569.8352\n",
      "\n",
      "Epoch 00001: val_loss improved from 1480343.11603 to 1212569.83520, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 1059825.8700 - val_loss: 1132872.3985\n",
      "\n",
      "Epoch 00002: val_loss improved from 1212569.83520 to 1132872.39854, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 995934.3212 - val_loss: 1051426.5970\n",
      "\n",
      "Epoch 00003: val_loss improved from 1132872.39854 to 1051426.59697, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 970437.0423 - val_loss: 1030346.6295\n",
      "\n",
      "Epoch 00004: val_loss improved from 1051426.59697 to 1030346.62948, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 958381.7518 - val_loss: 1041879.3632\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 940996.3738 - val_loss: 1049597.2797\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 928318.2192 - val_loss: 1054949.4098\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 925564.0179 - val_loss: 1066749.0813\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 911385.3987 - val_loss: 1062641.8122\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 896893.4079 - val_loss: 1068104.3924\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 883612.2538 - val_loss: 1088281.0583\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 878428.0885 - val_loss: 1072877.1093\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 853857.1806 - val_loss: 1087396.9854\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 838296.4775 - val_loss: 1083748.7988\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 821246.9238 - val_loss: 1095467.5987\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 806226.4286 - val_loss: 1096132.8784\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 790665.6582 - val_loss: 1109635.7489\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 761330.9466 - val_loss: 1133710.6794\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 751239.0962 - val_loss: 1132759.5476\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 741531.6027 - val_loss: 1155263.3548\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 721472.2801 - val_loss: 1218312.3122\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 707733.3083 - val_loss: 1201421.8234\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 682858.4483 - val_loss: 1246889.9154\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 666646.7846 - val_loss: 1225207.1261\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 655132.1920 - val_loss: 1281605.0729\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 641519.6067 - val_loss: 1306525.7607\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 621843.1773 - val_loss: 1264178.5056\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 617709.2186 - val_loss: 1342213.8117\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 588510.6385 - val_loss: 1393870.7724\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 586804.2408 - val_loss: 1414795.4047\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 565253.1432 - val_loss: 1438234.9086\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 566122.6380 - val_loss: 1534617.6357\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 552282.3212 - val_loss: 1517136.8660\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 534843.9335 - val_loss: 1488301.4703\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 526855.1731 - val_loss: 1498163.2304\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 517272.1967 - val_loss: 1576160.2719\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 514293.0652 - val_loss: 1573234.5202\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 499163.0538 - val_loss: 1601319.4596\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 476445.4647 - val_loss: 1581852.0376\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 459756.8189 - val_loss: 1618182.4770\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 449157.9849 - val_loss: 1574308.6743\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1030346.62948\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 75us/step - loss: 468214.0115 - val_loss: 1689092.9355\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1030346.62948\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 444587.0614 - val_loss: 1683241.2063\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1030346.62948\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 431810.0223 - val_loss: 1728545.6536\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1030346.62948\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 72us/step - loss: 414361.3017 - val_loss: 1709049.5348\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1030346.62948\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 11s 3ms/step - loss: 2456817.4210 - val_loss: 2366193.2085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366193.20852, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 88us/step - loss: 2453602.9375 - val_loss: 2352834.6256\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366193.20852 to 2352834.62556, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 2410456.3284 - val_loss: 2174833.7601\n",
      "\n",
      "Epoch 00003: val_loss improved from 2352834.62556 to 2174833.76009, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 2073434.6422 - val_loss: 1393104.5555\n",
      "\n",
      "Epoch 00004: val_loss improved from 2174833.76009 to 1393104.55549, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 1323456.9129 - val_loss: 1597884.8430\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1393104.55549\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 1133886.2233 - val_loss: 1188321.8425\n",
      "\n",
      "Epoch 00001: val_loss improved from 1393104.55549 to 1188321.84249, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 1053935.2690 - val_loss: 1108323.3924\n",
      "\n",
      "Epoch 00002: val_loss improved from 1188321.84249 to 1108323.39238, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 1009940.4366 - val_loss: 1052612.9288\n",
      "\n",
      "Epoch 00003: val_loss improved from 1108323.39238 to 1052612.92881, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 977767.2908 - val_loss: 1019076.9277\n",
      "\n",
      "Epoch 00004: val_loss improved from 1052612.92881 to 1019076.92769, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 960521.3932 - val_loss: 1027634.3834\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 943610.5157 - val_loss: 1023663.5942\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 933513.1364 - val_loss: 1033719.5224\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 916289.8766 - val_loss: 1022004.3503\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 901479.3823 - val_loss: 1033847.2618\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 878514.8693 - val_loss: 1042430.1390\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 869098.7937 - val_loss: 1049781.7057\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 855940.7597 - val_loss: 1047112.5163\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 831447.3871 - val_loss: 1063391.1357\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 805725.5798 - val_loss: 1055519.9950\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 795744.5514 - val_loss: 1086328.6015\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 773898.4384 - val_loss: 1081922.0034\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 762333.3904 - val_loss: 1094546.5645\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 748234.0199 - val_loss: 1101039.1278\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 731529.6310 - val_loss: 1116179.8459\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 717332.8224 - val_loss: 1150698.1390\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 690912.9151 - val_loss: 1163366.2584\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 688061.1651 - val_loss: 1181330.4983\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 678760.0621 - val_loss: 1199754.6973\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 663429.0771 - val_loss: 1232056.4989\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 655258.7647 - val_loss: 1240683.9602\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 634949.8744 - val_loss: 1236832.2052\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 614950.7810 - val_loss: 1308986.2147\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 608929.7268 - val_loss: 1362655.4109\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 594907.7032 - val_loss: 1304674.6340\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 588104.1148 - val_loss: 1354759.3302\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 579676.5090 - val_loss: 1325443.4574\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 564493.4340 - val_loss: 1360013.4187\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 557028.9538 - val_loss: 1424383.2203\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 550344.0783 - val_loss: 1451781.1850\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 75us/step - loss: 550287.9227 - val_loss: 1536012.0376\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 514529.4723 - val_loss: 1557190.2528\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 521135.3737 - val_loss: 1319415.3318\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 515205.3450 - val_loss: 1620879.9770\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 504171.5989 - val_loss: 1533739.6048\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 496053.0214 - val_loss: 1505698.7982\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 493319.0762 - val_loss: 1435644.8913\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1019076.92769\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 477637.1131 - val_loss: 1530232.3229\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1019076.92769\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 468478.5228 - val_loss: 1489596.5555\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1019076.92769\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 448211.4944 - val_loss: 1611735.8004\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1019076.92769\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 443836.4365 - val_loss: 1588707.1368\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1019076.92769\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 12s 3ms/step - loss: 2456850.1971 - val_loss: 2366377.0448\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366377.04484, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 84us/step - loss: 2454829.1414 - val_loss: 2357722.8756\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366377.04484 to 2357722.87556, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 2426319.8687 - val_loss: 2242296.2623\n",
      "\n",
      "Epoch 00003: val_loss improved from 2357722.87556 to 2242296.26233, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 2188827.4578 - val_loss: 1577844.3924\n",
      "\n",
      "Epoch 00004: val_loss improved from 2242296.26233 to 1577844.39238, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 1421613.9048 - val_loss: 1469762.2119\n",
      "\n",
      "Epoch 00005: val_loss improved from 1577844.39238 to 1469762.21188, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 1124675.7580 - val_loss: 1051307.1928\n",
      "\n",
      "Epoch 00001: val_loss improved from 1469762.21188 to 1051307.19283, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 1018925.6874 - val_loss: 1009974.5404\n",
      "\n",
      "Epoch 00002: val_loss improved from 1051307.19283 to 1009974.54036, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 981477.3446 - val_loss: 1012591.7685\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 962254.5857 - val_loss: 1011509.1048\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 949649.1610 - val_loss: 1012730.3442\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 942313.2311 - val_loss: 1017605.3565\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 923821.4689 - val_loss: 1021486.7321\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 913154.8328 - val_loss: 1021870.1586\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 903236.2179 - val_loss: 1032475.2562\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 890957.3568 - val_loss: 1040160.7651\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 870276.6203 - val_loss: 1041985.7769\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 851515.0027 - val_loss: 1039229.1026\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 839906.4526 - val_loss: 1054433.0863\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 814465.3740 - val_loss: 1049120.5717\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 800542.7934 - val_loss: 1069953.4226\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 773532.1544 - val_loss: 1071678.7321\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 762900.5599 - val_loss: 1103631.1889\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 748279.2071 - val_loss: 1100051.6676\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 742532.6203 - val_loss: 1099464.3369\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 722653.3691 - val_loss: 1129541.6065\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 700555.3933 - val_loss: 1109426.9193\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 679393.3226 - val_loss: 1124732.8386\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 684692.0832 - val_loss: 1106268.3643\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 660035.1878 - val_loss: 1152508.7164\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 642069.7279 - val_loss: 1184620.2741\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 637397.6947 - val_loss: 1216417.0633\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 618433.7604 - val_loss: 1158350.6211\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 76us/step - loss: 606968.5891 - val_loss: 1237524.4271\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 609398.6939 - val_loss: 1189420.1642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 597804.3779 - val_loss: 1262418.2466\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 571124.9317 - val_loss: 1308682.1228\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 570987.4302 - val_loss: 1239472.8453\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 554121.4743 - val_loss: 1320582.7629\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 536258.0716 - val_loss: 1271581.2601\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 528154.2728 - val_loss: 1312415.5022\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 519947.9732 - val_loss: 1375892.3100\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 499774.6974 - val_loss: 1572632.9675\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 507254.9125 - val_loss: 1333874.0740\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 478546.6686 - val_loss: 1409435.7024\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 461513.7534 - val_loss: 1414951.1592\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 453753.5108 - val_loss: 1542142.4249\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1009974.54036\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 453438.0270 - val_loss: 1383357.9378\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1009974.54036\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 442379.7341 - val_loss: 1508290.0667\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1009974.54036\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 438216.6405 - val_loss: 1632584.9154\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1009974.54036\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 429786.8636 - val_loss: 1602655.3061\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1009974.54036\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 13s 3ms/step - loss: 2456848.5157 - val_loss: 2366328.4070\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366328.40695, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 87us/step - loss: 2454335.6567 - val_loss: 2354035.8442\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366328.40695 to 2354035.84417, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 84us/step - loss: 2418893.4570 - val_loss: 2187456.5000\n",
      "\n",
      "Epoch 00003: val_loss improved from 2354035.84417 to 2187456.50000, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 2131895.5726 - val_loss: 1507569.6469\n",
      "\n",
      "Epoch 00004: val_loss improved from 2187456.50000 to 1507569.64686, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 1392713.6760 - val_loss: 1920967.6099\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1507569.64686\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 1173576.4314 - val_loss: 1209114.8313\n",
      "\n",
      "Epoch 00001: val_loss improved from 1507569.64686 to 1209114.83128, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 1077020.2449 - val_loss: 1116114.2164\n",
      "\n",
      "Epoch 00002: val_loss improved from 1209114.83128 to 1116114.21637, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 1011497.0608 - val_loss: 1050262.3593\n",
      "\n",
      "Epoch 00003: val_loss improved from 1116114.21637 to 1050262.35930, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 974664.9357 - val_loss: 1018022.5448\n",
      "\n",
      "Epoch 00004: val_loss improved from 1050262.35930 to 1018022.54484, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 956945.7586 - val_loss: 1015152.8543\n",
      "\n",
      "Epoch 00005: val_loss improved from 1018022.54484 to 1015152.85426, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 937329.6712 - val_loss: 1012781.2517\n",
      "\n",
      "Epoch 00001: val_loss improved from 1015152.85426 to 1012781.25168, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 918874.2764 - val_loss: 1024893.1138\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 903759.9714 - val_loss: 1018653.8335\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 886858.7296 - val_loss: 1027692.8929\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 873267.0536 - val_loss: 1038652.7511\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 860295.4960 - val_loss: 1036863.5432\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012781.25168\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 849053.6113 - val_loss: 1051705.6743\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 819167.2980 - val_loss: 1064874.2450\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 799980.7644 - val_loss: 1059591.7758\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 777656.3238 - val_loss: 1090754.7074\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 777860.3043 - val_loss: 1073557.6491\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012781.25168\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 747809.3071 - val_loss: 1105200.8795\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 729701.5498 - val_loss: 1103509.3324\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 726525.6089 - val_loss: 1133070.4058\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 711228.3378 - val_loss: 1130840.9905\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 682030.0659 - val_loss: 1153772.7668\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012781.25168\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 673259.2559 - val_loss: 1170922.8212\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 654927.8277 - val_loss: 1255775.4025\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 656153.5564 - val_loss: 1209503.0045\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 623018.6927 - val_loss: 1257975.2164\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 621544.0946 - val_loss: 1312763.2668\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012781.25168\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 598765.7334 - val_loss: 1271408.8453\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 578138.3056 - val_loss: 1388522.7775\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 583188.9898 - val_loss: 1396037.0045\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 567368.3879 - val_loss: 1283680.4176\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 549067.2392 - val_loss: 1351520.4137\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012781.25168\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 536418.3972 - val_loss: 1439580.1923\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 523629.1785 - val_loss: 1400767.7607\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 529952.5605 - val_loss: 1418308.8038\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 503158.5133 - val_loss: 1444966.6592\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 502264.9735 - val_loss: 1435918.9277\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012781.25168\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 491740.1680 - val_loss: 1676332.5561\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 73us/step - loss: 469857.2420 - val_loss: 1492056.1642\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 477748.4206 - val_loss: 1445164.7438\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 74us/step - loss: 461772.8526 - val_loss: 1451055.7511\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 75us/step - loss: 463296.1196 - val_loss: 1597026.2915\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1012781.25168\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 444301.8792 - val_loss: 1487472.2769\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1012781.25168\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 431822.4181 - val_loss: 1626993.7354\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1012781.25168\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 430479.8542 - val_loss: 1595389.3313\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1012781.25168\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 414183.1986 - val_loss: 1729093.3980\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1012781.25168\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 14s 3ms/step - loss: 2456784.3538 - val_loss: 2366085.6390\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366085.63901, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 86us/step - loss: 2453108.2972 - val_loss: 2350359.6525\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366085.63901 to 2350359.65247, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 2406087.4601 - val_loss: 2165370.0796\n",
      "\n",
      "Epoch 00003: val_loss improved from 2350359.65247 to 2165370.07960, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 2075408.4012 - val_loss: 1435435.8425\n",
      "\n",
      "Epoch 00004: val_loss improved from 2165370.07960 to 1435435.84249, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 1301161.4294 - val_loss: 1754957.8997\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1435435.84249\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 1137075.1824 - val_loss: 1176702.2887\n",
      "\n",
      "Epoch 00001: val_loss improved from 1435435.84249 to 1176702.28868, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 1036123.8442 - val_loss: 1122691.7859\n",
      "\n",
      "Epoch 00002: val_loss improved from 1176702.28868 to 1122691.78587, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 985593.1670 - val_loss: 1068186.0353\n",
      "\n",
      "Epoch 00003: val_loss improved from 1122691.78587 to 1068186.03531, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 955835.0092 - val_loss: 1043284.7651\n",
      "\n",
      "Epoch 00004: val_loss improved from 1068186.03531 to 1043284.76513, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 937654.9768 - val_loss: 1040691.7965\n",
      "\n",
      "Epoch 00005: val_loss improved from 1043284.76513 to 1040691.79652, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 927220.3641 - val_loss: 1046651.1357\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 910841.9401 - val_loss: 1050813.8229\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 905740.8352 - val_loss: 1057339.1726\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 872215.1456 - val_loss: 1060650.0919\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 859115.7147 - val_loss: 1069164.5841\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 838948.4356 - val_loss: 1067023.2881\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 819887.8007 - val_loss: 1075998.8257\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 807071.7995 - val_loss: 1077336.8492\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 79us/step - loss: 790816.1971 - val_loss: 1088741.5090\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 775923.8380 - val_loss: 1087970.6228\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 759112.0752 - val_loss: 1103693.0107\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 750279.1561 - val_loss: 1095867.1385\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 734070.9050 - val_loss: 1096756.8851\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 721562.8704 - val_loss: 1110811.7993\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 705757.7408 - val_loss: 1119694.0835\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 692904.2351 - val_loss: 1161270.7842\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 678720.1467 - val_loss: 1146551.3055\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 671998.1442 - val_loss: 1151717.2360\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 660735.9586 - val_loss: 1147225.0325\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 657401.4244 - val_loss: 1220297.1300\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 627332.6996 - val_loss: 1235706.5006\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 608443.8463 - val_loss: 1205234.1984\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 611396.7941 - val_loss: 1234650.4013\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 606046.0816 - val_loss: 1272751.4114\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 594673.5960 - val_loss: 1271911.5930\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 564773.0718 - val_loss: 1307128.4647\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 554379.2540 - val_loss: 1276937.2702\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 543908.1893 - val_loss: 1295729.7147\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 76us/step - loss: 551138.9829 - val_loss: 1299923.8918\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 546595.7348 - val_loss: 1377370.6312\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 529995.4683 - val_loss: 1381225.0465\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 535361.3990 - val_loss: 1384159.6357\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 520985.3685 - val_loss: 1300875.0359\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 516941.5888 - val_loss: 1404629.9361\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 492906.6419 - val_loss: 1329817.1783\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 480389.0463 - val_loss: 1383932.5818\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1040691.79652\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 475343.2529 - val_loss: 1355343.7180\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1040691.79652\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 466328.7103 - val_loss: 1428168.3481\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1040691.79652\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 465966.1025 - val_loss: 1376733.9972\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1040691.79652\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 458915.9979 - val_loss: 1542943.3649\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1040691.79652\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 15s 4ms/step - loss: 2456855.7996 - val_loss: 2366332.5471\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366332.54709, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 89us/step - loss: 2454397.6823 - val_loss: 2353461.1906\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366332.54709 to 2353461.19058, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 86us/step - loss: 2416184.9925 - val_loss: 2191721.4630\n",
      "\n",
      "Epoch 00003: val_loss improved from 2353461.19058 to 2191721.46300, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 2121005.1520 - val_loss: 1498552.6962\n",
      "\n",
      "Epoch 00004: val_loss improved from 2191721.46300 to 1498552.69619, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 84us/step - loss: 1372359.4862 - val_loss: 1483004.4669\n",
      "\n",
      "Epoch 00005: val_loss improved from 1498552.69619 to 1483004.46693, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 84us/step - loss: 1147975.4794 - val_loss: 1136897.4013\n",
      "\n",
      "Epoch 00001: val_loss improved from 1483004.46693 to 1136897.40135, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 85us/step - loss: 1064984.2106 - val_loss: 1117239.3688\n",
      "\n",
      "Epoch 00002: val_loss improved from 1136897.40135 to 1117239.36883, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 1013311.5321 - val_loss: 1055015.0471\n",
      "\n",
      "Epoch 00003: val_loss improved from 1117239.36883 to 1055015.04709, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 85us/step - loss: 966797.6390 - val_loss: 1025160.7752\n",
      "\n",
      "Epoch 00004: val_loss improved from 1055015.04709 to 1025160.77522, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 85us/step - loss: 941906.4828 - val_loss: 1014186.4916\n",
      "\n",
      "Epoch 00005: val_loss improved from 1025160.77522 to 1014186.49159, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 79us/step - loss: 925244.2934 - val_loss: 1031803.7203\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 908974.7455 - val_loss: 1017789.0034\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 888953.4698 - val_loss: 1025055.6766\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 871616.0038 - val_loss: 1040177.2943\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 857985.6454 - val_loss: 1047101.1480\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 841468.7166 - val_loss: 1044491.4899\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 829074.3727 - val_loss: 1048783.9400\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 804466.4918 - val_loss: 1060578.6183\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 787304.0930 - val_loss: 1091005.0600\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 770987.4128 - val_loss: 1072016.5476\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 752909.0149 - val_loss: 1091236.9434\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 748379.4035 - val_loss: 1090567.5460\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 749877.5438 - val_loss: 1101344.1609\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 713068.2861 - val_loss: 1130550.3234\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 698845.0660 - val_loss: 1153958.1659\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 694780.6590 - val_loss: 1140835.1609\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 686958.3170 - val_loss: 1167818.1749\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 666064.7349 - val_loss: 1172885.3918\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 663985.7555 - val_loss: 1229137.1676\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 647022.6913 - val_loss: 1219765.2993\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 634178.5589 - val_loss: 1224388.8666\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 612777.3156 - val_loss: 1233717.9451\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 613924.3726 - val_loss: 1257776.5987\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 580189.9481 - val_loss: 1288726.4826\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 571318.6413 - val_loss: 1344282.8812\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 560531.4007 - val_loss: 1341009.1541\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 570614.3440 - val_loss: 1413236.6541\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 546525.3303 - val_loss: 1415620.7377\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 532081.9703 - val_loss: 1387135.4765\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 510983.7649 - val_loss: 1359473.4837\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 512571.5929 - val_loss: 1384279.9400\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 494617.1223 - val_loss: 1492343.2455\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 482018.7508 - val_loss: 1555079.4445\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 467428.5099 - val_loss: 1492887.6939\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 485484.3924 - val_loss: 1596372.3055\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 453135.1970 - val_loss: 1520007.0919\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1014186.49159\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 441188.1938 - val_loss: 1500905.3851\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1014186.49159\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 432118.3493 - val_loss: 1588607.6239\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1014186.49159\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 430483.7510 - val_loss: 1625073.1306\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1014186.49159\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 413396.4044 - val_loss: 1618554.9793\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1014186.49159\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 16s 4ms/step - loss: 2456843.7633 - val_loss: 2366295.7635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2366295.76345, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 89us/step - loss: 2454187.1812 - val_loss: 2354539.6177\n",
      "\n",
      "Epoch 00002: val_loss improved from 2366295.76345 to 2354539.61771, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 83us/step - loss: 2418198.8818 - val_loss: 2202955.5224\n",
      "\n",
      "Epoch 00003: val_loss improved from 2354539.61771 to 2202955.52242, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 83us/step - loss: 2148127.8552 - val_loss: 1449020.1138\n",
      "\n",
      "Epoch 00004: val_loss improved from 2202955.52242 to 1449020.11379, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 1403654.9032 - val_loss: 2174817.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 1449020.11379\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 1141979.0555 - val_loss: 1370327.1642\n",
      "\n",
      "Epoch 00001: val_loss improved from 1449020.11379 to 1370327.16424, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 1046963.8388 - val_loss: 1096336.9428\n",
      "\n",
      "Epoch 00002: val_loss improved from 1370327.16424 to 1096336.94283, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 996850.1456 - val_loss: 1024712.4630\n",
      "\n",
      "Epoch 00003: val_loss improved from 1096336.94283 to 1024712.46300, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 85us/step - loss: 969254.9847 - val_loss: 1025840.4557\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1024712.46300\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 955725.0016 - val_loss: 1011897.3352\n",
      "\n",
      "Epoch 00005: val_loss improved from 1024712.46300 to 1011897.33520, saving model to ./model/BranchRCheckpoint\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 945864.7927 - val_loss: 1009867.7848\n",
      "\n",
      "Epoch 00001: val_loss improved from 1011897.33520 to 1009867.78475, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 934347.2659 - val_loss: 999641.4092\n",
      "\n",
      "Epoch 00002: val_loss improved from 1009867.78475 to 999641.40919, saving model to ./model/BranchRCheckpoint\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 915514.9247 - val_loss: 1025992.1003\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 904282.6504 - val_loss: 1030957.8156\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 883737.0970 - val_loss: 1051183.2735\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 874313.7094 - val_loss: 1063774.9770\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999641.40919\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 855494.4810 - val_loss: 1060861.1721\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999641.40919\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 836034.6702 - val_loss: 1069571.2937\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 822879.4869 - val_loss: 1118458.3649\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 804214.2817 - val_loss: 1122878.7623\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 795308.4217 - val_loss: 1122273.5611\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999641.40919\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 771704.2010 - val_loss: 1129718.5303\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999641.40919\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 759170.6985 - val_loss: 1171881.4238\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 745374.0590 - val_loss: 1173845.4008\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 83us/step - loss: 716870.6753 - val_loss: 1192058.9798\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 84us/step - loss: 700328.2703 - val_loss: 1183771.2304\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999641.40919\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 690581.0840 - val_loss: 1243614.7674\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999641.40919\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 677077.6182 - val_loss: 1222791.0073\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 668297.2490 - val_loss: 1236872.7450\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 659687.0862 - val_loss: 1282087.1200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 78us/step - loss: 631969.3838 - val_loss: 1299947.6906\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999641.40919\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 624765.6967 - val_loss: 1329845.9333\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999641.40919\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 601448.0110 - val_loss: 1343623.6861\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 594838.8654 - val_loss: 1377643.5583\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 589198.6543 - val_loss: 1350433.1015\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 83us/step - loss: 573665.0943 - val_loss: 1397665.6048\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999641.40919\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 559942.2029 - val_loss: 1368357.9417\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999641.40919\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 544928.6152 - val_loss: 1408818.1491\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 83us/step - loss: 544927.6517 - val_loss: 1429046.0398\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 521131.2290 - val_loss: 1417113.8262\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 83us/step - loss: 532764.4912 - val_loss: 1399809.3442\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999641.40919\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 496909.6208 - val_loss: 1476382.7853\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999641.40919\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 505954.5250 - val_loss: 1504514.7965\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 484550.0238 - val_loss: 1499333.0359\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 82us/step - loss: 470418.3063 - val_loss: 1583286.8206\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n",
      "Train on 4014 samples, validate on 446 samples\n",
      "Epoch 1/5\n",
      "4014/4014 [==============================] - 0s 80us/step - loss: 462008.8396 - val_loss: 1539593.2085\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 999641.40919\n",
      "Epoch 2/5\n",
      "4014/4014 [==============================] - 0s 81us/step - loss: 465303.0599 - val_loss: 1630236.9978\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 999641.40919\n",
      "Epoch 3/5\n",
      "4014/4014 [==============================] - 0s 79us/step - loss: 449590.5608 - val_loss: 1593949.1127\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 999641.40919\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014/4014 [==============================] - 0s 80us/step - loss: 431394.1885 - val_loss: 1631956.0353\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 999641.40919\n",
      "Epoch 5/5\n",
      "4014/4014 [==============================] - 0s 77us/step - loss: 437929.1802 - val_loss: 1570160.7113\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 999641.40919\n"
     ]
    }
   ],
   "source": [
    "REG = REGModel()\n",
    "REG.load_weights('./model/REGweights')\n",
    "\n",
    "mean_benes = []\n",
    "death_rates = []\n",
    "death_counts = []\n",
    "mean_survivals = []\n",
    "\n",
    "for j in range(30):\n",
    "    BM, TBM, DEC, hist = train_branch_r()\n",
    "    TBM.load_weights('./model/BranchRCheckpoint')\n",
    "    \n",
    "    total_benefit = 0\n",
    "    death_count = 0\n",
    "    fail_count = 0\n",
    "    total_survival = 0\n",
    "    \n",
    "    R = np.copy(RR)\n",
    "    D = np.copy(DD)\n",
    "    S = np.copy(SS)\n",
    "\n",
    "    for i in range(len(D)):\n",
    "        d = D[i]\n",
    "    \n",
    "        if len(S) < 1:\n",
    "            break\n",
    "        \n",
    "        ts = DEC.predict(d.reshape(1, -1))\n",
    "        t = np.argmax(ts)\n",
    "    \n",
    "        rs = R[:500]\n",
    "        ys = BM.predict(rs)\n",
    "        bts = np.argmax(ys, axis=-1)\n",
    "        idxs = np.where(bts==t)[0]\n",
    "        if len(idxs) > 0:\n",
    "            idx = idxs[0]\n",
    "        else:\n",
    "            fail_count += 1\n",
    "            ts = np.repeat(ts, len(rs), axis=0)\n",
    "            y = np.sum(ys * ts, axis=-1)\n",
    "            y = y.reshape(-1)\n",
    "            potential_benefits = y - S[:500]\n",
    "            idx = np.argmax(potential_benefits)\n",
    "   \n",
    "        r = R[idx]\n",
    "        s = S[idx]\n",
    "        x = np.hstack([d, r]).reshape(1, -1)\n",
    "        y = REG.predict(x)[0][0]\n",
    "        total_survival += y\n",
    "        benefit = max(0, y - s)\n",
    "        total_benefit += benefit\n",
    "    \n",
    "        R = np.delete(R, idx, axis=0)\n",
    "        S = np.delete(S, idx)\n",
    "        \n",
    "        total_survival += len(S)\n",
    "    \n",
    "        S -= 1\n",
    "        death_count += np.sum(np.where(S==0, 1, 0))\n",
    "        non_zero = np.nonzero(S)\n",
    "        R = R[non_zero]\n",
    "        S = S[non_zero]\n",
    "    \n",
    "    mean_benefit = total_benefit / (len(D) - death_count)\n",
    "    death_rate = death_count / len(D)\n",
    "    mean_survival = total_survival / len(D)\n",
    "    \n",
    "    mean_benes.append(mean_benefit)\n",
    "    death_rates.append(death_rate)\n",
    "    death_counts.append(death_count)\n",
    "    mean_survivals.append(mean_survival)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632.2769969213097\n",
      "0.15816143497757848\n",
      "2755.69162719239\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.sort(mean_benes)[-10:]))\n",
    "print(np.mean(np.sort(death_rates)[:10]))\n",
    "print(np.mean(np.sort(mean_survivals)[-10:]))\n",
    "\n",
    "data = {}\n",
    "data['death_counts'] = list(np.array(death_counts).astype('float'))\n",
    "data['mean_benefits'] = list(np.array(mean_benes).astype('float'))\n",
    "data['mean_survivals'] = list(np.array(mean_survivals).astype('float'))\n",
    "data['death_rates'] = list(np.array(death_rates).astype('float'))\n",
    "\n",
    "with open('./model/allocBDTF.json', 'w+', encoding='utf8') as f:\n",
    "    f.write(json.dumps(data, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/allocBDTF.json', 'r', encoding='utf8') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "death_rates = np.array(data['death_rates'])\n",
    "mean_benes = np.array(data['mean_benefits'])\n",
    "mean_survivals = np.array(data['mean_survivals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699.7506397132967\n",
      "0.1562780269058296\n",
      "2764.1689167304958\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.sort(mean_benes)[-1:]))\n",
    "print(np.mean(np.sort(death_rates)[:1]))\n",
    "print(np.mean(np.sort(mean_survivals)[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
